[{
  "history_id" : "4sk2vrlluhj",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init as features\n# import visualization\nimport models_init as models\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-16 13:45:51,536 Logging to ../logs/2022-11-16_13_45_51.log\n2022-11-16 13:45:51,537 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-16 13:45:51,537 workdir=../data/evaluation list=[]\n2022-11-16 13:45:51,537 arxiv on cpu calc on cpu\n2022-11-16 13:45:51,538 getconstfeatures: datadir=../data/evaluation/.. list=['Autoencoder', '.DS_Store', 'features', 'evaluation', 'processed', 'lightning_logs', 'raw']\n2022-11-16 13:45:51,538 Loading ../data/evaluation/grid_cells.geojson\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 70, in <module>\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n  File \"/Users/uhhmed/gw-workspace/4sk2vrlluhj/features_inputs.py\", line 16, in getinputs\n    stmeta,grid,constfeatures = getconstfeatures(workdir, uregions, awsurl = awsurl, print=print)\n  File \"/Users/uhhmed/gw-workspace/4sk2vrlluhj/features_constFeatures.py\", line 26, in getconstfeatures\n    with open(file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/evaluation/grid_cells.geojson'\n",
  "history_begin_time" : 1668624343179,
  "history_end_time" : 1668624351938,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "r3Wu83VIA9Vt",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init as features\n# import visualization\nimport models_init as models\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-15 10:07:57,482 Logging to ../logs/2022-11-15_10_07_57.log\n2022-11-15 10:07:57,483 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-15 10:07:57,485 workdir=../data/evaluation list=[]\n2022-11-15 10:07:57,485 arxiv on cpu calc on cpu\n2022-11-15 10:07:57,486 getconstfeatures: datadir=../data/evaluation/.. list=['Autoencoder', '.DS_Store', 'features', 'evaluation', 'processed', 'lightning_logs', 'raw']\n2022-11-15 10:07:57,486 Loading ../data/evaluation/grid_cells.geojson\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 70, in <module>\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n  File \"/Users/uhhmed/gw-workspace/r3Wu83VIA9Vt/features_inputs.py\", line 16, in getinputs\n    stmeta,grid,constfeatures = getconstfeatures(workdir, uregions, awsurl = awsurl, print=print)\n  File \"/Users/uhhmed/gw-workspace/r3Wu83VIA9Vt/features_constFeatures.py\", line 26, in getconstfeatures\n    with open(file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/evaluation/grid_cells.geojson'\n",
  "history_begin_time" : 1668524852376,
  "history_end_time" : 1668524878191,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "N1Wacmiu4r9f",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init as features\n# import visualization\nimport models_init as models\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-08 18:52:12,913 Logging to ../logs/2022-11-08_18_52_12.log\n2022-11-08 18:52:12,913 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-08 18:52:12,914 workdir=../data/evaluation list=[]\n2022-11-08 18:52:12,914 arxiv on cpu calc on cpu\n2022-11-08 18:52:12,914 getconstfeatures: datadir=../data/evaluation/.. list=['Autoencoder', '.DS_Store', 'features', 'evaluation', 'processed', 'lightning_logs', 'raw']\n2022-11-08 18:52:12,914 Loading ../data/evaluation/grid_cells.geojson\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 70, in <module>\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n  File \"/Users/uhhmed/gw-workspace/N1Wacmiu4r9f/features_inputs.py\", line 16, in getinputs\n    stmeta,grid,constfeatures = getconstfeatures(workdir, uregions, awsurl = awsurl, print=print)\n  File \"/Users/uhhmed/gw-workspace/N1Wacmiu4r9f/features_constFeatures.py\", line 26, in getconstfeatures\n    with open(file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/evaluation/grid_cells.geojson'\n",
  "history_begin_time" : 1667951530481,
  "history_end_time" : 1667951533242,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NtYbkFa8Y9Va",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init\n# import visualization\nimport models_init as models\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-08 18:51:59,733 Logging to ../logs/2022-11-08_18_51_59.log\n2022-11-08 18:51:59,733 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-08 18:51:59,734 workdir=../data/evaluation list=[]\n2022-11-08 18:51:59,736 arxiv on cpu calc on cpu\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 70, in <module>\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \nNameError: name 'features' is not defined\n",
  "history_begin_time" : 1667951517205,
  "history_end_time" : 1667951520070,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zmOGE7aczzvs",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init\n# import visualization\nimport models_init\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-08 18:51:08,223 Logging to ../logs/2022-11-08_18_51_08.log\n2022-11-08 18:51:08,223 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-08 18:51:08,224 workdir=../data/evaluation list=[]\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 68, in <module>\n    print(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\nNameError: name 'models' is not defined\n",
  "history_begin_time" : 1667951465663,
  "history_end_time" : 1667951468565,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SQT85ECkvxKm",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init\n# import visualization\nimport models_init\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "Traceback (most recent call last):\n  File \"snowcast_main.py\", line 10, in <module>\n    from tictoc import TicTocGenerator\nModuleNotFoundError: No module named 'tictoc'\n",
  "history_begin_time" : 1667951445844,
  "history_end_time" : 1667951447114,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Sa0Ost1x09yB",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\n\nfrom snowcast_tictoc import TicTocGenerator\n\nfrom features_inputs import getinputs\n\n# from visualization_boxplot import boxplot\n# from visualization_month import monthests\nfrom visualization_figures import temporal,spatial,importance,latentsd\n\nimport models_addons as addons\nfrom models_apply import apply, valid_bs, applymodels\n# import models_functions as functions\nfrom models_inference import inference, test, getests\nfrom models_loadmodels import loadmodels\nfrom models_models import Model, Model3\n# from models_oanet4s import DLOA, Model0\n\n\ncalcdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = inference(inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        spatial(inputs['test'], dates['test'], result, figdir)\n        importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                inference(inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "Traceback (most recent call last):\n  File \"snowcast_main.py\", line 16, in <module>\n    from features_inputs import getinputs\n  File \"/Users/uhhmed/gw-workspace/Sa0Ost1x09yB/features_inputs.py\", line 8, in <module>\n    import features\nModuleNotFoundError: No module named 'features'\n",
  "history_begin_time" : 1667944749090,
  "history_end_time" : 1667944752886,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : null,
  "indicator" : "Failed"
},]
