[{
  "history_id" : "uforzh0xc3m",
  "history_input" : "import models_functions as functions\nimport numpy as np\nfrom models_oanet4s import DLOA, Parameter, Model0\nimport torch\n\ngetstyle = lambda style: 'season' if style is None else style\n\nclass Calibration(torch.nn.Module):\n    def __init__(self, **kwargin):\n        torch.nn.Module.__init__(self)\n        # self.p = Parameter([ 4.,  0., -4., -0.5, -4., -3.])\n        self.p = Parameter([ 3.,  0., -3., -0.5, -5.])\n    def forward(self,r):\n        p = self.p.to(r.device)\n        r1 = r/10.\n        if p.numel() == 5:\n            k = p[2]+r1*(p[3]+r1*p[4])        \n        else:\n            k = p[2]+r1*(p[3]+r1*(p[4]+r1*p[5]))\n        return (r+p[0].clamp(max=-p[2])+k*torch.exp(-r1*functions.sigma_act.apply(p[1]))).clamp_(min=0.)\n\nclass Model(DLOA):\n    def __init__(self, ninputs, nfunc, nlatent=3, norm=False, nmonths=1, calibr=False,\n                  initgamma=1., embedding=0, nstations=1, style=None, mc=1, dropout=0., initemb=None, **kw):\n        kw['neddvalgrad'] = initgamma is not None        \n        self.style = getstyle(style)\n        if self.style[-3:] == 'pos':\n            nfunc *= 2\n        # DLOA.__init__(self, 2*ninputs+embedding, nfunc, nlatent, **kw)\n        # self.m0 = DLOA(0,1,0,implicit=False,edging=True,rmax=1.,usee=False,biased=False,relativer=True)\n        DLOA.__init__(self, ninputs+embedding, nfunc, nlatent, **kw)\n        self.calcsd = self.dist.calcsd\n        self.embedding = embedding\n        self.mc = mc\n        self.nmonths = nmonths\n        if embedding > 0:\n            emb = torch.nn.Embedding(nstations*nmonths, embedding, max_norm=2.)\n            if initemb is None:\n                emb.weight.data.uniform_(-0.15, 0.15)\n                emb.weight.data[:nmonths] = 0\n            else:\n                emb.weight.data[:,0] = initemb            \n            if dropout > 0:\n                self.emb = torch.nn.Sequential(emb, torch.nn.Dropout(dropout))\n            else:\n                self.emb = torch.nn.Sequential(emb)\n            self.embmult = 0.2\n            # self.net[1].weight.data[:,len(arglist):len(arglist)+self.embedding] = \\\n            #     self.embmult*self.net[1].weight.data[:,len(arglist):len(arglist)+self.embedding]\n        self.calibr = None\n        if calibr:\n            self.calibr = Calibration()\n        self.norm = norm\n        if initgamma is not None:\n            self.gamma = Parameter(initgamma)      \n    def forward(self, xx, isval=True):\n        sel = torch.isfinite(xx['xval'][...,0]).sum(1) > self.points\n        select = (~sel).sum() > 0\n        if sel.sum() > 0:\n            x = {key: xx[key][sel] for key in xx} if select else xx\n        else:\n            return torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device),None\n        self.dist.calcsd = self.calcsd\n        xval = x['xval'].clone()\n        usey = 'ylo' in x\n        xi = x['xinput'].clone()\n        if usey:\n            yi = x['yinput'].clone()\n        # embinterp = (not self.training) and (self.embedding > 0)\n        # # embinterp = self.embedding > 0\n        # if embinterp:\n        #     xemb = torch.cat((x['xemb'],x['yemb']),1) if usey else x['xemb']\n        #     device = xemb.device\n        #     isy = xemb[0] == 0\n        #     embedding = self.embedding; self.embedding = 0\n        #     if isy.sum() > 0:\n        #         isx = ~isy\n        #         if isx.sum() > 0:\n        #             xe = {'x'+key: torch.cat((x['x'+key],x['y'+key]),1) if usey else x['x'+key] for key in ['lo', 'la']}\n        #             xx = {key: xe[key][:,isx] for key in xe}\n        #             xx.update({'y'+key[1:]: xe[key][:,isy] for key in xe})                \n        #             res = torch.zeros(xe['xlo'].shape+(embedding,), device=device)\n        #             xx['xval'] = self.emb(xemb[:,isx]).mul_(self.embmult)\n        #             res[:,isx] = xx['xval']\n        #             res[:,isy] = Model0.to(device) (xx)\n        #             x['xinput'] = torch.cat([x['xinput'],res[:,:x['xinput'].shape[1]]],-1)\n        #             if usey:\n        #                 x['yinput'] = torch.cat([x['yinput'],res[:,x['xinput'].shape[1]:]],-1)\n        #         else:\n        #             x['xinput'] = torch.cat([x['xinput'],torch.zeros(x['xlo'].shape+(embedding,), device=device)],-1)\n        #             if usey:\n        #                 x['yinput'] = torch.cat([x['yinput'],torch.zeros(x['ylo'].shape+(embedding,), device=device)],-1)\n        #     else:\n        #         x['xinput'] = torch.cat([x['xinput'],self.emb(x['xemb'])],-1)\n        #         if usey:\n        #             x['yinput'] = torch.cat([x['yinput'],self.emb(x['yemb'])],-1)\n        sd = None\n        if isval:\n            xv = x['xval']/10. if self.biased else x['xval']\n            bad = torch.isnan(xv)\n            if hasattr(self, 'gamma'):\n                gamma = torch.sigmoid(self.gamma).to(xv.device)\n                ok = (~bad)&(xv>0.)\n                xv[ok] = torch.pow(xv[ok], gamma)       \n            if self.norm:\n                xv  = torch.nan_to_num(xv,0.)\n                nrm = (xv.sum(1,keepdim=True)/(xv>0.).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n                xv = xv/nrm\n            if (self.mc > 1) and self.training and (not self.net[0].frozen):\n            # if self.mc > 1:\n                res = []\n                if self.training:\n                    rnd = torch.randn_like(xv)\n                for f in [-0.15,0.15,0.0][:self.mc]:\n                    # f1 = f*(1.-self.net[0].factor)\n                    x['xval'] = xv*((f*rnd + 1.).clamp_(min=0.,max=2.) if self.training else (f + 1.))\n                    x['xval'][bad] = np.nan\n                    res.append(DLOA.forward(self,x)[...,None])\n                res = torch.cat(res,-1).mean(-1)\n            else:\n                xv[bad] = np.nan\n                x['xval'] = xv\n                if self.style[-3:] == 'pos':\n                    x['xval'] = torch.cat((x['xval'],torch.tanh(xv)),-1)\n                res = DLOA.forward(self,x)\n            if self.norm:\n                if self.style[-3:] == 'pos':\n                    res = res[...,:xv.shape[-1]].clamp(min=0.)*(2.*res[...,xv.shape[-1]:]-1.).clamp(min=0.,max=1.)*nrm\n                else:\n                    res = res*nrm\n        else:\n            res = DLOA.forward(self,x)\n        if isval:\n            if hasattr(self, 'gamma'):\n                ok = torch.isfinite(res)&(res>0.)\n                res[ok] = torch.pow(res[ok], 1./gamma)\n            if self.biased:\n                res = (res*10.).clamp(min=0.)\n            if self.calibr is not None:\n                res = self.calibr(res)\n            x['xval'] = xval\n        # if embinterp:\n        #     self.embedding = embedding\n        x['xinput'] = xi\n        if usey:\n            x['yinput'] = yi\n        if 'sd' in x:\n            for key in ['sd','esd','ysigma','yval']:\n                if key in x:\n                    x[key] = x[key][...,:1]*nrm[...,:1] if self.norm else x[key][...,:1]\n                    if hasattr(self, 'gamma'):\n                        x[key] = torch.pow(x[key], 1./gamma)\n            sd = x['sd']\n        if select:\n            res1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'].shape+res.shape[-1:], np.nan, device=xx['xval'].device)\n            res1[sel] = res            \n            if sd is not None:\n                sd1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'].shape+(1,), np.nan, device=xx['xval'].device)\n                sd1[sel] = sd\n                return res1,sd1\n            return res1,None\n        return res,sd\n    def freeze(self, frozen):\n        self.dist.kf.frozen = frozen\n        if self.usee:\n            self.enet.kf.frozen = frozen\n        self.net[0].frozen = frozen>0\n    def prints(self):\n        return DLOA.prints(self) + (f' gamma={torch.sigmoid(self.gamma).item():.4}' if hasattr(self, 'gamma') else '')\n    def kfparams(self):\n        return [p for p in self.dist.parameters() if p.requires_grad]+\\\n              ([p for p in self.enet.parameters() if p.requires_grad] if self.usee else [])+\\\n              ([self.gamma] if hasattr(self, 'gamma') else [])+\\\n              ([self.calibr.p] if self.calibr is not None else [])\n    def netparams(self):\n        return [p for p in self.net.parameters() if p.requires_grad]+\\\n              ([p for p in self.emb.parameters() if p.requires_grad] if self.embedding > 0 else [])\n    def importance(self, arglist):\n        w = self.net[1].weight; w = torch.sqrt((w*w).sum(0))\n        roundw = lambda x: np.round(x.item(),3) if x.numel() == 1 else list(np.round(x.detach().cpu().numpy(),3))\n        imp = {key: roundw(w[i]) for i,key in enumerate(arglist)}\n        if self.embedding > 0:\n            imp['emb'] = roundw(w[len(arglist):len(arglist)+self.embedding])\n        w = w[len(arglist)+self.embedding:]\n        if self.densed:\n            nkf = self.dist.kf.rm.shape[0]\n            imp.update({'fg': roundw(w[:-nkf]), 'dense': roundw(w[-nkf:])})\n        else:\n            imp['fg'] = roundw(w)\n        return imp\n    def nonzero(self):\n        nz = {p.weight: torch.ones_like(p.weight,dtype=torch.bool) for p in self.net[1:-1:2]}\n        if self.embedding > 0 and self.nmonths > 1:\n            nz.update ({p: torch.ones_like(p,dtype=torch.bool) for p in self.emb.parameters() if p.requires_grad})\n        return nz\n              \nclass Model3(torch.nn.Module):\n    def __init__(self, weights, *argin, layers=2, style=None, mc=1, commonnet=True, **kwargin):\n        torch.nn.Module.__init__(self)\n        self.style = getstyle(style)\n        # mc=1\n        nets = [Model(*argin, style=style, mc=mc, **kwargin)]\n        nnets = {'season': 1, 'season_pos': 1}\n        if self.style == 'stack':\n            kwargin.update({'usee': False, 'rmax': kwargin['rmax2']}) #, 'implicit': True\n            nets += [Model(argin[0]+argin[1], *argin[1:], style=style, mc=mc, **kwargin) for k in range(layers-1)]\n        else:\n            if self.style == 'pos':\n                # kwargin.update({'initgamma': None, 'sigmed': False})\n                kwargin.update({'initgamma': None})\n            nets += [Model(*argin, style=style, mc=mc, **kwargin) for k in range(nnets[self.style] if self.style in nnets else layers-1)]\n        self.nets = torch.nn.ModuleList(nets)\n        self.calcsd = self.nets[0].dist.calcsd\n        self.commonnet = self.style not in ['stack'] and commonnet\n        # self.commonnet = False\n        if weights is not None:\n            weights = {key: weights[key].clone() for key in weights}\n            if self.style == 'season' and 'gamma' in weights:\n                weights['gamma'].add_(0.2)\n            self.nets[0].load_state_dict(weights)\n            if self.style not in ['stack']:\n                if self.style[:6] == 'season' and 'gamma' in weights:\n                    weights['gamma'].sub_(0.4)\n                if self.nets[1].sigmed:\n                    for net in self.nets[1:]:\n                        net.load_state_dict(weights)\n        if self.style not in ['stack'] and self.commonnet:\n            for net in self.nets[1:]:\n                net.net = self.nets[0].net\n                if self.nets[0].embedding > 0:\n                    net.emb = self.nets[0].emb\n        self.points = self.nets[0].points\n    def nonzero(self, nzero=None):\n        getnz = lambda net: net.nonzero().keys()\n        if nzero is None:\n            nzero = self.nets[0].nonzero()\n        nz = {p: nzero[p0] for (p,p0) in zip(getnz(self.nets[0]),nzero.keys())}\n        for net in self.nets[1:]:\n            if self.style == 'stack':\n                nz.update ({p: torch.ones(p.shape,dtype=torch.bool,device=p.device) for (p,p0) in zip(getnz(net),nzero.keys())})\n            else:\n                nz.update ({p: nzero[p0] for (p,p0) in zip(getnz(net),nzero.keys())})\n        return nz        \n    def forward(self, xx):\n        for net in self.nets:\n            net.points = self.points\n            net.dist.calcsd = self.calcsd\n        sel = torch.isfinite(xx['xval'][...,0]).sum(1) > self.points\n        select = (~sel).sum() > 0\n        if sel.sum() > 0:\n            x = {key: xx[key][sel] for key in xx} if select else xx\n        else:\n            return torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device),None\n        sd = None\n        if self.style == 'stack':\n            res,sd = self.nets[0](x)\n            xin = x['xinput'].clone()\n            if 'ylo' in x:\n                yin = x['yinput'].clone()\n                x['xinput'] = torch.cat([x['xinput'], x['xval']], -1)\n            yinp = 'yinput' if 'ylo' in x else 'xinput'\n            x[yinp] = torch.cat([x[yinp], res], -1)\n            res,sd = self.nets[1](x)\n            for net in self.nets[2:]:\n                x[yinp][...,-res.shape[-1]] = res\n                res,sd = net(x)\n            x['xinput'] = xin\n            if 'ylo' in x:\n                x['yinput'] = yin\n            return res,sd\n        elif self.style[:6] == 'season':\n            rets = []\n            sd = []\n            for i in range(len(self.nets)):\n                # x['xinputs2'] = i\n                # x['yinputs2'] = i\n                rets.append(self.nets[i](x)[0][...,None])\n                if 'sd' in x:\n                    sd.append(x['sd'])\n            rets = torch.cat(rets,-1)\n            d = rets.mean(-1)\n            d = torch.sigmoid(d[...,:1]-d[...,-1:])\n            res = (rets[...,:1,0]-rets[...,:1,1])*d+rets[...,:1,1]\n            if len (sd) > 0:\n                # printshape(sd+[d])\n                sd = (sd[0]-sd[1])*d+sd[1]\n            else:\n                sd = None\n        else:\n            res = torch.cat([net(x)[...,None] for net in self.nets],-1).mean(-1)\n        if select:\n            res1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device)\n            res1[sel] = res            \n            if sd is not None:\n                sd1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device)\n                sd1[sel] = sd\n                return res1,sd1\n            return res1,None\n        return res,sd\n    def freeze(self, frozen):\n        for net in self.nets:\n            net.freeze(frozen)\n    def prints(self):\n        return ' '.join([net.prints() for net in self.nets])\n    def kfparams(self):\n        r = []\n        for par in [net.kfparams() for net in self.nets]:\n            r += par\n        return r\n    def netparams(self):                        \n        p = [net.netparams() for net in self.nets]\n        r = p[0]\n        if not self.commonnet:\n            for par in p[1:]:\n                r += par[2:] if self.style[:6] == 'season' else par\n        return r\n    def loss(self, *argin, **kwargin):\n        return self.nets[0].loss(*argin, **kwargin)\n    def importance(self, arglist):\n        if self.commonnet:\n            return self.nets[0].importance(arglist)\n        imp = [net.importance(arglist) for net in self.nets]\n        return {key: [i[key] for i in imp] for key in imp[0]}\n    ",
  "history_output" : "",
  "history_begin_time" : 1668624366817,
  "history_end_time" : 1668624368950,
  "history_notes" : null,
  "history_process" : "91wt3d",
  "host_id" : "100001",
  "indicator" : "Done"
},]
