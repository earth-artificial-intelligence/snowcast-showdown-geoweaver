[{
  "history_id" : "dubc5an0lzh",
  "history_input" : "from models_init import addons\nimport models_functions as functions\nimport numpy as np\t\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\ntry:\n    from torch_scatter import scatter\nexcept:\n    scatter = None\n\nParameter = lambda x: nn.Parameter(x.float().requires_grad_(True), requires_grad=True) if isinstance(x,torch.Tensor) \\\n                      else nn.Parameter(torch.tensor(x, dtype=torch.float32, requires_grad=True))\nVariable  = lambda x: torch.autograd.Variable(x if isinstance(x,torch.Tensor) else torch.tensor(x), requires_grad=False)\n\ndef MLP(ninputs, nf, noutputs, bias=True, sgd2=False):\n    layers = [addons.Normalize([ninputs], dims=[0])]\n    Linear = addons.Linear2 if sgd2 else nn.Linear\n    for nin,nout in zip([ninputs]+nf[:-1],nf):\n        layers += [Linear (nin,nout), nn.ReLU(inplace=True)]\n        # layers += [Linear (nin,nout), nn.Tanh()]\n    layers.append (Linear (nf[-1] if len(nf)>0 else ninputs, noutputs, bias=bias))\n    net = nn.Sequential (*layers)\n    net.apply(lambda m: addons.weight_init(m,gain=0.9))\n    net[-1].weight.data.mul_(0.5/np.sqrt(noutputs))\n    return net\n\n# bigdist = 256.*256.\nbigdist = 1024.\ndef distancesq(lo1, la1, lo2, la2):\n    with torch.no_grad():\n        dlo = torch.remainder(torch.abs(lo1-lo2)+180.,360.)-180.\n        dlo.mul_(torch.cos((la1+la2)*0.00872665)); dlo.mul_(dlo)\n        dla = la1-la2; dla.mul_(dla).add_(dlo)\n        return torch.nan_to_num_(dla,bigdist)\n\nclass nearestf(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, lo1, la1, lo2, la2, points, map1=None, map2=None, exclude=0, bad=None):\n        with torch.no_grad():\n            sh = lo2.shape\n            s1 = (sh[0]+4)*sh[1]*lo1.shape[1]\n            if map1 is not None and map2 is not None:\n                s1 *= 1+map1.shape[-1]\n            splits = int(np.ceil(s1/5.0e8))\n            sz     = int(np.ceil(sh[1]/splits))\n            inds = []; diss=[]\n            for i in range(splits):\n                i0 = i*sz\n                i1 = (i+1)*sz if i < splits-1 else lo2.shape[1]\n                dis = distancesq(lo2.data[0,i0:i1,None], la2.data[0,i0:i1,None], lo1.data[0,None], la1.data[0,None])\n                dis = dis[None]+(bad.to(dis.dtype)*bigdist)[:,None].to(dis.dtype) if bad is not None else dis[None,:,:]\n                if map1 is not None and map2 is not None:\n                    d = map1.data[:,None]-map2.data[:,i0:i1,None]\n                    dis = torch.einsum ('k...j,k...j->k...', d, d).add_(dis)\n                dis,ind = torch.topk(dis, points+exclude, dim=-1, largest=False)\n                if exclude>0:\n                    ind = ind[..., exclude:];  dis = dis[..., exclude:]\n                if ind.shape[0] != sh[0]:\n                    ind = ind.expand(sh[0],-1,-1)\n                    dis = dis.expand(sh[0],-1,-1)\n                diss.append(dis);  inds.append(ind)\n            if splits>1:\n                ind = torch.cat(inds,dim=1)\n                dis = torch.cat(diss,dim=1)\n        ctx.save_for_backward(map1,map2,ind)\n        return ind.detach(),dis.detach()\n    @staticmethod\n    def backward(ctx, gind, gdis):\n        dm1=None; dm2=None\n        if ctx.needs_input_grad[5]:\n            map1,map2,ind = ctx.saved_tensors\n            ind1 = ind.reshape(*((ind.shape[0],-1)+(1,)*(map1.ndim-2)))\n            # d = map2.unsqueeze(-2)\n            d = map2.unsqueeze(ind.ndim-1) - torch.gather(map1, 1, ind1.expand(-1,-1,*map1.shape[2:])).reshape(*ind.shape,*map1.shape[2:])\n            d.mul_(gdis.unsqueeze(-1)*2.)\n            if ctx.needs_input_grad[6]:\n                dm2 = d.sum(ind.ndim-1)\n            d = d.reshape(d.shape[0], -1, *d.shape[2-map1.ndim:])\n            if scatter:\n                dm1 = scatter (d, ind1, dim=1, dim_size=map1.shape[1], reduce=\"sum\")\n            else:\n                dm1 = torch.zeros_like(map1)\n                dm1.scatter_add_(1, ind1.expand(-1,-1,*d.shape[2-map1.ndim:]), d)\n            dm1 = -dm1\n        return None,None,None,None,None,dm1,dm2,None,None\n        \nclass distsq(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, y, dim=-1):        \n        ctx.save_for_backward(x, y); d = x-y\n        ctx.dim = dim\n        return torch.einsum ('k...j,k...j->k...' if dim == -1 else '...jk,...jk->...k', d, d).detach()\n    @staticmethod\n    def backward(ctx, gdis):\n        dx=None; dy=None\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n            x,y = ctx.saved_tensors\n            d = (x - y)*gdis.unsqueeze(ctx.dim)\n            if ctx.needs_input_grad[0]:\n                xdim = [i for i in range(d.ndim) if d.shape[i]>x.shape[i]]\n                dx = d.sum(xdim, keepdim=True).mul_(2.) if len (xdim) > 0 else d*2.\n            if ctx.needs_input_grad[1]:\n                ydim = [i for i in range(d.ndim) if d.shape[i]>y.shape[i]]\n                dy = (d.sum(ydim, keepdim=True) if len (ydim) > 0 else d).mul_(-2.)\n        return dx,dy,None\n\nclass MyKF(nn.Module):\n    def __init__(self, nfunc, rmax=1., *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.rm = Parameter(torch.zeros(nfunc,1))\n        self.rmax2 = rmax**2 \n        # self.relativer = relativer\n        self.frozen = 0\n    def prints(self):\n        r = torch.sqrt(self.getrm()).data.cpu().numpy()\n        if r.size > 1:\n            return f' Rg: [{111./r[0,0]:.4} {111./r[-1,0]:.4}]'\n        return f' Rg: {111./r[0,0]:.4}'\n    def getrm(self):\n        return functions.sigma_act.apply(self.rm)/self.rmax2\n# calc KF (sqrt(d))\n    def forward(self, d, exp):\n        C = functions.xp1expm.apply(d.reshape(1,-1)*self.getrm().to(d.device), exp)\n        return C.reshape(*C.shape[:-1],*d.shape)\n\nclass DistOA(nn.Module):\n    def __init__(self, nfunc, edging=False, implicit=True, eps=-1.5, neddvalgrad=False, calcsd=0, *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.implicit = implicit\n        self.edging = edging\n        self.logsigmoid = torch.nn.LogSigmoid()\n        self.kf = MyKF(nfunc, *argin, **kwargin)\n        self.softdot = functions.softdot1.apply if neddvalgrad else functions.softdot.apply\n        self.neddvalgrad = neddvalgrad\n        self.calcsd = calcsd\n        self.lusolve = torch.linalg.solve if hasattr(torch,'linalg') and hasattr(torch.linalg,'solve') else lambda C,b: torch.solve(b,C)[0]\n        self.eps = Parameter(eps*torch.ones(nfunc))\n    def prints(self):\n        self.kf.eval()\n        eps = self.geteps(torch.device('cpu'))[...,0,0,0]\n        if not self.implicit:\n            eps = torch.exp(eps)\n        msg = self.kf.prints()\n        if self.implicit or not self.edging:\n            msg += ' eps:' + np.array2string(eps.detach().cpu().numpy())\n        self.kf.train(self.training)\n        return msg.replace('\\n','')\n    def func(self, d, eval=True, sq=False):\n        if eval:\n            self.kf.eval()\n        eps = self.geteps(d.device).squeeze(-1).squeeze(-1)\n        if not self.implicit:\n            eps = torch.exp(eps)\n        if eps.ndim == 2:\n            eps = eps[:,None]*torch.eye(eps.shape[0],device=eps.device) \n        f = self.kf (d if sq else d*d, True)\n        f = f[None]*torch.eye(eps.shape[0],device=f.device)[..., None]\n        if eval:\n            self.kf.train(self.training)\n        de = torch.diag(f[:,:,0] + eps[:,:,0])\n        return f/torch.sqrt(de[None,:,None]*de[:,None,None])\n    def graph(self, plot=True, rmax=None):\n        if rmax is None:\n            rmax = 3./np.sqrt(self.kf.getrm().min().item())\n        t = torch.arange(0.,rmax,rmax/99.9,device=self.eps.device)\n        f = self.func(t)\n        if f.ndim > 2:\n            diag = torch.eye(f.shape[0], device=f.device, dtype=torch.bool)\n            f = torch.cat([f[diag], f[~diag]],0)\n        f = f.cpu().detach().numpy().T\n        r = 111.*t.cpu().detach().numpy()\n        if plot:\n            plt.plot (r, f)\n        return r[:,None],f\n    def solve(self, b, C):        \n        if self.edging:\n            n1 = 1./C.shape[-1]\n            d = self.lusolve(torch.cat((torch.cat((C,torch.full(C.shape[:-1]+(1,),-n1,dtype=C.dtype,device=C.device)),axis=-1),\n                            torch.cat((torch.full(C.shape[:-2]+(1,C.shape[-1]),n1,dtype=C.dtype,device=C.device),\n                                       torch.zeros(C.shape[:-2]+(1,1),dtype=C.dtype,device=C.device)),axis=-1)),axis=-2),\n                            torch.cat((b, torch.full(b.shape[:-2]+(1,b.shape[-1]),n1,dtype=b.dtype,device=b.device)),axis=-2))[...,:-1,:]\n        else:\n            d = self.lusolve(C,b)            \n        wg = (torch.abs(d).sum(-2,keepdim=True)-1.).clamp_(min=0.).mul(2.).clamp_(max=1.)\n        d1 = b/C.sum(-1,keepdim=True)\n        return d*(1.-wg) + d1*(wg/d1.sum(-2,keepdim=True).clamp_(1e-6 if self.edging else 1.))\n    def geteps(self, device):\n        return (torch.sigmoid(self.eps) if self.implicit else self.logsigmoid(self.eps)).to(device)[...,None,None,None]\n    def forward(self, C, val, bad=None):        \n        bsh = C.shape[:-2]+(C.shape[-1]-C.shape[-2],)\n        kfdims = 2\n        resh = C.ndim > kfdims+1\n        if resh:\n            C = C.reshape(-1, *C.shape[-kfdims:])\n            if val.ndim>3:\n                val = val.reshape(-1, *val.shape[-2:])\n                if bad is not None:\n                    bad = bad.reshape(val.shape[:-1])        \n        if bad is None:\n            bad = torch.isnan(val).any(-1)\n        with torch.no_grad():\n            eye = torch.eye(C.shape[1],dtype=val.dtype,device=C.device)\n            if not self.implicit:\n                eye.sub_(1.)\n        # self.returnsd = (self.calcsd==1 and not (self.training and (self.kf.frozen>=2))) or (self.calcsd==2)\n        self.returnsd = (self.calcsd==1 and self.training and (self.kf.frozen==0)) or (self.calcsd==2)\n        if self.implicit or not self.edging:\n            eps = self.geteps(C.device)*eye\n        val0 = (val.clone() if self.neddvalgrad else val.detach()).nan_to_num_(0.)\n        C = self.kf(C, self.implicit)\n        \n        b = C[..., C.shape[-2]:]; C = C[..., :C.shape[-2]]\n        if self.implicit:\n            C.add_(eps)            \n            if bad.any():\n                if self.training:\n                    b = b*~bad.unsqueeze(-1)\n                    C = C*~torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2))\n                else:\n                    b.mul_(~bad.unsqueeze(-1))\n                    C.mul_(~torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)))\n            r = self.solve(b, C)\n            if self.returnsd:\n                d = 1.-torch.einsum ('k...ip,k...ip->...pk', r, b)\n            r = torch.einsum ('k...jp,...jk->...pk', r, val0)\n        else:\n            if self.training:\n                if not self.edging:\n                    C = C - eps\n                if bad.any():\n                    b = b.add(bad.unsqueeze(-1), alpha=-bigdist)\n                    C = C.add(torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)), alpha=-bigdist)\n            else:\n                if not self.edging:\n                    C.sub_(eps)\n                if bad.any():\n                    b.add_(bad.unsqueeze(-1), alpha=-bigdist)\n                    C.add_(torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)), alpha=-bigdist)\n            r = b-torch.logsumexp(C,dim=-1,keepdim=True)\n            if self.returnsd:\n                r = nn.functional.log_softmax(r,dim=-2) if self.edging else r-torch.logsumexp(r,dim=-2,keepdim=True).clamp(min=0.)\n                d = 1.-torch.exp(r+b).sum([-2,-1]).T\n            r = self.softdot(r.permute(1,2,3,0), val0.unsqueeze(-2), 1, None if self.edging else 1.)\n        if resh:\n            r = r.reshape(*bsh[:2],-1)\n        if self.returnsd:\n            if resh:\n                d = d.reshape(*bsh[:2],-1)\n            return r,torch.sqrt(d.clamp(min=0.,max=1.))\n        return r\n            \ndef gather (x, lab):\n    return (torch.gather(x[lab], 1, x['ind1']).reshape(x['ind'].shape) if x[lab].ndim<3 else\n            torch.gather(x[lab], 1, x['ind1'][(...,) + (None,)*(x[lab].ndim-2)].expand(-1,-1,*x[lab].shape[2:])).reshape(*x['ind'].shape, *x[lab].shape[2:]))\ndef gathera (x, lab, index='ind'):\n    return x[lab].reshape(-1,*x[lab].shape[2:])[x[index]]\n \ndef printshape(a, prefix=''):\n    print (prefix+str({key: (a[key].dtype,a[key].shape,torch.isfinite(a[key]).sum().item() if isinstance(a[key],torch.Tensor) else np.isfinite(a[key]).sum()) for key in a} \\\n           if isinstance(a,dict) else [(x.dtype,x.shape,torch.isfinite(x).sum().item() if isinstance(x,torch.Tensor) else np.isfinite(x).sum()) for x in a]))\n    \nclass DLOA(nn.Module):\n    def __init__(self, ninputs, nfunc, nlatent=3, individual=False, usee=True, sigmed=False, reselect=True,\n                 nf=[32], netinit=None, points=16, densed=False, neddvalgrad=False,\n                 calcsd=0, rmax=1., rmaxe=None, sgd2=True, biased=False, relativer=False, gradloss=0.,\n                 *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.sigmed = sigmed\n        self.biased = biased\n        self.individual = individual\n        nkf = nfunc if self.individual else 1        \n        self.neddvalgrad = neddvalgrad\n        self.relativer = relativer\n        self.usee = usee\n        self.densed = usee and densed\n        self.reselect = reselect or not self.usee\n        self.embedding = 0\n        self.gradloss = gradloss\n        self.dist = DistOA(nkf, neddvalgrad=self.biased or self.sigmed or neddvalgrad, calcsd=calcsd, \n                           rmax=rmax,#+np.sqrt(nlatent*0.5),\n                           *argin, **kwargin)\n        if usee:\n            kwargin.update({'implicit': False, 'edging': True, 'eps': -4. if densed else -2.})\n            self.enet = DistOA(nkf, neddvalgrad=False, calcsd=2 if self.densed else calcsd, \n                               rmax=rmax if rmaxe is None else rmaxe, *argin, **kwargin)\n        self.nfunc = nfunc\n        self.nlatent = nlatent\n        self.outs = {'map': nlatent}\n        if self.sigmed:\n            self.outs['sigma'] = nfunc        \n        if self.biased:\n            self.outs['bias'] = nfunc        \n        if netinit is None:\n            netinit = lambda nin,nout,bias: MLP (nin, nf, nout, bias, sgd2=sgd2)\n        netargs = ninputs+(nfunc+(nkf if self.densed else 0) if self.usee else 0)\n        netouts = sum([self.outs[key] for key in self.outs])\n        if netargs>0 and netouts>0:\n            self.net = netinit (netargs, netouts, bias=self.biased or self.sigmed)\n            if self.sigmed:\n                self.net[-1].bias.data = torch.cat([torch.ones(self.outs[key])*(0. if key=='eps' else ( 2.)) for key in self.outs]).detach()\n        else:\n            self.net = None\n        self.points = points\n        self.history = {}\n    def prints(self):\n        return (self.dist.prints() + (self.enet.prints() if self.usee else '')).replace('\\n','')\n    def return_KF(self):\n        return False\n    def graph(self, *argin, **kwargin):\n        return self.dist.graph(*argin, **kwargin)\n    def mappingx (self, x, lab):\n        if self.net is not None:\n            y = [x[lab+'input']]\n            if self.embedding > 0:\n                x[lab+'emba'] = self.emb(x[lab+'emb' if lab+'emb' in x else 'xemb']).mul_(self.embmult)\n                y.append(x[lab+'emba'])\n            if self.usee:\n                y.append(x[lab+'val'])\n            if self.densed:\n                y.append(torch.zeros(x['xval'].shape[:-1]+self.enet.eps.shape[:1],device=x['xval'].device) if lab=='x' else x['esd'])\n            y = torch.cat(y,-1) if len(y)>1 else x[lab+'input']\n            ok = torch.isfinite(y).all(-1)\n            if ok.any():\n                ymap = self.net (y[ok])\n                i0 = 0\n                for key in self.outs:\n                    i1 = i0+self.outs[key]\n                    s = ymap[..., i0:i1]; i0 = i1          \n                    if key == 'sigma':\n                        x[lab+key] = torch.ones (y.shape[:-1]+s.shape[-1:], dtype=ymap.dtype, device=ymap.device)                        \n                        s = functions.sigma_act.apply(s)\n                    else:\n                        x[lab+key] = torch.zeros (y.shape[:-1]+s.shape[-1:], dtype=ymap.dtype, device=ymap.device)\n                    x[lab+key][ok] = s\n            if lab == 'x':\n                x[lab+'ok'] = ok\n        else:\n            x[lab+'map'] = None\n            if lab == 'x':\n                x[lab+'ok'] = torch.isfinite(x[lab+'val']).all(-1)\n    def distmatr(self, x, bad, ismap, usey):\n        args = [x['xlo'], x['xla'], x['ylo' if usey else 'xlo'], x['yla' if usey else 'xla'], self.points]\n        args += [x['xmap'], x['ymap']] if ismap else [None, None]\n        x['ind'],b = nearestf.apply (*args, 1 if self.training or not usey else 0, bad)\n        with torch.no_grad():\n            x['ind1'] = x['ind'].reshape(x['ind'].shape[0],-1)\n            ylo = gather(x, 'xlo'); yla = gather(x, 'xla')\n            C = distancesq(ylo.unsqueeze(-2), yla.unsqueeze(-2), ylo.unsqueeze(-1), yla.unsqueeze(-1))\n        return C,b\n\n    def forward (self, x):        \n        self.dist.kf.train(self.training)\n        usey = 'ylo' in x\n        bad = torch.isnan(x['xval']).all(-1)\n        self.mappingx(x, 'x')\n        w = nn.functional.softmax(torch.arange(0,-self.points//2,-1,dtype=torch.float32,device=x['xval'].device),dim=-1)\n        if self.usee:\n            C,b = self.distmatr (x, bad, False, usey)\n            xg = gather(x, 'xval')\n            Cb = torch.cat((C,b.unsqueeze(-1)),-1)\n            x['yval'] = self.enet (Cb/(b[...,:w.shape[0]]*w).sum(-1,keepdim=True)[...,None] if self.relativer else Cb, xg)\n            if self.enet.returnsd:\n                x['yval'],x['esd'] = x['yval']\n        if usey or self.usee:\n            if not usey:\n                x['yinput'] = x['xinput']\n            self.mappingx(x, 'y')\n        else:\n            for key in self.outs:\n                x['y'+key] = x['x'+key]\n        \n        reselect = self.reselect # and not self.training\n        if reselect:      \n            bad = (~x['xok']) if self.usee else (~x['xok']) | torch.isnan(x['xval']).any(-1)\n            C,b = self.distmatr (x, bad, True, usey)\n        if x['xmap'] is not None:\n            xmap = gather (x, 'xmap')\n            if not reselect and b is not None:\n                b = distsq.apply(x['ymap'].unsqueeze(-2), xmap).add_(b)\n            C.add_(distsq.apply(xmap.unsqueeze(-3), xmap.unsqueeze(-2)))\n\n        x['xn'] = torch.nan_to_num(x['xval'], 0.)\n        if self.training:\n            if self.biased:\n                x['xn'] = x['xn']-x['xbias']\n            if self.sigmed:\n                x['xn'] = x['xn']/x['xsigma']\n        else:\n            if self.biased:\n                x['xn'].sub_(x['xbias'])\n            if self.sigmed:\n                x['xn'].div_(x['xsigma'])\n        x['xn'][bad] = np.nan\n        xg = gather (x, 'xn')\n        Cb = torch.cat((C,b.unsqueeze(-1)),-1)\n        yn = self.dist (Cb/(b[...,:w.shape[0]]*w).sum(-1,keepdim=True)[...,None] if self.relativer else Cb, xg, None)\n        if self.dist.returnsd:\n            yn,x['sd'] = yn        \n        if self.training:\n            if self.sigmed:\n                yn = yn*x['ysigma']\n                if self.dist.returnsd:\n                    x['sd'] = x['sd']*x['ysigma']\n        else:\n            if self.sigmed:\n                yn.mul_(x['ysigma'])\n                if self.dist.returnsd:\n                    # x['sd'].mul_(x['ysigma'])\n                    x['sd'] = x['sd']*x['ysigma']\n        if self.biased:\n            yn.add_(x['ybias'])\n        return yn\n    def loss(self, x, result, lab=None, lossfun='mse'):\n        dims = list(range(result.ndim-1))\n        loss = torch.zeros_like(result)\n        if lab is None:\n            lab = 'train' if self.training else 'test'\n        if lab+'_'+lossfun not in self.history:\n            self.history[lab+'_'+lossfun] = []\n        ok = torch.isfinite(x['target']) & torch.isfinite(result)\n        cnt = ok.sum(dims).float()\n        if lossfun=='hyber':\n            loss[ok] = nn.functional.smooth_l1_loss (x['target'][ok], result[ok], reduction='none')\n        elif lossfun=='mse':\n            loss[ok] = nn.functional.mse_loss (x['target'][ok], result[ok], reduction='none')\n        elif lossfun=='smape':\n            loss[ok] = 2.*nn.functional.mse_loss (x['target'][ok], result[ok], reduction='none')/(x['target'][ok] + result[ok]).clamp_(min=2.)\n        self.history[lab+'_'+lossfun].append ((loss.sum()/cnt.sum()).item())\n        # printshape(x)\n        # print(result.shape)\n        if self.training and self.gradloss > 0.:\n            g = x['ysigma'][ok[...,0]]\n            e = torch.ones_like(g, requires_grad=True)\n            g = torch.autograd.grad((g*e).sum(), x['yval'], create_graph=True, retain_graph=True)[0]\n            g = torch.autograd.grad(-g.clamp(max=0.).sum(), e, create_graph=True, retain_graph=True)[0]\n            g = (g*g).sum(-1)\n            loss[ok] += g*self.gradloss\n            \n            if self.embedding > 0:\n                g = x['ysigma'][ok[...,0]]\n                e = torch.ones_like(g, requires_grad=True)\n                g = torch.autograd.grad((g*e).sum(), x['yemba'], create_graph=True, retain_graph=True)[0]\n                g = torch.autograd.grad(-g.clamp(max=0.).sum(), e, create_graph=True, retain_graph=True)[0]\n                g = (g*g).sum(-1)\n                loss[ok] += g*self.gradloss\n            # print([g.mean(), g.max()])\n        if self.dist.returnsd and 'sd' in x:# and lossfun != 'mse':\n            ok = torch.isfinite(x['target']) & torch.isfinite(x['sd']) & torch.isfinite(result) & (x['sd'] > 0.)\n            pred = x['target'][ok] - result[ok]; sd = x.pop('sd')[ok].clamp(min=0.2,max=50.)\n            # print(sd.min())\n            nrm = (pred/sd).mul_(np.sqrt(0.5)).clamp(min=-6.,max=6.)\n            # loss[ok] = (pred*torch.erf(nrm)).add_((torch.exp(-nrm*nrm-0.5*np.log(2.))-(sd/x['ysigma'][ok] if self.sigmed else sd)).mul_(np.sqrt(1./np.pi)))\n            nrm = torch.exp(-nrm)*sd\n            loss[ok] = ((pred+nrm)*(pred>0)+(sd+nrm)*0.5*(pred<0)).div_(np.sqrt(0.5))\n            if self.usee and self.enet.returnsd and 'esd' in x and 'yval' in x:\n                ok1 = torch.isfinite(x['target']) & torch.isfinite(x['esd']) & torch.isfinite(x['yval']) & (x['esd'] > 0.)\n                pred = x['target'][ok1] - x['yval'][ok1]; sd = x.pop('esd')[ok1].clamp(min=0.2,max=50.)\n                # print(sd.min())\n                nrm = (pred/sd).mul_(np.sqrt(0.5)).clamp(min=-6.,max=6.)\n                # loss[ok1] = loss[ok1] + 0.2*((pred*torch.erf(nrm)).add_((torch.exp(-nrm*nrm-0.5*np.log(2.))-(sd)).mul_(np.sqrt(1./np.pi))))\n                nrm = torch.exp(-nrm)*sd\n                loss[ok1] = loss[ok1] + ((pred+nrm)*(pred>0)+(sd+nrm)*0.5*(pred<0)).div_(5.*np.sqrt(0.5))\n                cnt += 0.2*ok1.sum(dims).float()\n            if lab+'_sd' not in self.history:\n                self.history[lab+'_sd'] = []\n            self.history[lab+'_sd'].append ((loss.sum()/cnt.sum()).item())\n        if lab+'_n' not in self.history:\n            self.history[lab+'_n'] = []\n        self.history[lab+'_n'].append (cnt.sum().item())\n        return loss.sum(dims), cnt\n\nModel0 = DLOA(0,1,0,implicit=False,edging=True,rmax=1.,usee=False,biased=False,relativer=True).eval()",
  "history_output" : "",
  "history_begin_time" : 1668624365895,
  "history_end_time" : 1668624369757,
  "history_notes" : null,
  "history_process" : "fva4b9",
  "host_id" : "100001",
  "indicator" : "Done"
},]
