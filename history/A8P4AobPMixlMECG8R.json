[{
  "history_id" : "4sk2vrlluhj",
  "history_input" : "from argparse import ArgumentParser\nimport copy\nfrom datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport logging\nimport pandas as pd\nfrom os import path,listdir,makedirs\nimport matplotlib.pyplot as plt\nfrom snowcast_tictoc import TicTocGenerator\nfrom threading import Thread, Lock\nimport torch\nfrom traceback import format_exc\nimport features_init as features\n# import visualization\nimport models_init as models\nfrom models_init import addons\n\nawsurl = 'https://drivendata-public-assets.s3.amazonaws.com/'\n\nparser = ArgumentParser(description='Snowcast Showdown solution (c) FBykov')\nparser.add_argument('--mode', default = 'oper', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'train', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'finalize', type=str, help='mode: train/oper/test/finalize')\n# parser.add_argument('--mode', default = 'test', type=str, help='mode: train/oper/test/finalize')\nparser.add_argument('--maindir', default = '..', type=str, help='path to main directory')\n\n#### Options for training - not significant for inference ####\nparser.add_argument('--implicit', default = 0, type=int, help='implicit')\nparser.add_argument('--individual', default = 0, type=int, help='individual')\nparser.add_argument('--relativer', default = 0, type=int, help='relativer')\nparser.add_argument('--norm', default = 0, type=int, help='normalization')\nparser.add_argument('--springfine', default = 0, type=int, help='fine on spring')\nparser.add_argument('--stack', default = 0, type=int, help='stack')\nparser.add_argument('--calibr', default = 0, type=int, help='calibration')\nparser.add_argument('--embedding', default = 0, type=int, help='embedding size')\nparser.add_argument('--lossfun', default = 'smape', type=str, help='loss function')\nparser.add_argument('--modelsize', default = '', type=str, help='modelsize')\n#### Options for training - not significant for inference ####\n\ntry:\n    args = parser.parse_args()\n    args.notebook = False\nexcept:\n    args = parser.parse_args(args=[])\n    args.notebook = True\nargs = args.__dict__\n      \nmaindir = args['maindir']\ndatestr = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\nlogdir = path.join (maindir, 'logs')\nmakedirs (logdir,exist_ok=True)\nlogfile = path.join (logdir, datestr+'.log')\nlogging.basicConfig (level=logging.INFO, format=\"%(asctime)s %(message)s\",\n                     handlers=[logging.FileHandler(logfile), logging.StreamHandler()])\nprint = logging.info\nprint(f\"Logging to {logfile}\")\n\nprint(f\"Starting mode={args['mode']} maindir={maindir} args={args}\")\nworkdir = path.join(maindir,'data','evaluation' if args['mode'] == 'oper' else 'development')\nmakedirs (workdir,exist_ok=True)\nprint(f\"workdir={workdir} list={listdir(workdir)}\")\n\nmodelsdir = path.join(maindir,'models')\nmaindevice = torch.device('cpu')   \nwithpred = True\nnmonths = 1\nprint(f\"arxiv on {maindevice} calc on {models.calcdevice}\")\ninputs, arglist, uregions, stswe, gridswe, days, dates, nstations = \\\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n                       maindevice=maindevice, awsurl=awsurl, print=print)\n\ngc.collect()\nif torch.cuda.is_available():\n    print ([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nnousest='isemb' not in arglist\nmodelslist={}\nif args['mode'] in ['oper','test']:\n    models.apply.valid_bs = 8\n    lab0 = ('finalize' if args['mode'] in ['oper'] else 'train')\n    totalbest=[100.]*8\n    if args['mode'] in ['test']:\n        spring = np.array([d.month*100+d.day for d in dates['test']])\n        ok = torch.tensor(np.array([(d.year>=2021)|(d.month>10) for d in dates['test']]), device=maindevice)\n        ok = ok&torch.tensor((spring>=215)&(spring<=701), device=maindevice)\n        for key in inputs['test']:\n            inputs['test'][key] = inputs['test'][key][ok]\n        ok = ok.nonzero().cpu().numpy()[:,0]\n        dates['test'] = [dates['test'][i] for i in ok]\n        days['test'] = [days['test'][i] for i in ok]\n    # for lab in [ '_ex_ind', '_imp_ind' ]:\n    # for lab in [ '_ex_ind', 'a_ex_ind', '_imp_ind', 'a_imp_ind' ]: # 14\n    # for lab in [ '_ex_ind_noemb', '_imp_ind_noemb', '_ex_com_noemb', '_imp_com_noemb',\n    #              '_ex_ind', '_imp_ind', '_ex_com', '_imp_com']: #15 v1\n    for lab in [ '_ex_ind_noemb', '_imp_com_noemb', '_ex_ind', '_imp_com']: #15 v2\n        for big in ['B', '']:\n            file = lab0+big+'_smapea'+lab\n            try:\n                models1 = models.loadmodels(file, inputs, modelsdir, arglist, print=None)\n                print(f\"Loaded {file} {len(models1)} models from {modelsdir}\")\n            except:\n                print(f\"Cannot load models {file}\")\n                continue\n            if len(models1) == 0:\n                print(f\"Cannot load models {file}\")\n                continue\n            for s in models1:\n                if (big != 'B') or s not in [7,8]:\n                    modelslist[len(modelslist)] = models1[s]\n            # if args['mode'] in ['test']:\n            #     for s in models1:\n            #         r = models.inference (inputs['test'], {0: models1[s]}, dates['test'], lab=lab+str(s), print=print, nousest=nousest)\n            #     r = models.inference (inputs['test'], models1, dates['test'], lab=lab, print=print, nousest=nousest)\n    result = models.inference (inputs['test'], modelslist, dates['test'], print=print, nousest=nousest)\n            \n    if args['mode'] in ['oper']:        \n        iweek = torch.isfinite(result).any(1).nonzero()[-1].item()+1\n        evalday = days['test'][iweek-1]\n        out = {tday: result[i] for i,tday in enumerate(days['test'])}\n        out['cell_id'] = gridswe['test'].index\n        sub = pd.DataFrame(out).set_index('cell_id')\n        file = path.join(evalday,'submission'+datestr+'.csv')\n        fname = path.join(workdir,file)\n        subdir = path.join(workdir,evalday)\n        makedirs (subdir,exist_ok=True)\n        print('Saving submission to '+fname)\n        sub.to_csv(fname)\n        print ('Mean values: ' + str({key: np.round(sub[key].mean(),4) for key in sub}))\n        try:\n            isubs = pd.DataFrame({'week': [iweek], 'day': [evalday], 'file': [file]})\n            subslistfile = path.join(workdir,'submissionslist.csv')            \n            if path.isfile(subslistfile):\n                subslist = pd.read_csv(subslistfile)\n                pd.concat((subslist.set_index(subslist.columns[0]), isubs)).to_csv(subslistfile)\n                isst = (inputs['test']['yemb'][0]>0).cpu().numpy()\n                for file in subslist['file']:\n                    try:\n                        old = pd.read_csv(path.join(workdir,file)).set_index('cell_id')\n                        print (f'Compare with {file}\\n'+\n                               str({key: [np.round((sub[key]-old[key])[isst].mean(),4), np.round(np.abs(sub[key]-old[key])[isst].mean(),4),\n                                          np.round((sub[key]-old[key])[~isst].mean(),4), np.round(np.abs(sub[key]-old[key])[~isst].mean(),4)]\n                                for key in old if np.isfinite(sub[key]).sum()>0}))\n                    except:\n                        print(format_exc())\n            else:\n                isubs.to_csv(subslistfile)\n            from shutil import copyfile\n            copyfile(path.join(workdir,'ground_measures_features.csv'), path.join(subdir,'ground_measures_features.csv'))\n            copyfile(logfile, path.join(subdir,datestr+'.log'))\n        except:\n            print(format_exc())\n    if args['mode'] in ['test']:\n        figdir = path.join (maindir, 'reports', 'figures')\n        makedirs (figdir,exist_ok=True)\n        results = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n        # monests = visualization.monthests (inputs, dates, uregions, result)\n        visualization.temporal(inputs['test'], dates['test'], results, result, uregions, figdir)\n        visualization.spatial(inputs['test'], dates['test'], result, figdir)\n        visualization.importance(inputs['test'], dates['test'], modelslist, result, arglist, nousest, figdir)\n        visualization.latentsd(inputs['test'], days['test'], 8, modelslist[1], figdir)\n\nelif 'train' in inputs:\n    for key in ['relativer','implicit','individual','norm','springfine']:\n        args[key] = args[key]>0.5\n    inputs['train']['isstation'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0)>10        \n    # visualization.boxplot(inputs,arglist)\n    splits = np.array([True]+[dates['train'][i+1]-dates['train'][i]>timedelta(days=100) for i in range(len(dates['train'])-1)]+[True]).nonzero()[0]\n    folds = list(range(1,len(splits)))\n    totalbest = [100.]*max(folds)    \n    devices = [torch.device('cuda:'+str(igpu)) for igpu in range(torch.cuda.device_count())]\n    lock = Lock()\n    prefix = args['mode']+args['modelsize'][:1]+'_'+args['lossfun']+('' if args['relativer'] else 'a')+('_imp' if args['implicit'] else '_ex')+\\\n            ('_ind' if args['individual'] else '_com')+('_n' if args['norm'] else '')+(f\"_s{args['stack']}\" if args['stack']>0 else '')+\\\n            ('_c' if args['calibr'] else '')+('_noemb' if args['embedding']==0 else '')+('_s' if args['springfine'] else '')\n    # prefix = datestr\n    \n    def getinitemb(inputs, select, nstations, norm=True):\n        pwr=0.6\n        xv = inputs['xval'][select,:,0]\n        device = xv.device\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            nrm = (xv.sum(1,keepdim=True)/(xv>0).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            # nrm = (xv.sum(1,keepdim=True)/ok.float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n            xv = xv/nrm\n        initemb = torch.zeros(nstations*nmonths, device=device)\n        count   = torch.zeros(nstations*nmonths, device=device)\n        initemb.scatter_add_(0, inputs['xemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['xemb'][select].reshape(-1), ok.reshape(-1).float())\n        xv = inputs['yval'][select,:,0]\n        ok = torch.isfinite(xv)\n        xv = torch.nan_to_num(xv,0.)**pwr #; ok = xv>0\n        if norm:\n            xv = xv/nrm\n        initemb.scatter_add_(0, inputs['yemb'][select].reshape(-1), xv.reshape(-1))\n        count.scatter_add_(0, inputs['yemb'][select].reshape(-1), ok.reshape(-1).float())        \n        initemb.div_(count.clamp(min=1.))\n        me = initemb[count>10].mean()\n        initemb[count<=10] = me\n        return (initemb-me)*0.2\n    \n    valid_bs = models.valid_bs\n    def trainnew (inputs, modelslist, netarg,\n                  lab='', seed=0, folds=1, lossfun = 'mse', lossval = 'mse', autoepochs=10, onlyauto=False, schedulerstep=2., lenplateau=2, igpu=0, springfine=True,\n                  lr=0.01, weight_decay=0.001, bs=3, model3=1, freeze_ep=2, epochs=50, initdecay=0.8, L1=0., best=[100.], prune=1, pruneval=0.05, points0=3, fine=False):\n        kw = copy.deepcopy(netarg)\n        device = devices[igpu]\n        arxdevice = inputs['train']['xlo'].device\n        print(lab+f\" arxiv on {arxdevice} calc on {device} save to {prefix}\")\n        TicToc = TicTocGenerator()\n        mode = 'train'\n        selecty= inputs[mode]['isstation'].to(device)\n        if len(best) < max(folds):\n            best = best*max(folds)\n        for fold in folds:\n            gc.collect()\n            torch.cuda.empty_cache()\n            if len(folds) > 1:\n                subset = torch.cat((torch.arange(splits[fold-1], device=arxdevice),torch.arange(splits[fold], inputs[mode]['xinput'].shape[0], device=arxdevice)))\n                if fine:\n                    spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                    subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                valset = torch.arange(splits[fold-1], splits[fold], device=arxdevice)\n                spring = np.array([d.month*100+d.day for d in dates[mode][splits[fold-1]:splits[fold]]])\n                valset = valset[torch.tensor((spring>=215)&(spring<=701),device=valset.device)]\n                if ((torch.isfinite(inputs[mode]['xinput'][valset]).all(-1).sum(1)>kw['points']).sum() == 0) or \\\n                   ((torch.isfinite(inputs[mode]['xinput'][subset]).all(-1).sum(1)>kw['points']).sum() == 0):\n                    continue\n            better = False\n            addons.set_seed(fold+int(100.*(kw['rmaxe']*(1. if kw['rmax'] is None else kw['rmax'])))+seed*1000)\n            print(kw)            \n            batches = int(np.ceil(inputs['train']['xinput'].shape[0]/bs))\n            if fine:\n                m = modelslist[fold-1].to(device=device)\n                models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_00', smart=True, device=device, print=print, nousest=nousest)\n            else:\n                netargin = [inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1]]\n                m = models.Model(*netargin, initemb=getinitemb(inputs[mode], subset, netarg['nstations']), **kw).to(device=device)\n                # if model3 > 1:\n                #     m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n            decay = min(0.9,max(weight_decay,(1.0-np.power(initdecay, 1./batches/freeze_ep))/lr))\n            print(lab+f' initdecay={decay} inputs={inputs[mode][\"xinput\"].shape[-1]} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n            getoptimizer = lambda m, lr=lr, warmup=5: addons.AdamW(m.netparams(), weight_decay=decay, lr=lr, warmup=5, belief=True, gc=0., amsgrad=True, L1=L1)\n            getkfoptimizer = lambda m, lr=lr: addons.AdamW(m.kfparams(), weight_decay=0., lr=lr, warmup=0, belief=False, gc=0., amsgrad=True)\n            getscheduler = lambda optimizer: torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: max(0.1,1.0/(1.0 + step/schedulerstep)) )\n            optimizer = getoptimizer (m)\n            kfoptimizer = getkfoptimizer (m, lr=0.1*lr)\n            scheduler = getscheduler (optimizer)\n                    \n            next(TicToc)\n            cpoints = points0\n            plateau = 0\n            if prune > 0:\n                nzero = m.nonzero()\n                if fine:\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n            for epoch in range(epochs):                \n                # frozen = min(2,(2*epoch)//autoepochs)\n                frozen = min(2,epoch//freeze_ep)\n                m.freeze(frozen)\n                if epoch==freeze_ep and not fine:\n                    for group in optimizer.param_groups:\n                        group['weight_decay'] = weight_decay\n                    if springfine:                        \n                        spring = np.array([dates[mode][i].month*100+dates[mode][i].day for i in subset.cpu().numpy()])\n                        subset = subset[torch.tensor((spring>=215)&(spring<=701),device=subset.device)]\n                perm = subset[torch.randperm(subset.shape[0], device=arxdevice)]\n                batches = int(np.ceil(perm.shape[0]/bs))\n                if epoch > 0 and epoch*prune//freeze_ep <= prune and (epoch*prune)%freeze_ep < prune:\n                    msg = ''\n                    for p in nzero:\n                        w = torch.abs (p.data)\n                        nzero[p] = w > torch.maximum(w.max(0,keepdim=True)[0],w.max(1,keepdim=True)[0]).clamp_(max=1.)*pruneval\n                        p.data.mul_(nzero[p])\n                        if 'exp_avg' in optimizer.state[p]:\n                            optimizer.state[p]['exp_avg'].mul_(nzero[p])\n                        msg += f'{nzero[p].sum()}/{nzero[p].numel()} '\n                    print(lab+' Nonzero grad '+msg)\n                prune1 = prune if prune>0 else 3\n                if model3>1 and epoch*prune1//freeze_ep == 1 and (epoch*prune1)%freeze_ep < prune1 and not fine:\n                # if model3>1 and epoch == 0 and not fine:\n                    m = models.Model3(m.state_dict(), *netargin, layers=model3, **kw).to(device=device)\n                    if prune > 0:\n                        nzero = m.nonzero (nzero)\n                    print(lab+f' Model3 style={m.style} parameters={sum([p.numel() for p in m.parameters() if p.requires_grad])}')\n                    optimizer = getoptimizer (m, lr=0.5*lr, warmup=batches//2)\n                    kfoptimizer = getkfoptimizer (m, lr=0.5*lr)\n                    scheduler = getscheduler (optimizer)\n                    cpoints = points0\n                if epoch == autoepochs and not onlyauto:\n                    for opt in [optimizer, kfoptimizer]:\n                        for p in opt.state:                                \n                            if 'exp_avg' in opt.state[p]:\n                                # opt.state[p]['step'] = 0\n                                opt.state[p]['exp_avg'].fill_(0.)\n                                # opt.state[p]['exp_avg_sq'].fill_(0.)\n                    for group in optimizer.param_groups:\n                        group['lr'] = lr\n                        # group['L1'] /= 5.\n                    scheduler = getscheduler(optimizer)\n                    cpoints = points0+2\n                sloss = 0.; cnts = 0.\n                m.train()\n                for i in range (batches):\n                    m.points = cpoints+np.random.randint(-1,2)\n                    sel = perm[i*bs:min((i+1)*bs,perm.shape[0])]\n                    optimizer.zero_grad()\n                    kfoptimizer.zero_grad()\n                    res,loss,cnt = models.apply(m, inputs[mode], sel, device=device, \n                                                autoloss=(epoch<autoepochs) or onlyauto, lab=mode, lossfun=lossfun, selecty=selecty)\n                    sloss += loss.item(); cnts += cnt.item()\n                    (loss/cnt).mean().backward()\n                    if epoch*prune1 >= freeze_ep:\n                    # if epoch >= freeze_ep:\n                        kfoptimizer.step()\n                    if prune > 0 and epoch*max(1,prune) >= freeze_ep:\n                        for p in nzero:\n                            p.grad.data.mul_(nzero[p])\n                    optimizer.step()\n                sloss = np.sqrt(sloss/cnts) if lossfun=='mse' else sloss/cnts\n                out = f\"train={sloss:.4}\"\n                if len(folds) > 1:\n                    m.eval()\n                    m.points = kw['points']\n                    batches = int(np.ceil(valset.shape[0]/valid_bs))\n                    sloss = 0.; cnts = 0.\n                    with torch.no_grad():\n                        for i in range (batches):                                \n                            sel = valset[torch.arange(i*valid_bs, min((i+1)*valid_bs,valset.shape[0]), device=arxdevice)]                                \n                            res,loss,cnt = models.apply(m, inputs['train'], sel, device=device, lab='valid', lossfun=lossval, selecty=selecty)\n                            sloss += loss.item(); cnts += cnt.item()\n                    val = np.sqrt(sloss/cnts) if lossval=='mse' else sloss/cnts\n                    out = f\"valid={val:.4} \" + out\n                    if val < best[fold-1]:\n                        best[fold-1] = val\n                        best_weights = m.state_dict()\n                        best_weights = {key: best_weights[key].clone() for key in best_weights}\n                                                \n                        plateau = 0\n                        if epoch > autoepochs:\n                            better = True\n                        out = ('I best ' if totalbest[fold-1] > best[fold-1] else 'better ')+out\n                        # if epoch*max(1,prune) >= autoepochs and 'test' in inputs:\n#                        if (epoch >= freeze_ep or fine) and 'test' in inputs:\n#                            models.inference (inputs['test'], {0:m}, dates['test'], lab=lab+f' y={fold-1}_{epoch}', device=device, print=print, nousest=nousest)\n                    else:\n                        out = '       '+out\n                        plateau += 1\n                        if plateau >= lenplateau:\n                            if plateau > 20 and epoch > 2*autoepochs:\n                                break\n                            scheduler.step()\n                            if plateau % 2 == 0:\n                                cpoints = min(cpoints+1,kw['points'])\n                print(lab + f\" {epoch} \" + out + f\" {next(TicToc):.3}s lr={optimizer.param_groups[0]['lr']:.4}\" + m.prints())\n            if better:\n                m.eval()\n                m.points = kw['points']\n                m.load_state_dict(best_weights)\n                print(m.importance(arglist))\n                if 'test' in inputs:\n                    if 'target' in inputs['test']:\n                        models.test(inputs, {0:m}, dates, lab, f' y={fold-1}', device=device, print=print, nousest=nousest)\n                if args['mode'] == 'train' or epoch >= freeze_ep:\n                    with lock:\n                        if totalbest[fold-1] > best[fold-1] or (fold-1 not in modelslist):\n                            # m.dist.graph()\n                            best_weights.update(kw)\n                            best_weights['best'] = best[fold-1]                            \n                            torch.save(best_weights, path.join(modelsdir, prefix+'_'+str(fold-1)+'.pt'))\n                            print(f'Copy {lab} to modelslist {fold-1} loss={best[fold-1]}'+m.prints())\n                            modelslist[fold-1] = m\n                            totalbest[fold-1] = best[fold-1]\n        print(lab+f'\\n {kw} \\n best={best}')\n        if 'test' in inputs:\n            if 'target' in inputs['test']:\n                models.test(inputs, modelslist, dates, lab, device=device, print=print, nousest=nousest)\n        return modelslist\n    \n    netarg = {'usee': True, 'biased': False, 'sigmed': True, 'densed': True, 'implicit': args['implicit'],\n              'edging': True,  'initgamma': None, 'norm': args['norm'], 'eps': -1.3, 'individual': args['individual'],\n              'nf': [24], 'nlatent': 3,\n#               'nf': [32], 'nlatent': 5,\n              'embedding': args['embedding'], 'nmonths': nmonths, 'points': 20, 'gradloss': 0., 'style': 'stack',\n              'nstations': nstations, 'dropout': 0.2, 'calcsd': 0, 'mc': 3,\n              'rmaxe': 0.9, 'rmax': 0.7, 'rmax2': 0.5, 'relativer': args['relativer'], 'commonnet': True, 'calibr': args['calibr'] }\n    ep = int(np.log (len(days['train'])/8))\n    trarg = {'weight_decay': 0.01, 'L1': 0.01, 'initdecay': 0.35, 'igpu': 0,\n              'best': [100.], 'lab': '', 'pruneval': 0.05, 'folds': folds, 'onlyauto': False,\n              'lossfun': 'smape', 'lossval': 'mse', 'fine': False, 'springfine': args['springfine'],\n              'lr': 0.01, 'bs': 2*ep*ep, 'freeze_ep': 10*ep, 'autoepochs': 15*ep,\n              'lenplateau': 1, 'schedulerstep': 4.*ep, 'epochs': 70*ep, 'prune': ep, 'model3': 1+args['stack']}\n    if netarg['mc']>1 and trarg['model3']>1:\n        trarg['bs'] //= 2\n        trarg['lr'] /= 2\n    if True:\n        threads=[None]*len(devices)\n        igpu = 0        \n        m3 = 1 #+withpred\n        idec = 0.35\n        for seed in range(3):\n            for rmax in [0.9,0.7]:\n#            for idec in [0.7,0.9,0.5,0.35]:\n#                rmax = 0.8\n                    rmaxe = np.round(rmax*13.)/10\n                    nlatent = 3 if netarg['embedding']>0 else 5\n                    if args['modelsize'][:1] == 'B':\n                        nf = [48 if netarg['embedding']>0 else 96]\n                    else:\n                        nf = [24 if netarg['embedding']>0 else 48]\n                # for nlatent,nf in zip([3 if netarg['embedding']>0 else 5],[[24 if netarg['embedding']>0 else 48]]):\n#                for nlatent,nf in zip([5,3,2,5,3,2,3,2],[[16],[16],[16],[24],[24],[24],[32],[32]]):\n                    trarg1 = copy.deepcopy(trarg)\n                    netarg1 = copy.deepcopy(netarg)\n                    igpu = (igpu+1)%len(devices)\n                    netarg1.update ({'rmax': rmax, 'rmax2': rmax, 'rmaxe': rmaxe, 'nf': nf, 'nlatent': nlatent})\n                    trarg1.update ({'lab': f\"r={rmax}\"+('r' if netarg1['relativer'] else '')+f\",{rmaxe}nf={nf}nl={nlatent}_{args['lossfun']} \"+\n                                    ('imp' if netarg1['implicit'] else 'ex')+('n' if netarg1['norm'] else ' ')+f' m3={m3} idec={idec}',\n                                    'igpu': igpu, 'lossfun': args['lossfun'], 'seed': seed, 'model3': m3, 'initdecay': idec})\n                    if len(devices) > 1:\n                        if threads[igpu] is not None:\n                            threads[igpu].join()\n                        print(trarg1)\n                        threads[igpu] = Thread(target=trainnew, args=(inputs, modelslist, netarg1), kwargs=trarg1)\n                        threads[igpu].start()\n                    else:\n                        print(trarg1)\n                        modelslist = trainnew(inputs, modelslist, netarg1, **trarg1)\n        for thread in threads:\n            thread.join()        \n    modelslist = models.loadmodels(prefix, inputs, modelsdir, arglist)\n    \n    print('valid='+np.array2string(np.array(totalbest)))\n    imp = [modelslist[m].importance(arglist) for m in modelslist]\n    for key in imp[0]:\n        print({key: list([i[key] for i in imp])})\n    if 'test' in inputs:\n        r = models.test(inputs, modelslist, dates, print=print, nousest=nousest)",
  "history_output" : "2022-11-16 13:45:51,536 Logging to ../logs/2022-11-16_13_45_51.log\n2022-11-16 13:45:51,537 Starting mode=oper maindir=.. args={'mode': 'oper', 'maindir': '..', 'implicit': 0, 'individual': 0, 'relativer': 0, 'norm': 0, 'springfine': 0, 'stack': 0, 'calibr': 0, 'embedding': 0, 'lossfun': 'smape', 'modelsize': '', 'notebook': False}\n2022-11-16 13:45:51,537 workdir=../data/evaluation list=[]\n2022-11-16 13:45:51,537 arxiv on cpu calc on cpu\n2022-11-16 13:45:51,538 getconstfeatures: datadir=../data/evaluation/.. list=['Autoencoder', '.DS_Store', 'features', 'evaluation', 'processed', 'lightning_logs', 'raw']\n2022-11-16 13:45:51,538 Loading ../data/evaluation/grid_cells.geojson\ndata\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\nTraceback (most recent call last):\n  File \"snowcast_main.py\", line 70, in <module>\n    features.getinputs(workdir, args['mode'], modelsdir, withpred=withpred, nmonths=nmonths, \n  File \"/Users/uhhmed/gw-workspace/4sk2vrlluhj/features_inputs.py\", line 16, in getinputs\n    stmeta,grid,constfeatures = getconstfeatures(workdir, uregions, awsurl = awsurl, print=print)\n  File \"/Users/uhhmed/gw-workspace/4sk2vrlluhj/features_constFeatures.py\", line 26, in getconstfeatures\n    with open(file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/evaluation/grid_cells.geojson'\n",
  "history_begin_time" : 1668624343179,
  "history_end_time" : 1668624351938,
  "history_notes" : null,
  "history_process" : "9b0289",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "2lq447wjlz2",
  "history_input" : "import time    \n\ndef TicTocGenerator():\n        ti = 0           # initial time\n        tf = time.time() # final time\n        while True:\n            ti = tf\n            tf = time.time()\n            yield tf-ti # returns the time difference    \nTicToc = TicTocGenerator() # create an instance of the TicTocGen generator",
  "history_output" : "",
  "history_begin_time" : 1668624353306,
  "history_end_time" : 1668624353985,
  "history_notes" : null,
  "history_process" : "zkxdmp",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hd8a6d12dju",
  "history_input" : "import cv2\nimport gc\nfrom geotiff import GeoTiff\nimport numpy as np\nfrom os import path,makedirs\nimport matplotlib.pyplot as plt\nimport planetary_computer\nfrom pystac_client import Client\nfrom scipy.io import savemat\nimport wget\n\ndef getdem (lat1=31.,lat2=51.,lon1=-126.,lon2=-102.,dir = 'dem', matfile='dem.mat'):\n    makedirs(dir,exist_ok=True)\n    client = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",\n                          ignore_conformance=True)\n    cosd = lambda x: np.cos(np.radians(x))\n    \n    sz = 1200\n    dsm = []\n    for lon in np.arange(lon1+0.5,lon2):\n        col = []\n        for lat in np.arange(lat1+0.5,lat2):\n            file = path.join(dir, f'Copernicus_DSM_COG_30_N{int(lat)}_00_W{int(1-lon)}_00_DEM.tif')            \n            if not path.isfile(file):\n                search = client.search(collections=[\"cop-dem-glo-90\"],\n                                        intersects={\"type\": \"Point\", \"coordinates\": [lon, lat]})\n                items = list(search.get_items())        \n                if len(items)>0:            \n                    print(f\"Returned {len(items)} items\")\n                    signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n                    fname = wget.download(signed_asset.href, dir)\n                else:\n                    print(\"Not found \"+file)\n            if path.isfile(file):\n                file = GeoTiff(file)\n                arr = np.array(file.read())[-1::-1]\n                print([file.get_coords(0,0),file.get_coords(arr.shape[0]-1,arr.shape[1]-1)])\n                if (arr.shape[1] != sz) or (arr.shape[0] != sz):\n                   arr =  cv2.resize(arr,(sz,sz))\n                # plt.imshow(arr)\n            else:\n                arr = np.zeros((sz,sz))\n            col.append(arr)\n        dsm.append (np.concatenate(col,axis=0))\n    dsm = np.concatenate(dsm,axis=1)\n    del col,arr\n    gc.collect()\n    \n    rm = 2.\n    r = 5\n    sh = (r-1)//2\n    av = dsm.reshape(dsm.shape[0]//r,r,-1,r).mean(-1).mean(1)\n    x = dsm[1:]-dsm[:-1]\n    south = x.copy()\n    cv2.GaussianBlur(x[:,1:]+x[:,:-1], (2*int(rm*r)+1, 2*int(rm*r)+1), r, south, r)\n    south = south[sh::r,sh::r]\n    \n    x = np.abs(x)\n    x = 0.5*(x[:,1:]+x[:,:-1])\n    y = dsm[:,1:]-dsm[:,:-1]\n    del dsm\n    gc.collect()\n    y /= cosd(np.arange(lat1,lat2,1/sz))[:,None]\n    east = y.copy()\n    cv2.GaussianBlur(y[1:]+y[:-1], (2*int(rm*r)+1, 2*int(rm*r)+1), r, east, r)\n    east = east[sh::r,sh::r]\n    y = np.abs(y)\n    y = 0.5*(y[1:]+y[:-1])    \n    aspect = np.hypot(x,y)\n    del x,y\n    gc.collect()\n    \n    avs = aspect.copy()\n    cv2.GaussianBlur(aspect, (2*int(rm*r)+1, 2*int(rm*r)+1), r, avs, r)\n    aspect = avs[sh::r,sh::r]\n    del avs\n    \n    lat = np.arange(lat1+(sh+0.5)/sz,lat2-sh/sz,r/sz)\n    lon = np.arange(lon1+(sh+0.5)/sz,lon2-sh/sz,r/sz)\n    \n    out = {'lat': lat, 'lon': lon, 'aspect': aspect, 'elev': av, 'south': south, 'east': east}\n    print (f'Saving DEM to {matfile}')\n    savemat(matfile, out, do_compression=True)\n    return out\n\nif __name__ == '__main__':\n    getdem()\n    ",
  "history_output" : "Not found dem/Copernicus_DSM_COG_30_N31_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N32_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N33_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N34_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N35_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N36_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N37_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N38_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N39_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N40_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N41_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N42_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N43_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N44_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N45_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N46_00_W126_00_DEM.tif\nNot found dem/Copernicus_DSM_COG_30_N47_00_W126_00_DEM.tif\nReturned 1 items\nTraceback (most recent call last):\n  File \"data_dem.py\", line 87, in <module>\n    getdem()\n  File \"data_dem.py\", line 36, in getdem\n    arr = np.array(file.read())[-1::-1]\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 589, in __array__\n    a = self[...]\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 807, in __getitem__\n    result = self.get_basic_selection(pure_selection, fields=fields)\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 933, in get_basic_selection\n    return self._get_basic_selection_nd(selection=selection, out=out,\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 976, in _get_basic_selection_nd\n    return self._get_selection(indexer=indexer, out=out, fields=fields)\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 1267, in _get_selection\n    self._chunk_getitem(chunk_coords, chunk_selection, out, out_selection,\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/core.py\", line 1966, in _chunk_getitem\n    cdata = self.chunk_store[ckey]\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/zarr/storage.py\", line 724, in __getitem__\n    return self._mutable_mapping[key]\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/tifffile/tifffile.py\", line 9985, in __getitem__\n    return self._getitem(key)\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/tifffile/tifffile.py\", line 10593, in _getitem\n    chunk = keyframe.decode(chunk_bytes, chunkindex, **decodeargs)[0]  # type: ignore\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/tifffile/tifffile.py\", line 6817, in decode_raise_predictor\n    raise ValueError(f'{exc}')\nValueError: <PREDICTOR.FLOATINGPOINT: 3> requires the 'imagecodecs' package\n",
  "history_begin_time" : 1668624373967,
  "history_end_time" : 1668624383869,
  "history_notes" : null,
  "history_process" : "d25i0q",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "h1noacik1nu",
  "history_input" : "from azure.storage.blob import ContainerClient\nimport cv2\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport os\nfrom pyhdf.SD import SD, SDC\nfrom pyproj import Proj,transform\nimport re\nfrom scipy.interpolate import interp2d\nimport wget\n\nclass ModisSource:\n    modis_account_name = 'modissa'\n    modis_container_name = 'modis-061'\n    modis_account_url = 'https://' + modis_account_name + '.blob.core.windows.net/'\n    modis_blob_root = modis_account_url + modis_container_name + '/'\n        \n#     temp_dir = os.environ['MODISPATH']\n    temp_dir = 'data/modis'\n    if not os.path.isdir(temp_dir):\n        import tempfile\n        temp_dir = os.path.join(tempfile.gettempdir(),'modis')\n        os.makedirs(temp_dir,exist_ok=True)\n    tempfiles = os.listdir(temp_dir)\n    \n    print(f\"data\\\\modis.py: Modis dir is {temp_dir} contain {len(tempfiles)} files\")\n\n    fname = 'sn_bound_10deg.txt'\n    fn = os.path.join(temp_dir, fname)\n    if not os.path.isfile(fn):\n        wget.download(modis_blob_root + fname, fn)\n    \n    # Load this file into a table, where each row is (v,h,lonmin,lonmax,latmin,latmax)\n    modis_tile_extents = np.genfromtxt(fn, skip_header = 7, skip_footer = 3)\n    modis_container_client = ContainerClient(account_url=modis_account_url, \n                                             container_name=modis_container_name,\n                                             credential=None)\n    \ndef lat_lon_to_modis_tiles(lat,lon):\n    ok = (lat >= ModisSource.modis_tile_extents[:, 4]) & (lat <= ModisSource.modis_tile_extents[:, 5]) \\\n         & (lon >= ModisSource.modis_tile_extents[:, 2]) & (lon <= ModisSource.modis_tile_extents[:, 3])\n    return ModisSource.modis_tile_extents[ok.nonzero()[0], 1::-1].astype(np.int64)\ndef lat_lon_to_modis_tile(lat,lon):\n    \"\"\"\n    Get the modis tile indices (h,v) for a given lat/lon    \n    https://www.earthdatascience.org/tutorials/convert-modis-tile-to-lat-lon/\n    \"\"\"    \n    ok = (lat >= ModisSource.modis_tile_extents[:, 4]) & (lat <= ModisSource.modis_tile_extents[:, 5]) \\\n         & (lon >= ModisSource.modis_tile_extents[:, 2]) & (lon <= ModisSource.modis_tile_extents[:, 3])\n    i = ok.nonzero()[0]\n    i = i[1] if len(i) >=3 else i[-1]\n    return int(ModisSource.modis_tile_extents[i, 1]),int(ModisSource.modis_tile_extents[i, 0])\n\ndef list_blobs_in_folder(container_name,folder_name):\n    generator = ModisSource.modis_container_client.list_blobs(name_starts_with=folder_name)\n    return [blob.name for blob in generator]\n            \ndef list_hdf_blobs_in_folder(container_name,folder_name):\n    files = list_blobs_in_folder(container_name,folder_name)\n    return [fn for fn in files if fn.endswith('.hdf')]\n\nul_regex = re.compile(r'''UpperLeftPointMtrs=\\(\n                          (?P<upper_left_x>[+-]?\\d+\\.\\d+)\n                          ,\n                          (?P<upper_left_y>[+-]?\\d+\\.\\d+)\n                          \\)''', re.VERBOSE)\nlr_regex = re.compile(r'''LowerRightMtrs=\\(\n                          (?P<lower_right_x>[+-]?\\d+\\.\\d+)\n                          ,\n                          (?P<lower_right_y>[+-]?\\d+\\.\\d+)\n                          \\)''', re.VERBOSE)\n\n# keys = ['NDSI_Snow_Cover', 'NDSI_Snow_Cover_Basic_QA', 'NDSI']\n# keys = ['NDSI_Snow_Cover', 'NDSI']\nkeys = ['NDSI_Snow_Cover']\nqakey = 'NDSI_Snow_Cover_Basic_QA'\n\ndef calibrate(hdf, keys=None):\n    ga = hdf.attributes()['StructMetadata.0']\n    match = ul_regex.search(ga)\n    x0 = float(match.group('upper_left_x'))\n    y0 = float(match.group('upper_left_y'))\n    match = lr_regex.search(ga)\n    x1 = float(match.group('lower_right_x'))\n    y1 = float(match.group('lower_right_y'))\n    out = {}\n    for key in hdf.datasets():\n        if keys is not None:\n            if key not in keys and key != qakey:\n                continue\n        f = hdf.select(key)\n        data = f.get()\n        attr = f.attributes()    \n        # print(attr)\n        data_c = data.astype(np.float32)\n        if 'scale_factor' in attr:\n            data_c = data_c * float(attr['scale_factor'])            \n        if '_FillValue' in attr:\n            data_c[data==attr['_FillValue']] = np.nan\n        if 'valid_range' in attr:\n            valid_range = attr['valid_range']\n            data_c = np.minimum(np.maximum(data_c, valid_range[0]), valid_range[1])\n        out[key] = data_c\n    qa = out[qakey]==4\n    out['NDSI_Snow_Cover'][qa] = np.nan\n    if qakey not in keys:\n        out.pop(qakey)\n    return out,x0,x1,y0,y1\n    \ndef getmodis_hv(product, h, v, date, verbose=True):\n    # Files are stored according to:\n    # http://modissa.blob.core.windows.net/modis-006/[product]/[htile]/[vtile]/[year][day]/filename\n    # This is the MODIS surface reflectance product\n    folder = product + '/' + '{:0>2d}/{:0>2d}'.format(h,v) + '/' + date.strftime('%Y%j')\n    # Find all HDF files from this tile on this day    \n    # filename = folder+'/'+product+'.A'+date.strftime('%Y%j')+'.h{:0>2d}v{:0>2d}'.format(h,v)+'.006'+\n    filename = folder.replace('/','_')\n    filenames = [f for f in ModisSource.tempfiles if f[:len(filename)]==filename]\n    if len(filenames) == 0:\n        ModisSource.tempfiles = os.listdir(ModisSource.temp_dir)\n        filenames = [f for f in ModisSource.tempfiles if f[:len(filename)]==filename]\n    ex = False\n    if len(filenames) > 0:        \n        filename = os.path.join(ModisSource.temp_dir,filenames[0])\n        ex = os.path.isfile(filename)\n    if not ex:\n        filenames = list_hdf_blobs_in_folder(ModisSource.modis_container_name,folder)\n        if verbose:\n            print('Found {} matching file(s):'.format(len(filenames)))\n            for fn in filenames:\n                print(fn)    \n        if len(filenames) == 0:\n            return None\n        filename = os.path.join(ModisSource.temp_dir, filenames[0].replace('/','_'))\n        print(filename)\n        if not os.path.isfile(filename):\n            # Download to a temporary file\n            wget.download(ModisSource.modis_blob_root + filenames[0], filename)\n    return SD(filename, SDC.READ)\n\ndef getmodis_lat_lon(product, lat, lon, date, verbose=True):\n    h,v = lat_lon_to_modis_tile(lat,lon)\n    return getmodis_hv(product, h, v, date, verbose)\n\nrgauss = 2.0\ndef getaver (lon,lat,lons,lats,elev,ex,r):\n    ry = r/(1110./(lat.shape[0]-1))\n    mask = (2*int(rgauss*ry)+1, 2*int(rgauss*ry)+1)\n    av = np.nan_to_num(cv2.GaussianBlur(elev, mask, ry)/cv2.GaussianBlur(ex, mask, ry),-9999.)\n    f = interp2d(lon, lat, av, kind='linear')\n    ret = np.array([f(lons[k], lats[k])[0] for k in range(lons.shape[0])])\n    ret[ret<0.] = np.nan\n    return ret\n\nsinu = Proj('+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs')\n# wgs84 = Proj(\"+init=EPSG:4326\")\n# ih = np.array([[8,4],[8,5],[9,4],[9,5],[10,4]])\nih = np.array([[9,5],[8,4],[8,5],[9,4],[10,4]])\ndef getinterpolated(product, lat, lon, date, rads, verbose=False, keys=keys, new=True):    \n    out = {}\n    if keys is not None:\n        if new:\n            for r in rads:\n                out.update({key+str(r): np.full_like(lat, np.nan) for key in keys})\n        else:\n            out.update({key: np.full_like(lat, np.nan) for key in keys})\n    # x, y = transform(wgs84, sinu, lon, lat)\n    x, y = sinu(lon, lat)\n    ihs = [lat_lon_to_modis_tiles(latk,lonk) for lonk,latk in zip(lon,lat)]\n    ih = np.unique(np.concatenate (ihs,0), axis=0)\n    # badhv = np.array([[7,5],[8,4],[11,4]])    \n    for ifile in range(ih.shape[0]):\n        hdf = getmodis_hv(product, ih[ifile][0], ih[ifile][1], date, verbose=False)\n        if hdf is None:\n            continue\n        m,x0,x1,y0,y1 = calibrate(hdf, keys=keys)\n        sz = m[list(m.keys())[0]].shape\n        ok = (x<=x1) & (y>=y1) & (x>=x0) & (y<=y0) \n        xm = np.linspace(x0,x1,sz[0])\n        ym = np.linspace(y0,y1,sz[1])\n        xs=x[ok]; ys=y[ok]\n        print(f\"ih={ih[ifile]} count={ok.sum()}\")\n        for key in m:\n            ex = np.isfinite(m[key]).astype(np.float32)\n            fld = np.nan_to_num(m[key],0.)\n            for r in rads:\n                v = getaver (xm,ym,xs,ys,fld,ex,r)\n                if key+str(r) in out:\n                    v0 = out[key+str(r)][ok]\n                    vi = np.isnan(v)\n                    v[vi] = v0[vi]\n                out[key+str(r)][ok] = v\n    bad = [key for key in out if np.isnan(out[key]).all()]\n    for key in bad:\n        out.pop(key)\n    if verbose:\n        print([date, {key: np.isfinite(out[key]).sum() for key in out}])\n    return out\n\n# hdf = getmodis_hv('MOD10A1', 8,4,datetime(2003,6,10))\n# m,x0,x1,y0,y1 = calibrate(hdf, keys=['NDSI_Snow_Cover', 'NDSI_Snow_Cover_Basic_QA', 'NDSI'])\ndef getfields(product, date, h, v, out={}, verbose=False, keys=keys):\n    hdf = getmodis_hv(product, h, v, date, verbose=verbose)\n    if hdf is not None:        \n        m,x0,x1,y0,y1 = calibrate(hdf, keys=keys)\n        out['corners'] = x0,x1,y0,y1\n        for key in m:            \n            ex = np.isfinite(m[key]).astype(np.float32)\n            fld = np.nan_to_num(m[key],0.)\n            if key in out:\n                out[key] += fld; out[key+'_c'] += ex\n            else:\n                out[key]  = fld; out[key+'_c']  = ex\n            print(f\"{product} {date} {h}_{v} {key} {fld.sum()/ex.sum():.4}\")\n    return out\n\ndef getmeanfields(reqdates, reqdays, loaddates, keys=keys, out = {}):\n    reqdays = [d.replace('-','_') for d in reqdays]    \n    for ifile in range(ih.shape[0]):\n        hvstr = f'f_{ih[ifile][0]}_{ih[ifile][1]}_'\n        for date in loaddates:\n            read = False\n            for date1,tday1 in zip (reqdates, reqdays):\n                if np.abs(((date-date1).days+180)%365-180)<=14:\n                    read = True\n                    break\n            if read:\n                ret = [getfields(product, date, ih[ifile][0], ih[ifile][1], keys=keys) \n                       for product in ['MOD10A1', 'MYD10A1'][:1+(date>=datetime(2002,7,4))]]\n                ret = {key: np.nanmean([r[key] for r in ret if len(r)>0],0) for key in keys+['corners']}\n                for date1,tday1 in zip (reqdates, reqdays):\n                    if np.abs(((date-date1).days+180)%365-180)<=14:\n                        for key in ret:\n                            if key != 'corners':\n                                name = hvstr+tday1+'M'+key\n                                if name in out:\n                                    out[name] = out[name]+ret[key]\n                                else:\n                                    out[name] = ret[key].copy()\n                                # if np.isnan(out[name]).any():\n                                #     print(f\"{name} bads={np.isnan(out[name]).sum()} goods={np.isfinite(out[name]).sum()}\")\n                                # else:\n                                if name[-2:] == '_c':\n                                    print(f\"{name[:-2]} {(out[name[:-2]]/out[name]).mean():.4}\")\n                            elif hvstr+key not in out:\n                                out[hvstr+key] = ret[key]\n    for key in out:\n        if key+'_c' in out:\n            out[key] = out[key]/out[key+'_c']\n    for key in [k for k in out.keys() if k[-2:]=='_c']:\n        out.pop(key)\n    return out\n\ndef interpfields(arx, df, reqdates, reqdays, rads, verbose=False, keys=keys):\n    x, y = sinu(df['longitude'].values, df['latitude'].values)\n    for key in arx:\n        if key[-7:] != 'corners':\n            hvstr = '_'.join(key.split('_')[:3])+'_'\n            x0,x1,y0,y1 = arx[hvstr+'corners']\n            sz = arx[key].shape\n            ok = (x<=x1) & (y>=y1) & (x>=x0) & (y<=y0) \n            xm = np.linspace(x0,x1,sz[0])\n            ym = np.linspace(y0,y1,sz[1])\n            xs=x[ok]; ys=y[ok]\n            print(f\"{key} count={ok.sum()}\")\n            ex = np.isfinite(arx[key]).astype(np.float32)\n            fld = np.nan_to_num(arx[key],0.)\n            key1 = key[len(hvstr):]\n            for r in rads:\n                v = getaver (xm,ym,xs,ys,fld,ex,r)\n                name = key1[:10].replace('_','-')+key1[10:]+str(r)\n                if name in df:\n                    v0 = df[name][ok]\n                    vi = np.isnan(v)\n                    v[vi] = v0[vi]\n                else:\n                    df[name] = np.full_like(x, np.nan)\n                df[name][ok] = v\n    return df\n\nif __name__ == '__main__':\n    # import geojson\n    # import pandas as pd\n    # from os import path\n    # workdir = 'evaluation'\n    # # workdir = 'development'\n    # with open(path.join(workdir,'grid_cells.geojson')) as f:\n    #     grid = geojson.load(f)\n    # grid = pd.DataFrame([{'cell_id':g['properties']['cell_id'], 'region':g['properties']['region'], \n    #                       'corners': np.array(g['geometry']['coordinates'])} for g in grid['features']]).set_index('cell_id')\n    # gll = np.vstack(grid['corners'].values)    \n    # grid['latitude'] = gll[:,:,1].mean(1)\n    # grid['longitude'] = gll[:,:,0].mean(1)\n    # stmeta = pd.read_csv(path.join(workdir,'ground_measures_metadata.csv')).set_index('station_id')\n    \n    import matplotlib.pyplot as plt\n    file = getmodis_lat_lon('MYD10A1', 41.881832, -87.623177, datetime(2010, 5, 15))\n    print(list(file.datasets().keys()))    \n    rgb,x0,x1,y0,y1 = calibrate(file)\n    for key in rgb:\n        fig = plt.figure(frameon=False); ax = plt.Axes(fig,[0., 0., 1., 1.])\n        ax.set_axis_off(); fig.add_axes(ax)\n        plt.imshow(rgb[key])\n        ax.set_title(key)",
  "history_output" : "data\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nTraceback (most recent call last):\n  File \"data_modis.py\", line 12, in <module>\n    class ModisSource:\n  File \"data_modis.py\", line 31, in ModisSource\n    wget.download(modis_blob_root + fname, fn)\n  File \"/Users/uhhmed/env_snowcast/lib/python3.8/site-packages/wget.py\", line 526, in download\n    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 247, in urlretrieve\n    with contextlib.closing(urlopen(url, data)) as fp:\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 222, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 531, in open\n    response = meth(req, response)\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 640, in http_response\n    response = self.parent.error(\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 569, in error\n    return self._call_chain(*args)\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n    result = func(*args)\n  File \"/opt/anaconda3/lib/python3.8/urllib/request.py\", line 649, in http_error_default\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\nurllib.error.HTTPError: HTTP Error 409: Public access is not permitted on this storage account.\n",
  "history_begin_time" : 1668624385599,
  "history_end_time" : 1668624386938,
  "history_notes" : null,
  "history_process" : "mxd0ok",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "p5v2dntiuj0",
  "history_input" : "import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nfrom datetime import datetime, timedelta\nimport numpy as np\nfrom models_init import Model0\nimport os\nimport torch\n# import features_init as features\n\nsind = lambda x: np.sin(np.radians(x))\ncosd = lambda x: np.cos(np.radians(x))\ndef sundirect(data,lat,lon):\n    D = data-datetime(2000,1,1)\n    D = 2.*np.pi/365.242*(D.days+D.seconds/86400.)\n    D = 0.39785*np.sin(D+np.radians(279.9348+1.9148*np.sin(D)-0.0795*np.cos(D)+0.0199*np.sin(D+D)-0.0016*np.cos(D+D)))\n    D = np.sqrt(1.-D*D)+D*sind(lat)-1.\n    cfi = cosd(lat)\n    return {'CRayMin':D-cfi, 'CRayMax':D+cfi, 'SinYear':sind(lat)}\ndef printshape(a, prefix='', print=print):\n    print (prefix+str({key: (a[key].dtype,a[key].shape,torch.isfinite(a[key]).sum().item() if isinstance(a[key],torch.Tensor) else np.isfinite(a[key]).sum()) for key in a} \\\n           if isinstance(a,dict) else [(x.dtype,x.shape,torch.isfinite(x).sum().item() if isinstance(x,torch.Tensor) else np.isfinite(x).sum()) for x in a]))\n\ndef getdatadict(rmode, stswe, gridswe, constfeatures, rsfeatures, maindevice, withpred=True, nmonths=1, print=print):\n    # shifts = [1,2,4]\n    shifts = [1,2,3,4]\n    print ('getdatadict: '+(f'Withpred shifts: {shifts}' if withpred else 'Withoutpred'))\n    if nmonths == 1:\n        monthid = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 12:0}\n    elif nmonths == 2:\n        monthid = {1:0, 2:0, 3:1, 4:1, 5:1, 6:1, 12:0}\n    elif nmonths == 3:\n        monthid = {1:0, 2:0, 3:1, 4:1, 5:2, 6:2, 12:0}\n    elif nmonths == 6:\n        monthid = {1:0, 2:1, 3:2, 4:3, 5:4, 6:5, 12:6}    \n    Model0m = Model0.to(maindevice)\n    def apply0 (inputs, rmax=1., valid_bs=64):\n        sh = inputs['ylo' if 'ylo' in inputs else 'xlo'].shape\n        result = torch.zeros(sh+(inputs['xval'].shape[-1],), device=maindevice)\n        batches = int(np.ceil(inputs['xlo'].shape[0]/valid_bs))    \n        with torch.no_grad():\n            for i in range (batches):\n                sel = torch.arange(i*valid_bs, min((i+1)*valid_bs,inputs['xlo'].shape[0]), device=maindevice)\n                result[sel] = Model0m ({key: inputs[key][sel] for key in inputs})\n        return result\n    days = {mode: features.getdays(stswe[mode]) for mode in stswe}    \n    # if 'train' in days:\n    #     days['train'] = [d for d in days['train'] if int(d[5:7])<=7]\n    dates = {mode: [datetime.strptime(tday,'%Y-%m-%d') for tday in days[mode]] for mode in stswe}\n    inputs = {mode: {} for mode in stswe}    \n    for mode in stswe:\n        for tday,date in zip(days[mode], dates[mode]):\n            arg = {}\n            for lab,d in zip('xy',[stswe, gridswe]):\n                if tday in d[mode]:\n                    val = [d[mode][tday].values]\n                    if withpred:\n                        pred = []\n                        for dt in shifts:\n                        # for dt in [1]:\n                            tday1 = (date-timedelta(days=7*dt)).strftime('%Y-%m-%d')\n                            pred.append(d[mode][tday1].values if tday1 in d[mode] else val[0])\n                            bad = np.isnan(pred[-1])                            \n                            pred[-1][bad] = val[0][bad] if dt == 1 else pred[-2][bad]\n                        pred.reverse()\n                        val += pred\n                    arg.update({lab+'val': np.hstack([v[:,None] for v in val])})\n                arg.update({lab+'lo': d[mode]['longitude'].values, lab+'la': d[mode]['latitude'].values, \n                            lab+'emb': d[mode]['emb'].values*nmonths+monthid[date.month]})\n                s = sundirect(date, arg[lab+'la'], arg[lab+'lo'])\n                arg[lab+'input'] = np.hstack( [d[mode][key].values[:,None] for key in constfeatures]+\n                        [s[key][:,None] for key in s]+\n                        [d[mode][tday+key].values[:,None] if tday+key in d[mode] else np.full((d[mode].shape[0],1),np.nan) for key in rsfeatures]\n                       )\n                arglist = constfeatures + list(s.keys()) + rsfeatures\n            for key in arg:\n                if key not in inputs[mode]:\n                    inputs[mode][key] = []\n                inputs[mode][key].append(arg[key][None])\n        for key in inputs[mode]:\n            x = np.vstack(inputs[mode][key])\n            inputs[mode][key] = torch.tensor(x if key[1:] in ['lo', 'la', 'emb'] or x.ndim==3 else x[:,:,None], device=maindevice, \n                                             dtype=torch.long if key[1:]=='emb' else torch.float32)\n        if mode == 'train' or rmode not in ['oper']:\n            inputs[mode]['target'] = inputs[mode]['yval' if 'yval' in inputs[mode] else 'xval'][...,:1]         \n    print(arglist)\n    # xtoy = torch.tensor(np.array([a in ['CDEC','SNOTEL'] for a in arglist]),device=maindevice)\n    # ytox = torch.tensor(np.array([a in uregions for a in arglist]),device=maindevice)\n    with torch.no_grad():\n        for mode in inputs:\n            printshape(inputs[mode], prefix=mode+': ', print=print)\n            if rmode == 'oper':\n                x = {key: inputs[mode][key] for key in inputs[mode] if key[1:] in ['lo','la']}\n                xx = {'x'+key: torch.cat((x['y'+key],x['x'+key]),1).detach() for key in ['lo','la']}\n                for i, a in enumerate(arglist):\n                    res1 = torch.cat((inputs[mode]['xinput'][...,i],inputs[mode]['yinput'][...,i]),1).detach()\n                    bad = torch.isnan (res1)\n                    ok = ~bad\n                    if ((bad.sum(1)>0) & (ok.sum(1)>0)).any():\n                        xx ['xval'] = res1[...,None].clone()\n                        xy = {key: xx[key][:,ok.any(0)] for key in xx}\n                        bad1 = bad.any(0)\n                        xy.update ({'y'+key[1:]: xx[key][:,bad1] for key in xx})\n                        print(f\"{a}: ok={ok.sum().item()} bad={bad.sum().item()}\")\n                        # printshape(xy, print=print)\n                        res2 = res1[:,bad1]\n                        res2[bad[:,bad1]] = apply0 (xy)[...,0][bad[:,bad1]]\n                        inputs[mode]['xinput'][...,i] = res1[:,:x['xlo'].shape[1]]\n                        inputs[mode]['yinput'][...,i] = res1[:,x['xlo'].shape[1]:]\n            # if xtoy.sum()>0:\n            #         x = {key: inputs[mode][key] for key in inputs[mode] if key[1:] in ['lo','la']}\n            #         x['xval'] = inputs[mode]['xinput'][..., xtoy]\n            #         # x['xval'][torch.isnan(x['xval']).any(-1)] = np.nan\n            #         inputs[mode]['yinput'][..., xtoy] = apply0 (x)\n            # if ytox.sum()>0:\n            #         x = {key: inputs[mode][('y' if key[0]=='x' else 'x')+key[1:]] for key in inputs[mode] if key[1:] in ['lo','la']}\n            #         x['xval'] = inputs[mode]['yinput'][..., ytox]\n            #         # x['xval'][torch.isnan(x['xval']).any(-1)] = np.nan\n            #         inputs[mode]['xinput'][..., ytox] = apply0 (x)\n    if 'train' in inputs:\n        inputs['train']['istrainy'] = torch.isfinite(inputs['train']['yval'][...,0]).sum(0) > 0\n        for key in inputs['train']:\n            if key[0] == 'y' or key == 'target':\n                inputs['train'][key] = inputs['train'][key][:,inputs['train']['istrainy']]\n    print(\"getdatadict finish\")\n    return inputs, arglist, days, dates",
  "history_output" : "",
  "history_begin_time" : 1668624360915,
  "history_end_time" : 1668624364803,
  "history_notes" : null,
  "history_process" : "z7yrof",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6f3cxxsh7og",
  "history_input" : "import numpy as np\nfrom os import path\nimport pandas as pd\nfrom scipy import spatial\n\ndef getembindex(stswe, gridswe, days, modelsdir=path.join('..','models'), print=print):\n    fname = path.join(modelsdir,'eindex.csv')\n    if path.isfile(fname):\n        eindex = pd.read_csv(fname)\n    elif 'train' in stswe:\n        eindex = [stswe[mode].index for mode in stswe]+[\n                gridswe['train'].index[np.vstack([np.isfinite(gridswe['train'][d]) for d in days['train']]).sum(0) > 10]]\n        eindex = np.unique(np.hstack(eindex))\n        eindex = pd.DataFrame({'emb': np.arange(eindex.shape[0]+1), 'index': np.hstack(['None']+[eindex])})\n        for lab in ['longitude', 'latitude']:\n            eindex[lab] = np.full(eindex.shape[0],np.nan)\n        for mode in stswe:\n            _,i1,j1 = np.intersect1d(eindex['index'],stswe[mode].index,return_indices=True)\n            _,i2,j2 = np.intersect1d(eindex['index'],gridswe[mode].index,return_indices=True)\n            for lab in ['longitude', 'latitude']:\n                eindex[lab].values[i1] = stswe[mode][lab].values[j1]\n                eindex[lab].values[i2] = gridswe[mode][lab].values[j2]\n        eindex.set_index('index').to_csv(fname)\n    else:\n        print (\"Cannot load \"+fname)\n        \n    nstations = eindex.shape[0]\n    for mode in stswe:\n        for d in [stswe, gridswe]:\n            tree = spatial.KDTree(np.vstack((eindex['longitude'],eindex['latitude'])).T)\n            dis,ind = tree.query(np.vstack((d[mode]['longitude'], d[mode]['latitude'])).T)\n            ok = np.nonzero(dis<0.02)[0]\n            ind = ind[ok]\n            d[mode]['emb1'] = np.zeros(d[mode].shape[0],dtype=np.int64)\n            d[mode]['emb1'].values[ok] = eindex['emb'].values[ind]\n    eindex.pop('latitude')\n    eindex.pop('longitude')            \n    for mode in stswe:\n        if 'emb' in stswe[mode]:\n            stswe[mode].pop('emb')\n            gridswe[mode].pop('emb')\n        stswe[mode] = stswe[mode].join(eindex.rename({'index': 'station_id'}, axis=1).set_index('station_id'))\n        gridswe[mode] = gridswe[mode].join(eindex.rename({'index': 'cell_id'}, axis=1).set_index('cell_id'))\n        for d in [gridswe[mode], stswe[mode]]:\n            d['femb'] = np.isnan(d['emb'])\n            d['emb'].values[d['femb']] = d['emb1'].values[d['femb']]\n            d['emb'] = d['emb'].astype(np.int64)\n            d['isemb'] = (d['emb']==0).astype(np.float32)\n        \n    # eindex = pd.read_csv(fname)\n    # for d in [stswe, gridswe]:\n    #     for mode in d:\n    #         vi = (((d[mode]['emb'].values!=0)&(d[mode]['emb1'].values==0))|((d[mode]['emb1'].values!=0)&(d[mode]['emb'].values==0))).nonzero()[0]\n    #         e = d[mode]['emb'][vi]\n    #         e1 = d[mode]['emb1'][vi]\n    #         print({lab: np.vstack([eindex[lab].values[e],eindex[lab].values[e1]]).T for lab in ['longitude', 'latitude']})\n    isst = np.array([len(i)<20 for i in eindex['index']])\n    isst[0] = False\n    for mode in gridswe:\n        if mode == 'test':\n            gridswe[mode]['fstation'] = isst[gridswe[mode]['emb1']] & gridswe[mode]['femb']\n            # for tday in days[mode]:\n            #     g = gridswe[mode][tday].values[gridswe[mode]['fstation']]\n            #     ok = np.isfinite(g)\n            #     if ok.sum():\n            #         print(f\"{mode} {tday} {g[ok]}\")\n        else:\n            gridswe[mode]['fstation'] = np.full(gridswe[mode].shape[0], False)\n    # for mode in gridswe:\n    #     vi = isst[gridswe[mode]['emb1']] & gridswe[mode]['femb']\n    #     es = stswe[mode]['emb'].values.copy()\n    #     es[es==0] = -1\n    #     _,i,j = np.intersect1d(gridswe[mode]['emb1'].values*vi,es,return_indices=True)\n    #     for tday in days[mode]:\n    #         g = gridswe[mode][tday].values[i]\n    #         s = stswe[mode][tday].values[j]\n    #         ok = np.isfinite(g+s) # & (s==0)\n    #         if ok.sum():\n    #             print(f\"{mode} {tday} {np.abs(g[ok]-s[ok]).mean()} {np.vstack((g[ok],s[ok])).T}\")\n                # print(f\"{mode} {tday} {np.vstack((gridswe[mode]['longitude'].values[i][ok],gridswe[mode]['latitude'].values[i][ok],stswe[mode]['longitude'].values[j][ok],stswe[mode]['latitude'].values[j][ok],g[ok],s[ok])).T}\")\n    return stswe, gridswe, nstations",
  "history_output" : "",
  "history_begin_time" : 1668624361894,
  "history_end_time" : 1668624363867,
  "history_notes" : null,
  "history_process" : "ccrrp0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "62rmykp1lj0",
  "history_input" : "import gc\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nfrom os import path\nimport torch\n\n# from features_init import getdays\ngetdays = lambda keys: list(sorted([tday for tday in keys if tday[:4].isnumeric() and len(tday) == 10]))\n\nfrom features_constFeatures import getconstfeatures\n\ndef getinputs(workdir, rmode, modelsdir, withpred=True, nmonths=1, maindevice=torch.device('cpu'),\n              awsurl='https://drivendata-public-assets.s3.amazonaws.com/', print=print):\n    uregions = ['central rockies', 'sierras']\n    stmeta,grid,constfeatures = getconstfeatures(workdir, uregions, awsurl = awsurl, print=print)\n    \n    file = path.join(workdir,'submission_format.csv')\n    print(f\"Loading submission format from {file}\")\n    sub = pd.read_csv(file)\n    for tday in getdays(sub):\n        sub[tday] = np.full(sub.shape[0], np.nan)\n    sub = sub.rename({sub.columns[0]: 'cell_id'}, axis=1).set_index('cell_id')\n    gridswe_test  = sub.join (grid, on='cell_id')\n    stswe = {}; gridswe = {}\n        \n    if rmode == 'oper':\n        file = awsurl+'ground_measures_features.csv'\n        print(f\"Downloading {file}\")\n        stswe_test = pd.read_csv(file)\n        file = path.join(workdir,'ground_measures_features.csv')\n        stswe_test.to_csv(file)\n        # print(f\"Loading {file}\")\n        # stswe_test = pd.read_csv(file)\n    elif rmode == 'test':\n        file = path.join(workdir,'ground_measures_test_features.csv')\n        print(f\"Loading {file}\")\n        stswe_test = pd.read_csv(file)\n    else:\n        sttestfile = path.join(workdir,'ground_measures_test_features.csv')\n        sttrainfile = path.join(workdir,'ground_measures_train_features.csv')\n        gridtrainfile = path.join(workdir,'train_labels.csv')\n        print(f\"Loading {sttestfile} {sttrainfile} {gridtrainfile}\")\n        stswe_test = pd.read_csv(sttestfile)\n        stswe_train = pd.read_csv(sttrainfile)\n        stswe_train = stswe_train.rename({stswe_train.columns[0]: 'station_id'}, axis=1).set_index('station_id')\n        gridswe_train = pd.read_csv(gridtrainfile).set_index('cell_id')\n        \n        stswe_train = stswe_train.join (stmeta, on='station_id')\n        gridswe_train = gridswe_train.join (grid, on='cell_id')    \n        \n        stswe['train'] = stswe_train\n        gridswe['train'] = gridswe_train\n    stswe_test = stswe_test.rename({stswe_test.columns[0]: 'station_id'}, axis=1).set_index('station_id').join (stmeta, on='station_id')\n    \n    stswe['test'] = stswe_test\n    gridswe['test'] = gridswe_test\n    \n    days = {mode: features.getdays(stswe[mode]) for mode in stswe}\n    dates = {mode: [datetime.strptime(tday,'%Y-%m-%d') for tday in days[mode]] for mode in stswe}\n    \n    stswe, gridswe, nstations  = features.getembindex (stswe, gridswe, days, modelsdir=modelsdir, print=print)\n    stswe, gridswe, rsfeatures = features.getmodisfeatures (workdir, stswe, gridswe, dates, rmode, print=print)\n    \n    if rmode != 'oper':\n        file = path.join(workdir,'labels_2020_2021.csv')\n        print(f\"Loading {file}\")\n        trg = pd.read_csv(file).set_index('cell_id')\n        for key in trg:\n            if key in gridswe['test']:\n                gridswe['test'].pop(key)\n        gridswe['test'] = gridswe['test'].join (trg, on='cell_id')\n        if rmode[:8] == 'finalize':\n            for d in [stswe, gridswe]:\n                for key in d['test']:\n                    if key not in d['train']:\n                        d['train'][key] = d['test'][key]\n                d.pop('test')\n            days['train'] = days['train']+days.pop('test')\n            dates['train'] = dates['train']+dates.pop('test')\n        if rmode[:5] == 'train':\n            # days['train'] = days['train'][:-5]\n            # dates['train'] = dates['train'][:-5]\n            for d in [stswe, gridswe]:\n                for key in d['test']:\n                    if key not in d['train']:\n                        d['train'][key] = d['test'][key]\n            days['train'] = days['train']+days['test'][:-31]\n            dates['train'] = dates['train']+dates['test'][:-31]\n            days['test'] = days['test'][-31:]\n            dates['test'] = dates['test'][-31:]\n            print(f\"test: {days['test']}\")\n            print(f\"train: {days['train']}\")\n                \n    constfeatures = uregions + list(sorted(constfeatures)) + ['isemb']\n    rsfeatures = list(sorted(rsfeatures))\n    gc.collect()\n    inputs, arglist, days, dates = features.getdatadict(rmode, stswe, gridswe, constfeatures, rsfeatures, maindevice,\n                                   withpred=withpred, nmonths=nmonths, print=print)    \n    return inputs, arglist, uregions, stswe, gridswe, days, dates, nstations",
  "history_output" : "",
  "history_begin_time" : 1668624364340,
  "history_end_time" : 1668624364815,
  "history_notes" : null,
  "history_process" : "822vm5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yyildd9xhxw",
  "history_input" : "import math\nimport torch\nimport torch.nn as nn\nfrom torch.optim.optimizer import Optimizer\nimport torch.nn.init as init\nfrom collections import OrderedDict\nimport numpy as np\nimport random\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\nfrom threading import Thread\nimport os\n \ndef set_seed(seed, deterministic=False):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n        \ndef _applyfn(module, fn):\n    for key in module.__dict__:\n        if isinstance(module.__dict__[key], torch.Tensor):\n            module.__dict__[key] = fn(module.__dict__[key])\n\nclass Linear2Function(torch.autograd.Function):\n    # Note that both forward and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, input, weight, bias, eps=None, mom=None, covx=None, step=1):\n        # u = None\n        if covx is not None and input.shape[0] > 1:\n            nrm = input - input.mean(0, keepdim=True)\n            covx.data.addmm_(nrm.t(), nrm, beta=mom, alpha=(1.-mom)/input.shape[0])\n            # covx.data.addmm_(input.t(), input, beta=mom, alpha=(1.-mom)/input.shape[0])\n            # u = covx/(1.-mom**step)\n            ctx.eps=eps; ctx.mom=mom; ctx.step=step\n        ctx.save_for_backward(input, weight, covx)\n        ctx.withbias = bias is not None\n        return input.mm(weight.t()) + bias if ctx.withbias else input.mm(weight.t())\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, covx = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)            \n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n            if covx is not None and input.shape[0] > 1:\n                # grad_weight.add_(grad_weight.mean(1,keepdim=True), alpha=-0.9)\n                # grad_weight.sub_(grad_weight.mean(1,keepdim=True))\n                # grad_weight.div_(torch.sqrt((grad_weight*grad_weight).mean(1,keepdim=True)).add_(eps))\n                # grad_weight.div_(torch.abs(grad_weight).mean(1,keepdim=True).add_(eps))\n                u = covx/(1.-ctx.mom**ctx.step)\n                d = torch.sqrt(u.diagonal())\n                eye = torch.eye(*u.shape, out=torch.empty_like(u))\n                (u.div_(d[None,:]).div_(d[:,None])).add_(eye, alpha=1.+ctx.eps)\n                fp16 = u.dtype == torch.float16\n                # grad_weight = torch.mm(grad_weight, u.inverse())\n                gw = (grad_weight.float() if fp16 else grad_weight).unsqueeze(-1)\n                if hasattr(torch, 'linalg'):\n                    grad_weight = torch.linalg.solve(u.float() if fp16 else u,gw).squeeze(-1)\n                else:\n                    grad_weight = torch.solve(gw,u.float() if fp16 else u)[0].squeeze(-1)\n                if fp16:\n                    grad_weight = grad_weight.half()\n                # grad_weight.div_(torch.sqrt((grad_weight*grad_weight).mean(1,keepdim=True)).add_(eps))\n                # grad_weight = torch.mm(grad_weight, u.float().inverse().to(grad_weight.dtype))    \n        if ctx.withbias and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n        return grad_input, grad_weight, grad_bias, None, None, None, None\nclass Linear2(nn.Linear):\n    def __init__(self, in_features, out_features, bias=True, eps=1e-6, mom=0.9):\n        nn.Linear.__init__(self, in_features, out_features, bias=bias)        \n        self.eps,self.mom = eps, torch.autograd.Variable(torch.tensor(mom), requires_grad=False)\n        self.reset_parameters ()\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        self.covx = torch.autograd.Variable(torch.zeros(self.weight.shape[1],self.weight.shape[1],dtype=self.weight.dtype,device=self.weight.device), requires_grad=False)\n        self.step = 0\n    def forward(self, input):\n        sh = list(input.shape)\n        if self.training:\n            if self.covx.device == input.device:\n                self.step += 1\n                step = self.step\n                covx = self.covx\n            else:\n                step = self.step + 1\n                covx = self.covx.to(input.device)\n            if len(sh) > 2:\n                return Linear2Function.apply(input.reshape(-1,sh[-1]), self.weight, self.bias, self.eps, self.mom, covx, step).reshape(sh[:-1]+[-1])\n            else:\n                return Linear2Function.apply(input, self.weight, self.bias, self.eps, self.mom, covx, step)\n        else:\n            if len(sh) > 2:\n                return Linear2Function.apply(input.reshape(-1,sh[-1]), self.weight, self.bias).reshape(sh[:-1]+[-1])\n            else:\n                return Linear2Function.apply(input, self.weight, self.bias)\n    def _apply(self, fn):\n        nn.Linear._apply(self, fn)\n        _applyfn(self, fn)\n        return self\nclass LinearConv2(Linear2):\n    def __init__(self, in_features, out_features, bias=True, eps=1e-6, mom=0.9):\n        Linear2.__init__(self, in_features, out_features, bias=bias, eps=eps, mom=mom)\n    def forward(self, x):\n        return Linear2.forward(self, x.permute(0,2,1)).permute(0,2,1)\n        \nclass normalizeFunc(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, dims=[0], momentum=0.1, eps=1e-5, running_mean=None, running_var=None, factor=None):\n        if factor is not None:\n            mean = x.mean(dims, keepdim=True).float()\n            if torch.isfinite(mean).all():\n                running_mean.data.mul_(1. - momentum).add_(mean,alpha=momentum)\n                running_var.data.mul_(1. - momentum).add_((x*x).mean(dims, keepdim=True).float(),alpha=momentum)\n                # xmin = x.min(dims[-1], keepdim=True)[0]\n                # xmax = x.max(dims[-1], keepdim=True)[0]\n                # for d in dims[:-1]:\n                #     xmin = xmin.min(d, keepdim=True)[0]\n                #     xmax = xmax.max(d, keepdim=True)[0]\n                # running_var.data.mul_(1. - momentum).add_((xmax-xmin).float(),alpha=momentum)\n                factor.data.mul_(1. - momentum).add_(momentum)\n                bias1 = -running_mean/factor\n                weight.data.copy_((1./(torch.sqrt((running_var/factor - bias1*bias1 + eps).clamp_(min=eps)))).to(x.dtype))\n                # weight.data.copy_((1./(running_var/factor+eps)).to(x.dtype))\n                bias.data.copy_((weight*bias1).to(x.dtype))\n        ctx.save_for_backward(weight)\n        if x.is_mkldnn:\n            return (x.to_dense() * weight + bias).to_mkldnn()\n        else:\n            return x * weight + bias\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad = None\n        if ctx.needs_input_grad[0]:\n            grad = ctx.saved_tensors[0]*grad_output\n        return grad, None, None, None, None, None, None, None, None\n    \nclass Normalize(nn.Module):\n    def __init__(self, num_features, dims=[0], eps=1e-5, momentum=0.01, frozen=False, mkl=False, *args, **kwargs):\n        super(Normalize, self).__init__()        \n        self.num_features = num_features\n        self.dims = dims\n        self.eps = eps\n        self.mkl = mkl\n        self.momentum = momentum\n        self.frozen = frozen\n        shape = [1]*(len(dims)+len(num_features))\n        for j,i in enumerate ([i for i in range(len(shape)) if i not in dims]):\n            shape[i] = num_features[j]\n        self.register_buffer('weight', torch.ones(*shape))\n        self.register_buffer('bias', torch.zeros(*shape))\n        self.register_buffer('running_mean', torch.zeros(*shape))\n        self.register_buffer('running_var', torch.ones(*shape))\n        self.register_buffer('factor', torch.zeros(1))\n    def _apply(self, fn):\n        nn.Module._apply(self, fn)\n        _applyfn(self, fn)\n        return self\n    def float(self):\n        fp16 = self.weight.dtype is torch.float16\n        nn.Module.float(self)\n        if fp16:\n            self.weight = self.weight.half()\n            self.bias = self.bias.half()\n\n    def forward(self, x: torch.Tensor):\n        if not self.frozen and self.training and self.momentum > 0:\n            y = normalizeFunc.apply(x, self.weight, self.bias, self.dims, self.momentum, self.eps, self.running_mean, self.running_var, self.factor)\n        else:\n            y = normalizeFunc.apply(x, self.weight, self.bias)\n        return y.to_mkldnn() if self.mkl else y\n            \n    # def frombn(self,bn):\n    #     self.factor = torch.ones(1)\n    #     with torch.no_grad():\n    #         self.running_var = bn.running_var/bn.weight.data/bn.weight.data - self.eps\n    #         self.weight = bn.weight.data/torch.sqrt(bn.running_var + self.eps)\n    #         self.bias   = bn.bias.data - bn.running_mean * self.weight\n    #         self.running_mean = bn.running_mean - bn.bias.data / self.weight\n    #         self.running_var += self.running_mean*self.running_mean\n        \ndef weight_init(m,gain=1.):\n    '''\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    '''\n    if hasattr(m,'bias'):    \n        if m.bias is not None and isinstance(m.bias, torch.Tensor):\n            # init.constant_(m.bias.data, 0)\n            init.normal_(m.bias.data, mean=0, std=0.1*gain)\n    if isinstance(m, nn.Conv1d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.Conv2d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.Conv3d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose1d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose2d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose3d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.BatchNorm1d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.BatchNorm2d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.BatchNorm3d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.Linear):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.LSTM):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain*0.1, mean=param.data.mean())\n    elif isinstance(m, nn.LSTMCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.GRU):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.GRUCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.Embedding):\n        m.weight.data.uniform_(-0.01,0.01)\n\nclass AdamW(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0., L1=0.,\n                       warmup = 100, amsgrad=False, belief=False, gc=0.9, parallel=True, gradinit=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= gc <= 1.0:\n            raise ValueError(\"Invalid gc value: {}\".format(gc))\n        \n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, warmup = warmup, \n                        amsgrad=amsgrad, belief=belief, gc=gc, gradinit=gradinit, L1=L1)\n        params = list(params)\n        super(AdamW, self).__init__([params[i] for i in np.argsort([p.numel() for p in params])[-1::-1]], defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n            \n    def stepone(self, group, p):\n        p_data_fp32 = p.data.float()\n        grad = p.grad.data.float()\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['exp_avg'] = torch.zeros_like(p_data_fp32)\n            state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n            if group['amsgrad']:\n                # Maintains max of all exp. moving avg. of sq. grad. values\n                state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n        # else:\n        #     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n        #     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n        \n        warmup = group['warmup'] > state['step']\n        gradinit = group['gradinit'] and warmup and p_data_fp32.ndim>1 and p_data_fp32.numel() > 8\n        if gradinit:\n            # grad = torch.full_like(p_data_fp32, (grad*p_data_fp32).mean())\n            grad = p_data_fp32*(grad*p_data_fp32).mean(0,keepdim=True)\n            # grad = (grad*p_data_fp32).mean(0,keepdim=True).expand_as(p_data_fp32)\n        exp_avg = state['exp_avg']\n        exp_avg_sq = state['exp_avg_sq']\n        beta1, beta2 = group['betas']\n\n        state['step'] += 1\n        bias_correction1 = 1. - beta1 ** state['step']\n        bias_correction2 = math.sqrt(1. - beta2 ** state['step'])\n                \n        # Gradient centralization\n        if group['gc']>0. and grad.ndim > 1:\n            grad.add_(grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True), alpha=-group['gc'])\n        if group['belief'] and grad.ndim > 1:\n            grad_residual = grad.add(exp_avg, alpha=-1./bias_correction1)\n            exp_avg_sq.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1. - beta2)\n        else:\n            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1. - beta2)\n        exp_avg.mul_(beta1).add_(grad, alpha=1. - beta1)\n                \n        scheduled_lr = 1e-8+state['step']*group['lr']/group['warmup'] if warmup and not gradinit else group['lr']\n        if group['weight_decay'] != 0:\n            p_data_fp32.mul_(1. - group['weight_decay']*scheduled_lr)\n        if group['L1'] != 0:\n            p_data_fp32.add_(-p_data_fp32.sign() * group['L1']*scheduled_lr)\n        if group['amsgrad']:\n            # Maintains the maximum of all 2nd moment running avg. till now\n            torch.max(state['max_exp_avg_sq']*beta2, exp_avg_sq, out=state['max_exp_avg_sq'])\n            denom = state['max_exp_avg_sq'].sqrt()\n        else:\n            denom = exp_avg_sq.sqrt()\n        p_data_fp32.addcdiv_(exp_avg, denom.add_(group['eps']*bias_correction2), \n                             value = -scheduled_lr*bias_correction2/bias_correction1)\n        p.data.copy_(p_data_fp32)\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        threads = []\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                if torch.isnan(grad).any() or (grad == 0.).all():\n                    continue\n                if sum([d>1 for d in p.data.shape])>1:\n                    threads.append(Thread (target=self.stepone, args=(group, p)))\n                    threads[-1].start()\n                else:\n                    self.stepone(group, p)\n        for thread in threads:\n            thread.join()\n        return loss\n\ndef reduce_mem_usage(df, use_float16=False,output=print):\n#    \"\"\"\n#    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n#    \"\"\"\n#    start_mem = df.memory_usage().sum() / 1024**2\n#    output(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n#    end_mem = df.memory_usage().sum() / 1024**2\n#    output(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n#    output(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))    \n    return df                ",
  "history_output" : "",
  "history_begin_time" : 1668624357181,
  "history_end_time" : 1668624360299,
  "history_notes" : null,
  "history_process" : "b320xo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tkf692nwhgb",
  "history_input" : "import numpy as np\nimport torch\nfrom models_init import calcdevice\n\nvalid_bs = 8\n\ndef apply(m, inputs, sel, device=calcdevice, autoloss=False, lab='', lossfun=None, selecty=None, calcsd=False):     \n    x = {key: inputs[key][sel].detach() for key in inputs if key[0] != 'i'}\n    x['xid'] = torch.zeros_like(x['xlo'])\n    x['yid'] = torch.ones_like (x['ylo']) if 'ylo' in x else x['xid']\n    okx = torch.isfinite(x['xval'][..., 0]).any(0)\n    if m.training:\n        oky = torch.isfinite(x['yval'][..., 0]).any(0)        \n        x = {key: x[key][:,okx if key[0]=='x' else oky].to(device).detach() for key in x}\n        if autoloss:\n            if selecty is None:\n                selecty = inputs['isstation'].to(device)\n            xx = {'x'+key: torch.cat((x['y'+key][:,selecty[oky]],x['x'+key]),1).detach() for key in ['val','lo','la','input','id','emb'] if 'x'+key in x}\n            xx['yid'] = xx['xid']\n            xx['target'] = xx['xval'][...,:1].clone()\n            res = m (xx)[0][..., :1]\n            sely = ~selecty[oky.to(device)]; calcyx = (sely.sum()>0).item()\n            if calcyx:\n                yx = {key: xx[key] for key in ['xval','xlo','xla','xinput','xid','xemb'] if key in xx}\n                yx.update({key: x[key][:,sely].detach() for key in x if (key not in yx) and (key[0] != 'x')})\n                eres = m (yx)[0][..., :1]\n            if lossfun is not None:\n                loss,cnt = m.loss(xx, res, lab=lab, lossfun=lossfun)\n                if calcyx:\n                    eloss,ecnt = m.loss(yx, eres, lab=lab, lossfun=lossfun)\n                    return res, loss+eloss, cnt+ecnt\n                return res, loss, cnt\n    elif 'ylo' in x:        \n        if selecty is not None:\n            oky = torch.isfinite(x['yval'][..., 0]).any(0)\n            x = {key: x[key][:,okx if key[0]=='x' else oky&selecty.to(oky.device)].to(device).detach() for key in x}\n        else:\n            x = {key: (x[key][:,okx] if key[0]=='x' else x[key]).to(device).detach() for key in x}\n    else:\n        x = {key: x[key].to(device).detach() for key in x}\n    res,sd = m (x)\n    res = res[..., :1]\n    if sd is not None:\n        if calcsd:\n            return res,sd\n        else:\n            x['sd'] = sd[...,:1]\n    if lossfun is not None:\n        loss,cnt = m.loss(x, res, lab=lab, lossfun=lossfun)\n        return res, loss, cnt\n    return res\n\ndef applymodels (inputs, models, average=True, device=calcdevice, calcsd=False):\n    sh = inputs['ylo' if 'ylo' in inputs else 'xlo'].shape\n    arxdevice = inputs['xlo'].device\n    batches = int(np.ceil(sh[0]/valid_bs))\n    for iyear in models:\n        models[iyear].calcsd = 2 if calcsd else 0\n    with torch.no_grad():\n        if average is True:\n            average = torch.full([len(models)], 1./len(models), device=arxdevice)\n        if isinstance(average,torch.Tensor):\n            result = torch.zeros(sh, device=arxdevice)\n            # if calcsd:\n            #     std = torch.zeros(sh, device=arxdevice)\n            for i in range (batches):\n                sel = torch.arange(i*valid_bs, min((i+1)*valid_bs,sh[0]), device=arxdevice)\n                av = average[sel] if average.ndim>1 else average\n                # torch.cuda.empty_cache()\n                res = 0.\n                if calcsd:\n                    sd = 0.\n                    for i,iyear in enumerate(models.keys()):\n                        r,s = apply(models[iyear], inputs, sel, device=device, calcsd=calcsd)\n                        s = s[...,0].to(arxdevice)\n                        res = r[...,0].to(arxdevice)/s*av[...,i] + res\n                        sd = 1./s*av[...,i] + sd\n                    res = res*av.sum(-1)/sd\n                    # std[sel] = sd\n                else:\n                    for i,iyear in enumerate(models.keys()):\n                        res = apply(models[iyear], inputs, sel, device=device, calcsd=calcsd)[...,0].to(arxdevice)*av[...,i] + res\n                result[sel] = res\n        else:\n            result = torch.zeros(sh+(len(models),), device=arxdevice)\n            if calcsd:\n                std = torch.zeros(sh+(len(models),), device=arxdevice)\n                for i in range (batches):\n                    sel = torch.arange(i*valid_bs, min((i+1)*valid_bs,sh[0]), device=arxdevice)\n                    for i,iyear in enumerate(models):\n                        r,s = apply(models[iyear], inputs, sel, device=device, calcsd=calcsd)\n                        result[sel,...,i] = r[...,0].to(arxdevice)\n                        std[sel,...,i] = s[...,0].to(arxdevice)\n            else:\n                for i in range (batches):\n                    sel = torch.arange(i*valid_bs, min((i+1)*valid_bs,sh[0]), device=arxdevice)\n                    result[sel] = torch.cat([apply(models[iyear], inputs, sel, device=device, calcsd=calcsd).detach() for iyear in models], -1).to(arxdevice)\n    if calcsd and average is False:\n        return result,std\n    else:\n        return result",
  "history_output" : "",
  "history_begin_time" : 1668624357729,
  "history_end_time" : 1668624359883,
  "history_notes" : null,
  "history_process" : "t4juof",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qghm0hxigm0",
  "history_input" : "# import math\nimport torch\nimport torch.nn as nn\nimport numpy as np\n        \nclass softdot(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, d, b, dim=0, count=None):        \n        ctx.dim = dim\n        ctx.nocount = count is None\n        if ctx.nocount:\n            dist = nn.functional.softmax(d.data,dim=dim)\n            ctx.save_for_backward(b,dist)\n        else:\n            dist = torch.exp(d.data)\n            sdist = dist.sum(dim=dim,keepdim=True)\n            # print([[(sdist/count<q).float().mean().item()] for q in [0.75,1.,1.25,1.5]])\n            limited = sdist<count\n            sdist[limited] = count[limited].to(dist.dtype) if type(count) is torch.tensor else count\n            dist.div_(sdist)\n            ctx.save_for_backward(b,dist,~limited)\n        if dist.ndim < b.ndim:\n            dist = torch.unsqueeze(dist,b.ndim-1)\n        return (b.data*dist).sum(dim=dim).detach()\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.needs_input_grad[0]:\n            b = ctx.saved_tensors[0]; dist = ctx.saved_tensors[1]\n            grad = b.data*grad_output.unsqueeze(ctx.dim)\n            if dist.ndim < b.ndim:\n                grad = grad.sum(-1)\n            grad.mul_(dist.data)\n            if ctx.nocount:                \n                grad.sub_(dist.data*(grad.sum(dim=ctx.dim,keepdims=True)))\n            else:\n                grad.sub_(dist.data*grad.sum(dim=ctx.dim,keepdims=True)*ctx.saved_tensors[2].float())\n            return grad,None,None,None\n        else:\n            return None,None,None,None\n\nclass softdot1(softdot):\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad=softdot.backward(ctx, grad_output)[0] if ctx.needs_input_grad[0] else None\n        gradb = None\n        if ctx.needs_input_grad[1]:\n            dist = ctx.saved_tensors[1]\n            if dist.ndim < ctx.saved_tensors[0].ndim:\n                gradb = dist.data[:,:,None]*torch.unsqueeze(grad_output,ctx.dim)\n            else:\n                gradb = dist.data*torch.unsqueeze(grad_output,ctx.dim)\n        return grad,gradb,None,None     \n\nclass xp1expm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, exp):        \n        ctx.exp = exp     \n        y = torch.sqrt(x.data)\n        if ctx.exp:\n            z = torch.exp(-y)\n            ctx.save_for_backward(z)\n            return ((y+1)*z).detach()\n        else:\n            ctx.save_for_backward(y)\n            return (torch.log1p(y).sub_(y)).detach()\n    @staticmethod\n    def backward(ctx, grad_output):        \n        if ctx.needs_input_grad[0]:\n            y = ctx.saved_tensors[0]\n            return (grad_output*y if ctx.exp else grad_output/(y+1)).mul_(-0.5), None\n        return None, None\n        \nclass sigma_act(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return (x.clamp(min=0.) + 1./(1. - x.clamp(max=0.))).detach()\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.needs_input_grad[0]:\n            x1 = ctx.saved_tensors[0].clamp(max=0.)-1.\n            return grad_output/x1/x1\n        return None# Write first python in Geoweaver",
  "history_output" : "",
  "history_begin_time" : 1668624357279,
  "history_end_time" : 1668624359852,
  "history_notes" : null,
  "history_process" : "ismi7d",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "256nifud60u",
  "history_input" : "import gc\nimport numpy as np\nimport torch\nfrom models_init import calcdevice\nfrom models_apply import applymodels\n\ndef getests (inputs, result, factor=1., select=None, region=None):\n    e = inputs['target'][...,0]-result*factor\n    e1 = e[torch.isfinite(e)].detach().cpu()\n    est = np.round([e1.mean().item(), np.sqrt((e1*e1).mean()).item()],4)\n    if select is not None:\n        e = e[select]\n    if region is not None:\n        e = e[:,region]\n    e = e[torch.isfinite(e)].detach().cpu()\n    est = np.hstack((est,np.round([e.mean().item(), np.sqrt((e*e).mean()).item()],4)))\n    return est\n\ndef quantile(vals, w, q=0.5):\n    s,perm = torch.sort(vals, -1)\n    ws = torch.gather(w.expand_as(vals),-1,perm)\n    ws = torch.cumsum(ws,-1)\n    ws.div_(ws[...,-1:])\n    c = (ws<q).sum(-1,keepdim=True).clamp_(ws.shape[-1]-2)\n    w1 = torch.gather(ws,-1,c)\n    w2 = torch.gather(ws,-1,c+1)\n    return ((torch.gather(s,-1,c)*(w2-q) + torch.gather(s,-1,c+1)*(q-w1))/(w2-w1))[...,0]*w.sum(-1)\n\n# result = (rets1*w[:,None]).sum(-1)\n# result = quantile(rets1, w[:,None])\n# w = rets - torch.nan_to_num (x['target'])\n# L2 = 1.\n# w = 1./ ((w*w).clamp(max=10.).sum(1)/okx.sum(1) + L2*L2)\n# # w.mul_((w<w.max(-1,keepdim=True)[0])&(w>w.min(-1,keepdim=True)[0]))\n# w.div_(w.sum(-1,keepdim=True))\n# r = (rets*w[:,None]).sum(-1,keepdim=True)\n# w.mul_ ((r*torch.nan_to_num (x['target'])).sum(1)/(r*r).sum(1)).mul_ (0.825)\n# result = (rets1*w[:,None]).sum(-1)\n# models.getests (inputs['test'], result, select=spring)\n# trg = inputs['test']['target'][...,0]\n# ok  = torch.isfinite(trg-result); r = result[ok]\n# bfactor = ((trg[ok]*r).sum()/(r*r).sum()).item()\n# models.getests (inputs['test'], result, bfactor, select=spring)\n\n# xv = inputs['test']['xval'].clone()\n# inputs['test']['xval'][bad] = np.nan\n# rets11 = models.applymodels (inputs['test'], modelslist, average=False).clamp_(min=0.)\n# inputs['test']['xval'] = xv\n\ndef inference (inputs, models, dates, lab='', smart=True, L2=1., factor=1., device=calcdevice,\n               calcsd=False, noemb=False, nousest=True, print=print):\n    gc.collect()\n    torch.cuda.empty_cache()\n    with torch.no_grad():\n        if smart is not None and 'ylo' in inputs:\n            x = {key: inputs[key].detach() for key in inputs if key[0] == 'x'}\n            x['yid'] = x['xid'] = torch.zeros_like(x['xlo'])\n            x['target'] = x['xval'][...,:1].clone()\n            trg = x['target'].clone()\n            okx = torch.isfinite(trg)\n            rets = applymodels (x, models, average=False, device=device)*okx            \n            w = rets - torch.nan_to_num (trg) \n            w = 1./ ((w*w).sum(1)/okx.sum(1) + L2*L2)\n            # w = 1./ (torch.abs(w).sum(1)/okx.sum(1) + L2)\n            # kf = w.max(-1,keepdim=True)[0]/w.min(-1,keepdim=True)[0]\n            # print ([kf.max(), kf.mean(), kf.min()])\n            # w.mul_((w<w.max(-1,keepdim=True)[0])&(w>w.min(-1,keepdim=True)[0]))\n            w.div_(w.sum(-1,keepdim=True))\n            r = (rets*w[:,None]).sum(-1,keepdim=True)\n            # if qc:\n            #     bad = ((trg[...,0]<rets.min(-1)[0]-3.)&(trg[...,0]>3.)) | (trg[...,0]>rets.max(-1)[0]+3.)\n            #     print(f\"qc #{bad[torch.isfinite(trg)[...,0]].float().mean().item()*100.:.5}%\")\n            w.mul_ ((r*torch.nan_to_num (trg)).sum(1)/(r*r).sum(1))\n            # if nousest:\n            w.mul_ (0.875) #Magic constant\n            # else:\n            #     w.mul_ (0.925) #Magic constant\n            del x, rets, okx\n            gc.collect()\n            torch.cuda.empty_cache()\n            # if qc:\n            #     xv = inputs['xval'].clone()\n            #     inputs['xval'][bad] = np.nan\n            w = w[:,None]\n            if noemb and 'yemb' in inputs:\n                ws = w.sum(-1,keepdim=True)\n                w = w.expand(-1,inputs['yval'].shape[1],-1)\n                noembmodels = torch.tensor([models[m].embedding<=0 for m in models], device=w.device)\n                w = w*((inputs['yemb'][0,:,None]>0)|noembmodels)\n                w = w/w.sum(-1,keepdim=True)*ws\n            if nousest:\n                print('Noemb mult 0.85')\n                w = w*(1.-0.15*(inputs['yemb'][0,:,None]==0))\n            result = applymodels (inputs, models, average=w, device=device, calcsd=calcsd).clamp_(min=0.)\n            # if qc:\n            #     inputs['xval'] = xv\n        else:\n            result = applymodels (inputs, models, device=device, calcsd=calcsd).clamp_(min=0.)\n            result.mul_ (0.9)\n        # result [result<0.01] = 0.\n    if 'target' in inputs:\n        spring = np.array([d.month*100+d.day for d in dates])\n        spring = (spring>=215)&(spring<=701)#&np.array([d.year>=2021 for d in dates])\n        spring = torch.tensor(spring, device=result.device)\n        # spring = None\n        exper = torch.isfinite(inputs['target'][...,0]).float().mean(0)<0.2\n        ests = getests (inputs, result, select=spring)\n        msg = lab+(f' L2={L2}' if smart else '')+(' sd' if calcsd else '')+('' if nousest else '|new|')+f' raw={ests}'        \n        ests = getests (inputs, result, select=spring, region=exper)\n        msg += f'{ests[2:]}'\n        if factor != 1.:\n            ests = getests (inputs, result, factor, select=spring)\n            msg += f' factor={factor:.4}:{ests}'        \n        trg = inputs['target'][...,0]\n        ok  = torch.isfinite(trg-result); r = result[ok]\n        bfactor = ((trg[ok]*r).sum()/(r*r).sum()).item()\n        ests = getests (inputs, result, bfactor, select=spring)\n        msg += f' bfactor={bfactor:.4}:{ests}'\n        ests = getests (inputs, result, bfactor, select=spring, region=exper)\n        msg += f'{ests[2:]}'\n        print('Estimations: '+ msg)\n    result = result.mul_(factor) #.to(maindevice)\n    return result\n\ndef test(inputs, models, dates, lab='', iyear='', device=calcdevice, calcsd=False, print=print, nousest=True):\n    models = {key: models[key].to(device) for key in models}\n    if len(models) == 1:\n        calcsd = False\n    r = {mode: inference (inputs[mode], models, dates[mode], lab=lab+'_'+mode+iyear, smart=True, factor=1., device=device, calcsd=calcsd, print=print, nousest=nousest) for mode in inputs}\n    if 'test' in inputs:\n        inference (inputs['test'], models, dates['test'], lab=lab+'_test'+iyear, smart=None, device=device, calcsd=calcsd, print=print, nousest=nousest)\n        # if len(models) > 1:\n        #     inference (inputs['test'], models, dates['test'], lab=lab+'_test'+iyear, smart=None, device=device, calcsd=~calcsd, print=print, nousest=nousest)\n    return r",
  "history_output" : "",
  "history_begin_time" : 1668624360696,
  "history_end_time" : 1668624364794,
  "history_notes" : null,
  "history_process" : "cydlb6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "e89dvelzdpj",
  "history_input" : "from os import path\nimport torch\nimport models_init\n\ndef loadmodels(loadstr, inputs, modelsdir, arglist, print=print, modelslist=None, totalbest=[100.]):\n    better = modelslist is not None\n    if not better:\n        modelslist = {}    \n    for fold in range(1,100):\n        fname = path.join(modelsdir, loadstr+'_'+str(fold-1)+'.pt')\n        # print(fname)\n        if path.isfile(fname):\n            if fold > len(totalbest):\n                totalbest = totalbest + [100.]*(fold - len(totalbest))\n            bw = torch.load(fname, map_location='cpu')\n            kw = {key: bw[key] for key in bw if not isinstance(bw[key], torch.Tensor)}\n            if better and 'best' in kw and kw['best']>totalbest[fold-1]:\n                if print is not None:\n                    print('skip')\n                continue\n            if print is not None:\n                print(kw)\n            # fname = path.join('models', loadstr+'_f'+str(fold-1)+'.pt')\n            # if path.isfile(fname):\n            #     bw = torch.load(fname, map_location='cpu')\n            wg = {key: bw[key].to(models.calcdevice) for key in bw if key not in kw}        \n            if 'best' in kw:\n                totalbest[fold-1] = kw.pop('best')\n            mode = list(inputs.keys())[0]\n            if 'nets.0.dist.kf.rm' in wg:\n                m = models.Model3(None, inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1], **kw)\n                # for mm in m.nets:\n                #     mm.net[0].frozen = True\n            else:\n                m = models.Model(inputs[mode]['xinput'].shape[-1], inputs[mode]['xval'].shape[-1], **kw)\n                # m.net[0].frozen = True\n            # printshape(wg)\n            # printshape(m.state_dict())\n            m.load_state_dict(wg)\n            m.to(device=models.calcdevice).eval()            \n            # print(m.importance())\n            modelslist[fold-1] = m\n    if print is not None:\n        imp = [modelslist[i].importance(arglist) for i in modelslist]\n        print (imp)\n        print ({key: [i[key] for i in imp] for key in imp[0]})\n    return modelslist",
  "history_output" : "",
  "history_begin_time" : 1668624364914,
  "history_end_time" : 1668624368934,
  "history_notes" : null,
  "history_process" : "oniu21",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "uforzh0xc3m",
  "history_input" : "import models_functions as functions\nimport numpy as np\nfrom models_oanet4s import DLOA, Parameter, Model0\nimport torch\n\ngetstyle = lambda style: 'season' if style is None else style\n\nclass Calibration(torch.nn.Module):\n    def __init__(self, **kwargin):\n        torch.nn.Module.__init__(self)\n        # self.p = Parameter([ 4.,  0., -4., -0.5, -4., -3.])\n        self.p = Parameter([ 3.,  0., -3., -0.5, -5.])\n    def forward(self,r):\n        p = self.p.to(r.device)\n        r1 = r/10.\n        if p.numel() == 5:\n            k = p[2]+r1*(p[3]+r1*p[4])        \n        else:\n            k = p[2]+r1*(p[3]+r1*(p[4]+r1*p[5]))\n        return (r+p[0].clamp(max=-p[2])+k*torch.exp(-r1*functions.sigma_act.apply(p[1]))).clamp_(min=0.)\n\nclass Model(DLOA):\n    def __init__(self, ninputs, nfunc, nlatent=3, norm=False, nmonths=1, calibr=False,\n                  initgamma=1., embedding=0, nstations=1, style=None, mc=1, dropout=0., initemb=None, **kw):\n        kw['neddvalgrad'] = initgamma is not None        \n        self.style = getstyle(style)\n        if self.style[-3:] == 'pos':\n            nfunc *= 2\n        # DLOA.__init__(self, 2*ninputs+embedding, nfunc, nlatent, **kw)\n        # self.m0 = DLOA(0,1,0,implicit=False,edging=True,rmax=1.,usee=False,biased=False,relativer=True)\n        DLOA.__init__(self, ninputs+embedding, nfunc, nlatent, **kw)\n        self.calcsd = self.dist.calcsd\n        self.embedding = embedding\n        self.mc = mc\n        self.nmonths = nmonths\n        if embedding > 0:\n            emb = torch.nn.Embedding(nstations*nmonths, embedding, max_norm=2.)\n            if initemb is None:\n                emb.weight.data.uniform_(-0.15, 0.15)\n                emb.weight.data[:nmonths] = 0\n            else:\n                emb.weight.data[:,0] = initemb            \n            if dropout > 0:\n                self.emb = torch.nn.Sequential(emb, torch.nn.Dropout(dropout))\n            else:\n                self.emb = torch.nn.Sequential(emb)\n            self.embmult = 0.2\n            # self.net[1].weight.data[:,len(arglist):len(arglist)+self.embedding] = \\\n            #     self.embmult*self.net[1].weight.data[:,len(arglist):len(arglist)+self.embedding]\n        self.calibr = None\n        if calibr:\n            self.calibr = Calibration()\n        self.norm = norm\n        if initgamma is not None:\n            self.gamma = Parameter(initgamma)      \n    def forward(self, xx, isval=True):\n        sel = torch.isfinite(xx['xval'][...,0]).sum(1) > self.points\n        select = (~sel).sum() > 0\n        if sel.sum() > 0:\n            x = {key: xx[key][sel] for key in xx} if select else xx\n        else:\n            return torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device),None\n        self.dist.calcsd = self.calcsd\n        xval = x['xval'].clone()\n        usey = 'ylo' in x\n        xi = x['xinput'].clone()\n        if usey:\n            yi = x['yinput'].clone()\n        # embinterp = (not self.training) and (self.embedding > 0)\n        # # embinterp = self.embedding > 0\n        # if embinterp:\n        #     xemb = torch.cat((x['xemb'],x['yemb']),1) if usey else x['xemb']\n        #     device = xemb.device\n        #     isy = xemb[0] == 0\n        #     embedding = self.embedding; self.embedding = 0\n        #     if isy.sum() > 0:\n        #         isx = ~isy\n        #         if isx.sum() > 0:\n        #             xe = {'x'+key: torch.cat((x['x'+key],x['y'+key]),1) if usey else x['x'+key] for key in ['lo', 'la']}\n        #             xx = {key: xe[key][:,isx] for key in xe}\n        #             xx.update({'y'+key[1:]: xe[key][:,isy] for key in xe})                \n        #             res = torch.zeros(xe['xlo'].shape+(embedding,), device=device)\n        #             xx['xval'] = self.emb(xemb[:,isx]).mul_(self.embmult)\n        #             res[:,isx] = xx['xval']\n        #             res[:,isy] = Model0.to(device) (xx)\n        #             x['xinput'] = torch.cat([x['xinput'],res[:,:x['xinput'].shape[1]]],-1)\n        #             if usey:\n        #                 x['yinput'] = torch.cat([x['yinput'],res[:,x['xinput'].shape[1]:]],-1)\n        #         else:\n        #             x['xinput'] = torch.cat([x['xinput'],torch.zeros(x['xlo'].shape+(embedding,), device=device)],-1)\n        #             if usey:\n        #                 x['yinput'] = torch.cat([x['yinput'],torch.zeros(x['ylo'].shape+(embedding,), device=device)],-1)\n        #     else:\n        #         x['xinput'] = torch.cat([x['xinput'],self.emb(x['xemb'])],-1)\n        #         if usey:\n        #             x['yinput'] = torch.cat([x['yinput'],self.emb(x['yemb'])],-1)\n        sd = None\n        if isval:\n            xv = x['xval']/10. if self.biased else x['xval']\n            bad = torch.isnan(xv)\n            if hasattr(self, 'gamma'):\n                gamma = torch.sigmoid(self.gamma).to(xv.device)\n                ok = (~bad)&(xv>0.)\n                xv[ok] = torch.pow(xv[ok], gamma)       \n            if self.norm:\n                xv  = torch.nan_to_num(xv,0.)\n                nrm = (xv.sum(1,keepdim=True)/(xv>0.).float().sum(1,keepdim=True).clamp_(min=1.)).clamp_(min=0.1)\n                xv = xv/nrm\n            if (self.mc > 1) and self.training and (not self.net[0].frozen):\n            # if self.mc > 1:\n                res = []\n                if self.training:\n                    rnd = torch.randn_like(xv)\n                for f in [-0.15,0.15,0.0][:self.mc]:\n                    # f1 = f*(1.-self.net[0].factor)\n                    x['xval'] = xv*((f*rnd + 1.).clamp_(min=0.,max=2.) if self.training else (f + 1.))\n                    x['xval'][bad] = np.nan\n                    res.append(DLOA.forward(self,x)[...,None])\n                res = torch.cat(res,-1).mean(-1)\n            else:\n                xv[bad] = np.nan\n                x['xval'] = xv\n                if self.style[-3:] == 'pos':\n                    x['xval'] = torch.cat((x['xval'],torch.tanh(xv)),-1)\n                res = DLOA.forward(self,x)\n            if self.norm:\n                if self.style[-3:] == 'pos':\n                    res = res[...,:xv.shape[-1]].clamp(min=0.)*(2.*res[...,xv.shape[-1]:]-1.).clamp(min=0.,max=1.)*nrm\n                else:\n                    res = res*nrm\n        else:\n            res = DLOA.forward(self,x)\n        if isval:\n            if hasattr(self, 'gamma'):\n                ok = torch.isfinite(res)&(res>0.)\n                res[ok] = torch.pow(res[ok], 1./gamma)\n            if self.biased:\n                res = (res*10.).clamp(min=0.)\n            if self.calibr is not None:\n                res = self.calibr(res)\n            x['xval'] = xval\n        # if embinterp:\n        #     self.embedding = embedding\n        x['xinput'] = xi\n        if usey:\n            x['yinput'] = yi\n        if 'sd' in x:\n            for key in ['sd','esd','ysigma','yval']:\n                if key in x:\n                    x[key] = x[key][...,:1]*nrm[...,:1] if self.norm else x[key][...,:1]\n                    if hasattr(self, 'gamma'):\n                        x[key] = torch.pow(x[key], 1./gamma)\n            sd = x['sd']\n        if select:\n            res1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'].shape+res.shape[-1:], np.nan, device=xx['xval'].device)\n            res1[sel] = res            \n            if sd is not None:\n                sd1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'].shape+(1,), np.nan, device=xx['xval'].device)\n                sd1[sel] = sd\n                return res1,sd1\n            return res1,None\n        return res,sd\n    def freeze(self, frozen):\n        self.dist.kf.frozen = frozen\n        if self.usee:\n            self.enet.kf.frozen = frozen\n        self.net[0].frozen = frozen>0\n    def prints(self):\n        return DLOA.prints(self) + (f' gamma={torch.sigmoid(self.gamma).item():.4}' if hasattr(self, 'gamma') else '')\n    def kfparams(self):\n        return [p for p in self.dist.parameters() if p.requires_grad]+\\\n              ([p for p in self.enet.parameters() if p.requires_grad] if self.usee else [])+\\\n              ([self.gamma] if hasattr(self, 'gamma') else [])+\\\n              ([self.calibr.p] if self.calibr is not None else [])\n    def netparams(self):\n        return [p for p in self.net.parameters() if p.requires_grad]+\\\n              ([p for p in self.emb.parameters() if p.requires_grad] if self.embedding > 0 else [])\n    def importance(self, arglist):\n        w = self.net[1].weight; w = torch.sqrt((w*w).sum(0))\n        roundw = lambda x: np.round(x.item(),3) if x.numel() == 1 else list(np.round(x.detach().cpu().numpy(),3))\n        imp = {key: roundw(w[i]) for i,key in enumerate(arglist)}\n        if self.embedding > 0:\n            imp['emb'] = roundw(w[len(arglist):len(arglist)+self.embedding])\n        w = w[len(arglist)+self.embedding:]\n        if self.densed:\n            nkf = self.dist.kf.rm.shape[0]\n            imp.update({'fg': roundw(w[:-nkf]), 'dense': roundw(w[-nkf:])})\n        else:\n            imp['fg'] = roundw(w)\n        return imp\n    def nonzero(self):\n        nz = {p.weight: torch.ones_like(p.weight,dtype=torch.bool) for p in self.net[1:-1:2]}\n        if self.embedding > 0 and self.nmonths > 1:\n            nz.update ({p: torch.ones_like(p,dtype=torch.bool) for p in self.emb.parameters() if p.requires_grad})\n        return nz\n              \nclass Model3(torch.nn.Module):\n    def __init__(self, weights, *argin, layers=2, style=None, mc=1, commonnet=True, **kwargin):\n        torch.nn.Module.__init__(self)\n        self.style = getstyle(style)\n        # mc=1\n        nets = [Model(*argin, style=style, mc=mc, **kwargin)]\n        nnets = {'season': 1, 'season_pos': 1}\n        if self.style == 'stack':\n            kwargin.update({'usee': False, 'rmax': kwargin['rmax2']}) #, 'implicit': True\n            nets += [Model(argin[0]+argin[1], *argin[1:], style=style, mc=mc, **kwargin) for k in range(layers-1)]\n        else:\n            if self.style == 'pos':\n                # kwargin.update({'initgamma': None, 'sigmed': False})\n                kwargin.update({'initgamma': None})\n            nets += [Model(*argin, style=style, mc=mc, **kwargin) for k in range(nnets[self.style] if self.style in nnets else layers-1)]\n        self.nets = torch.nn.ModuleList(nets)\n        self.calcsd = self.nets[0].dist.calcsd\n        self.commonnet = self.style not in ['stack'] and commonnet\n        # self.commonnet = False\n        if weights is not None:\n            weights = {key: weights[key].clone() for key in weights}\n            if self.style == 'season' and 'gamma' in weights:\n                weights['gamma'].add_(0.2)\n            self.nets[0].load_state_dict(weights)\n            if self.style not in ['stack']:\n                if self.style[:6] == 'season' and 'gamma' in weights:\n                    weights['gamma'].sub_(0.4)\n                if self.nets[1].sigmed:\n                    for net in self.nets[1:]:\n                        net.load_state_dict(weights)\n        if self.style not in ['stack'] and self.commonnet:\n            for net in self.nets[1:]:\n                net.net = self.nets[0].net\n                if self.nets[0].embedding > 0:\n                    net.emb = self.nets[0].emb\n        self.points = self.nets[0].points\n    def nonzero(self, nzero=None):\n        getnz = lambda net: net.nonzero().keys()\n        if nzero is None:\n            nzero = self.nets[0].nonzero()\n        nz = {p: nzero[p0] for (p,p0) in zip(getnz(self.nets[0]),nzero.keys())}\n        for net in self.nets[1:]:\n            if self.style == 'stack':\n                nz.update ({p: torch.ones(p.shape,dtype=torch.bool,device=p.device) for (p,p0) in zip(getnz(net),nzero.keys())})\n            else:\n                nz.update ({p: nzero[p0] for (p,p0) in zip(getnz(net),nzero.keys())})\n        return nz        \n    def forward(self, xx):\n        for net in self.nets:\n            net.points = self.points\n            net.dist.calcsd = self.calcsd\n        sel = torch.isfinite(xx['xval'][...,0]).sum(1) > self.points\n        select = (~sel).sum() > 0\n        if sel.sum() > 0:\n            x = {key: xx[key][sel] for key in xx} if select else xx\n        else:\n            return torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device),None\n        sd = None\n        if self.style == 'stack':\n            res,sd = self.nets[0](x)\n            xin = x['xinput'].clone()\n            if 'ylo' in x:\n                yin = x['yinput'].clone()\n                x['xinput'] = torch.cat([x['xinput'], x['xval']], -1)\n            yinp = 'yinput' if 'ylo' in x else 'xinput'\n            x[yinp] = torch.cat([x[yinp], res], -1)\n            res,sd = self.nets[1](x)\n            for net in self.nets[2:]:\n                x[yinp][...,-res.shape[-1]] = res\n                res,sd = net(x)\n            x['xinput'] = xin\n            if 'ylo' in x:\n                x['yinput'] = yin\n            return res,sd\n        elif self.style[:6] == 'season':\n            rets = []\n            sd = []\n            for i in range(len(self.nets)):\n                # x['xinputs2'] = i\n                # x['yinputs2'] = i\n                rets.append(self.nets[i](x)[0][...,None])\n                if 'sd' in x:\n                    sd.append(x['sd'])\n            rets = torch.cat(rets,-1)\n            d = rets.mean(-1)\n            d = torch.sigmoid(d[...,:1]-d[...,-1:])\n            res = (rets[...,:1,0]-rets[...,:1,1])*d+rets[...,:1,1]\n            if len (sd) > 0:\n                # printshape(sd+[d])\n                sd = (sd[0]-sd[1])*d+sd[1]\n            else:\n                sd = None\n        else:\n            res = torch.cat([net(x)[...,None] for net in self.nets],-1).mean(-1)\n        if select:\n            res1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device)\n            res1[sel] = res            \n            if sd is not None:\n                sd1 = torch.full(xx['ylo' if 'ylo' in xx else 'xlo'][...,None].shape, np.nan, device=xx['xval'].device)\n                sd1[sel] = sd\n                return res1,sd1\n            return res1,None\n        return res,sd\n    def freeze(self, frozen):\n        for net in self.nets:\n            net.freeze(frozen)\n    def prints(self):\n        return ' '.join([net.prints() for net in self.nets])\n    def kfparams(self):\n        r = []\n        for par in [net.kfparams() for net in self.nets]:\n            r += par\n        return r\n    def netparams(self):                        \n        p = [net.netparams() for net in self.nets]\n        r = p[0]\n        if not self.commonnet:\n            for par in p[1:]:\n                r += par[2:] if self.style[:6] == 'season' else par\n        return r\n    def loss(self, *argin, **kwargin):\n        return self.nets[0].loss(*argin, **kwargin)\n    def importance(self, arglist):\n        if self.commonnet:\n            return self.nets[0].importance(arglist)\n        imp = [net.importance(arglist) for net in self.nets]\n        return {key: [i[key] for i in imp] for key in imp[0]}\n    ",
  "history_output" : "",
  "history_begin_time" : 1668624366817,
  "history_end_time" : 1668624368950,
  "history_notes" : null,
  "history_process" : "91wt3d",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dubc5an0lzh",
  "history_input" : "from models_init import addons\nimport models_functions as functions\nimport numpy as np\t\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\ntry:\n    from torch_scatter import scatter\nexcept:\n    scatter = None\n\nParameter = lambda x: nn.Parameter(x.float().requires_grad_(True), requires_grad=True) if isinstance(x,torch.Tensor) \\\n                      else nn.Parameter(torch.tensor(x, dtype=torch.float32, requires_grad=True))\nVariable  = lambda x: torch.autograd.Variable(x if isinstance(x,torch.Tensor) else torch.tensor(x), requires_grad=False)\n\ndef MLP(ninputs, nf, noutputs, bias=True, sgd2=False):\n    layers = [addons.Normalize([ninputs], dims=[0])]\n    Linear = addons.Linear2 if sgd2 else nn.Linear\n    for nin,nout in zip([ninputs]+nf[:-1],nf):\n        layers += [Linear (nin,nout), nn.ReLU(inplace=True)]\n        # layers += [Linear (nin,nout), nn.Tanh()]\n    layers.append (Linear (nf[-1] if len(nf)>0 else ninputs, noutputs, bias=bias))\n    net = nn.Sequential (*layers)\n    net.apply(lambda m: addons.weight_init(m,gain=0.9))\n    net[-1].weight.data.mul_(0.5/np.sqrt(noutputs))\n    return net\n\n# bigdist = 256.*256.\nbigdist = 1024.\ndef distancesq(lo1, la1, lo2, la2):\n    with torch.no_grad():\n        dlo = torch.remainder(torch.abs(lo1-lo2)+180.,360.)-180.\n        dlo.mul_(torch.cos((la1+la2)*0.00872665)); dlo.mul_(dlo)\n        dla = la1-la2; dla.mul_(dla).add_(dlo)\n        return torch.nan_to_num_(dla,bigdist)\n\nclass nearestf(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, lo1, la1, lo2, la2, points, map1=None, map2=None, exclude=0, bad=None):\n        with torch.no_grad():\n            sh = lo2.shape\n            s1 = (sh[0]+4)*sh[1]*lo1.shape[1]\n            if map1 is not None and map2 is not None:\n                s1 *= 1+map1.shape[-1]\n            splits = int(np.ceil(s1/5.0e8))\n            sz     = int(np.ceil(sh[1]/splits))\n            inds = []; diss=[]\n            for i in range(splits):\n                i0 = i*sz\n                i1 = (i+1)*sz if i < splits-1 else lo2.shape[1]\n                dis = distancesq(lo2.data[0,i0:i1,None], la2.data[0,i0:i1,None], lo1.data[0,None], la1.data[0,None])\n                dis = dis[None]+(bad.to(dis.dtype)*bigdist)[:,None].to(dis.dtype) if bad is not None else dis[None,:,:]\n                if map1 is not None and map2 is not None:\n                    d = map1.data[:,None]-map2.data[:,i0:i1,None]\n                    dis = torch.einsum ('k...j,k...j->k...', d, d).add_(dis)\n                dis,ind = torch.topk(dis, points+exclude, dim=-1, largest=False)\n                if exclude>0:\n                    ind = ind[..., exclude:];  dis = dis[..., exclude:]\n                if ind.shape[0] != sh[0]:\n                    ind = ind.expand(sh[0],-1,-1)\n                    dis = dis.expand(sh[0],-1,-1)\n                diss.append(dis);  inds.append(ind)\n            if splits>1:\n                ind = torch.cat(inds,dim=1)\n                dis = torch.cat(diss,dim=1)\n        ctx.save_for_backward(map1,map2,ind)\n        return ind.detach(),dis.detach()\n    @staticmethod\n    def backward(ctx, gind, gdis):\n        dm1=None; dm2=None\n        if ctx.needs_input_grad[5]:\n            map1,map2,ind = ctx.saved_tensors\n            ind1 = ind.reshape(*((ind.shape[0],-1)+(1,)*(map1.ndim-2)))\n            # d = map2.unsqueeze(-2)\n            d = map2.unsqueeze(ind.ndim-1) - torch.gather(map1, 1, ind1.expand(-1,-1,*map1.shape[2:])).reshape(*ind.shape,*map1.shape[2:])\n            d.mul_(gdis.unsqueeze(-1)*2.)\n            if ctx.needs_input_grad[6]:\n                dm2 = d.sum(ind.ndim-1)\n            d = d.reshape(d.shape[0], -1, *d.shape[2-map1.ndim:])\n            if scatter:\n                dm1 = scatter (d, ind1, dim=1, dim_size=map1.shape[1], reduce=\"sum\")\n            else:\n                dm1 = torch.zeros_like(map1)\n                dm1.scatter_add_(1, ind1.expand(-1,-1,*d.shape[2-map1.ndim:]), d)\n            dm1 = -dm1\n        return None,None,None,None,None,dm1,dm2,None,None\n        \nclass distsq(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, y, dim=-1):        \n        ctx.save_for_backward(x, y); d = x-y\n        ctx.dim = dim\n        return torch.einsum ('k...j,k...j->k...' if dim == -1 else '...jk,...jk->...k', d, d).detach()\n    @staticmethod\n    def backward(ctx, gdis):\n        dx=None; dy=None\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n            x,y = ctx.saved_tensors\n            d = (x - y)*gdis.unsqueeze(ctx.dim)\n            if ctx.needs_input_grad[0]:\n                xdim = [i for i in range(d.ndim) if d.shape[i]>x.shape[i]]\n                dx = d.sum(xdim, keepdim=True).mul_(2.) if len (xdim) > 0 else d*2.\n            if ctx.needs_input_grad[1]:\n                ydim = [i for i in range(d.ndim) if d.shape[i]>y.shape[i]]\n                dy = (d.sum(ydim, keepdim=True) if len (ydim) > 0 else d).mul_(-2.)\n        return dx,dy,None\n\nclass MyKF(nn.Module):\n    def __init__(self, nfunc, rmax=1., *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.rm = Parameter(torch.zeros(nfunc,1))\n        self.rmax2 = rmax**2 \n        # self.relativer = relativer\n        self.frozen = 0\n    def prints(self):\n        r = torch.sqrt(self.getrm()).data.cpu().numpy()\n        if r.size > 1:\n            return f' Rg: [{111./r[0,0]:.4} {111./r[-1,0]:.4}]'\n        return f' Rg: {111./r[0,0]:.4}'\n    def getrm(self):\n        return functions.sigma_act.apply(self.rm)/self.rmax2\n# calc KF (sqrt(d))\n    def forward(self, d, exp):\n        C = functions.xp1expm.apply(d.reshape(1,-1)*self.getrm().to(d.device), exp)\n        return C.reshape(*C.shape[:-1],*d.shape)\n\nclass DistOA(nn.Module):\n    def __init__(self, nfunc, edging=False, implicit=True, eps=-1.5, neddvalgrad=False, calcsd=0, *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.implicit = implicit\n        self.edging = edging\n        self.logsigmoid = torch.nn.LogSigmoid()\n        self.kf = MyKF(nfunc, *argin, **kwargin)\n        self.softdot = functions.softdot1.apply if neddvalgrad else functions.softdot.apply\n        self.neddvalgrad = neddvalgrad\n        self.calcsd = calcsd\n        self.lusolve = torch.linalg.solve if hasattr(torch,'linalg') and hasattr(torch.linalg,'solve') else lambda C,b: torch.solve(b,C)[0]\n        self.eps = Parameter(eps*torch.ones(nfunc))\n    def prints(self):\n        self.kf.eval()\n        eps = self.geteps(torch.device('cpu'))[...,0,0,0]\n        if not self.implicit:\n            eps = torch.exp(eps)\n        msg = self.kf.prints()\n        if self.implicit or not self.edging:\n            msg += ' eps:' + np.array2string(eps.detach().cpu().numpy())\n        self.kf.train(self.training)\n        return msg.replace('\\n','')\n    def func(self, d, eval=True, sq=False):\n        if eval:\n            self.kf.eval()\n        eps = self.geteps(d.device).squeeze(-1).squeeze(-1)\n        if not self.implicit:\n            eps = torch.exp(eps)\n        if eps.ndim == 2:\n            eps = eps[:,None]*torch.eye(eps.shape[0],device=eps.device) \n        f = self.kf (d if sq else d*d, True)\n        f = f[None]*torch.eye(eps.shape[0],device=f.device)[..., None]\n        if eval:\n            self.kf.train(self.training)\n        de = torch.diag(f[:,:,0] + eps[:,:,0])\n        return f/torch.sqrt(de[None,:,None]*de[:,None,None])\n    def graph(self, plot=True, rmax=None):\n        if rmax is None:\n            rmax = 3./np.sqrt(self.kf.getrm().min().item())\n        t = torch.arange(0.,rmax,rmax/99.9,device=self.eps.device)\n        f = self.func(t)\n        if f.ndim > 2:\n            diag = torch.eye(f.shape[0], device=f.device, dtype=torch.bool)\n            f = torch.cat([f[diag], f[~diag]],0)\n        f = f.cpu().detach().numpy().T\n        r = 111.*t.cpu().detach().numpy()\n        if plot:\n            plt.plot (r, f)\n        return r[:,None],f\n    def solve(self, b, C):        \n        if self.edging:\n            n1 = 1./C.shape[-1]\n            d = self.lusolve(torch.cat((torch.cat((C,torch.full(C.shape[:-1]+(1,),-n1,dtype=C.dtype,device=C.device)),axis=-1),\n                            torch.cat((torch.full(C.shape[:-2]+(1,C.shape[-1]),n1,dtype=C.dtype,device=C.device),\n                                       torch.zeros(C.shape[:-2]+(1,1),dtype=C.dtype,device=C.device)),axis=-1)),axis=-2),\n                            torch.cat((b, torch.full(b.shape[:-2]+(1,b.shape[-1]),n1,dtype=b.dtype,device=b.device)),axis=-2))[...,:-1,:]\n        else:\n            d = self.lusolve(C,b)            \n        wg = (torch.abs(d).sum(-2,keepdim=True)-1.).clamp_(min=0.).mul(2.).clamp_(max=1.)\n        d1 = b/C.sum(-1,keepdim=True)\n        return d*(1.-wg) + d1*(wg/d1.sum(-2,keepdim=True).clamp_(1e-6 if self.edging else 1.))\n    def geteps(self, device):\n        return (torch.sigmoid(self.eps) if self.implicit else self.logsigmoid(self.eps)).to(device)[...,None,None,None]\n    def forward(self, C, val, bad=None):        \n        bsh = C.shape[:-2]+(C.shape[-1]-C.shape[-2],)\n        kfdims = 2\n        resh = C.ndim > kfdims+1\n        if resh:\n            C = C.reshape(-1, *C.shape[-kfdims:])\n            if val.ndim>3:\n                val = val.reshape(-1, *val.shape[-2:])\n                if bad is not None:\n                    bad = bad.reshape(val.shape[:-1])        \n        if bad is None:\n            bad = torch.isnan(val).any(-1)\n        with torch.no_grad():\n            eye = torch.eye(C.shape[1],dtype=val.dtype,device=C.device)\n            if not self.implicit:\n                eye.sub_(1.)\n        # self.returnsd = (self.calcsd==1 and not (self.training and (self.kf.frozen>=2))) or (self.calcsd==2)\n        self.returnsd = (self.calcsd==1 and self.training and (self.kf.frozen==0)) or (self.calcsd==2)\n        if self.implicit or not self.edging:\n            eps = self.geteps(C.device)*eye\n        val0 = (val.clone() if self.neddvalgrad else val.detach()).nan_to_num_(0.)\n        C = self.kf(C, self.implicit)\n        \n        b = C[..., C.shape[-2]:]; C = C[..., :C.shape[-2]]\n        if self.implicit:\n            C.add_(eps)            \n            if bad.any():\n                if self.training:\n                    b = b*~bad.unsqueeze(-1)\n                    C = C*~torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2))\n                else:\n                    b.mul_(~bad.unsqueeze(-1))\n                    C.mul_(~torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)))\n            r = self.solve(b, C)\n            if self.returnsd:\n                d = 1.-torch.einsum ('k...ip,k...ip->...pk', r, b)\n            r = torch.einsum ('k...jp,...jk->...pk', r, val0)\n        else:\n            if self.training:\n                if not self.edging:\n                    C = C - eps\n                if bad.any():\n                    b = b.add(bad.unsqueeze(-1), alpha=-bigdist)\n                    C = C.add(torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)), alpha=-bigdist)\n            else:\n                if not self.edging:\n                    C.sub_(eps)\n                if bad.any():\n                    b.add_(bad.unsqueeze(-1), alpha=-bigdist)\n                    C.add_(torch.logical_xor(bad.unsqueeze(-1),bad.unsqueeze(-2)), alpha=-bigdist)\n            r = b-torch.logsumexp(C,dim=-1,keepdim=True)\n            if self.returnsd:\n                r = nn.functional.log_softmax(r,dim=-2) if self.edging else r-torch.logsumexp(r,dim=-2,keepdim=True).clamp(min=0.)\n                d = 1.-torch.exp(r+b).sum([-2,-1]).T\n            r = self.softdot(r.permute(1,2,3,0), val0.unsqueeze(-2), 1, None if self.edging else 1.)\n        if resh:\n            r = r.reshape(*bsh[:2],-1)\n        if self.returnsd:\n            if resh:\n                d = d.reshape(*bsh[:2],-1)\n            return r,torch.sqrt(d.clamp(min=0.,max=1.))\n        return r\n            \ndef gather (x, lab):\n    return (torch.gather(x[lab], 1, x['ind1']).reshape(x['ind'].shape) if x[lab].ndim<3 else\n            torch.gather(x[lab], 1, x['ind1'][(...,) + (None,)*(x[lab].ndim-2)].expand(-1,-1,*x[lab].shape[2:])).reshape(*x['ind'].shape, *x[lab].shape[2:]))\ndef gathera (x, lab, index='ind'):\n    return x[lab].reshape(-1,*x[lab].shape[2:])[x[index]]\n \ndef printshape(a, prefix=''):\n    print (prefix+str({key: (a[key].dtype,a[key].shape,torch.isfinite(a[key]).sum().item() if isinstance(a[key],torch.Tensor) else np.isfinite(a[key]).sum()) for key in a} \\\n           if isinstance(a,dict) else [(x.dtype,x.shape,torch.isfinite(x).sum().item() if isinstance(x,torch.Tensor) else np.isfinite(x).sum()) for x in a]))\n    \nclass DLOA(nn.Module):\n    def __init__(self, ninputs, nfunc, nlatent=3, individual=False, usee=True, sigmed=False, reselect=True,\n                 nf=[32], netinit=None, points=16, densed=False, neddvalgrad=False,\n                 calcsd=0, rmax=1., rmaxe=None, sgd2=True, biased=False, relativer=False, gradloss=0.,\n                 *argin, **kwargin):\n        nn.Module.__init__(self)\n        self.sigmed = sigmed\n        self.biased = biased\n        self.individual = individual\n        nkf = nfunc if self.individual else 1        \n        self.neddvalgrad = neddvalgrad\n        self.relativer = relativer\n        self.usee = usee\n        self.densed = usee and densed\n        self.reselect = reselect or not self.usee\n        self.embedding = 0\n        self.gradloss = gradloss\n        self.dist = DistOA(nkf, neddvalgrad=self.biased or self.sigmed or neddvalgrad, calcsd=calcsd, \n                           rmax=rmax,#+np.sqrt(nlatent*0.5),\n                           *argin, **kwargin)\n        if usee:\n            kwargin.update({'implicit': False, 'edging': True, 'eps': -4. if densed else -2.})\n            self.enet = DistOA(nkf, neddvalgrad=False, calcsd=2 if self.densed else calcsd, \n                               rmax=rmax if rmaxe is None else rmaxe, *argin, **kwargin)\n        self.nfunc = nfunc\n        self.nlatent = nlatent\n        self.outs = {'map': nlatent}\n        if self.sigmed:\n            self.outs['sigma'] = nfunc        \n        if self.biased:\n            self.outs['bias'] = nfunc        \n        if netinit is None:\n            netinit = lambda nin,nout,bias: MLP (nin, nf, nout, bias, sgd2=sgd2)\n        netargs = ninputs+(nfunc+(nkf if self.densed else 0) if self.usee else 0)\n        netouts = sum([self.outs[key] for key in self.outs])\n        if netargs>0 and netouts>0:\n            self.net = netinit (netargs, netouts, bias=self.biased or self.sigmed)\n            if self.sigmed:\n                self.net[-1].bias.data = torch.cat([torch.ones(self.outs[key])*(0. if key=='eps' else ( 2.)) for key in self.outs]).detach()\n        else:\n            self.net = None\n        self.points = points\n        self.history = {}\n    def prints(self):\n        return (self.dist.prints() + (self.enet.prints() if self.usee else '')).replace('\\n','')\n    def return_KF(self):\n        return False\n    def graph(self, *argin, **kwargin):\n        return self.dist.graph(*argin, **kwargin)\n    def mappingx (self, x, lab):\n        if self.net is not None:\n            y = [x[lab+'input']]\n            if self.embedding > 0:\n                x[lab+'emba'] = self.emb(x[lab+'emb' if lab+'emb' in x else 'xemb']).mul_(self.embmult)\n                y.append(x[lab+'emba'])\n            if self.usee:\n                y.append(x[lab+'val'])\n            if self.densed:\n                y.append(torch.zeros(x['xval'].shape[:-1]+self.enet.eps.shape[:1],device=x['xval'].device) if lab=='x' else x['esd'])\n            y = torch.cat(y,-1) if len(y)>1 else x[lab+'input']\n            ok = torch.isfinite(y).all(-1)\n            if ok.any():\n                ymap = self.net (y[ok])\n                i0 = 0\n                for key in self.outs:\n                    i1 = i0+self.outs[key]\n                    s = ymap[..., i0:i1]; i0 = i1          \n                    if key == 'sigma':\n                        x[lab+key] = torch.ones (y.shape[:-1]+s.shape[-1:], dtype=ymap.dtype, device=ymap.device)                        \n                        s = functions.sigma_act.apply(s)\n                    else:\n                        x[lab+key] = torch.zeros (y.shape[:-1]+s.shape[-1:], dtype=ymap.dtype, device=ymap.device)\n                    x[lab+key][ok] = s\n            if lab == 'x':\n                x[lab+'ok'] = ok\n        else:\n            x[lab+'map'] = None\n            if lab == 'x':\n                x[lab+'ok'] = torch.isfinite(x[lab+'val']).all(-1)\n    def distmatr(self, x, bad, ismap, usey):\n        args = [x['xlo'], x['xla'], x['ylo' if usey else 'xlo'], x['yla' if usey else 'xla'], self.points]\n        args += [x['xmap'], x['ymap']] if ismap else [None, None]\n        x['ind'],b = nearestf.apply (*args, 1 if self.training or not usey else 0, bad)\n        with torch.no_grad():\n            x['ind1'] = x['ind'].reshape(x['ind'].shape[0],-1)\n            ylo = gather(x, 'xlo'); yla = gather(x, 'xla')\n            C = distancesq(ylo.unsqueeze(-2), yla.unsqueeze(-2), ylo.unsqueeze(-1), yla.unsqueeze(-1))\n        return C,b\n\n    def forward (self, x):        \n        self.dist.kf.train(self.training)\n        usey = 'ylo' in x\n        bad = torch.isnan(x['xval']).all(-1)\n        self.mappingx(x, 'x')\n        w = nn.functional.softmax(torch.arange(0,-self.points//2,-1,dtype=torch.float32,device=x['xval'].device),dim=-1)\n        if self.usee:\n            C,b = self.distmatr (x, bad, False, usey)\n            xg = gather(x, 'xval')\n            Cb = torch.cat((C,b.unsqueeze(-1)),-1)\n            x['yval'] = self.enet (Cb/(b[...,:w.shape[0]]*w).sum(-1,keepdim=True)[...,None] if self.relativer else Cb, xg)\n            if self.enet.returnsd:\n                x['yval'],x['esd'] = x['yval']\n        if usey or self.usee:\n            if not usey:\n                x['yinput'] = x['xinput']\n            self.mappingx(x, 'y')\n        else:\n            for key in self.outs:\n                x['y'+key] = x['x'+key]\n        \n        reselect = self.reselect # and not self.training\n        if reselect:      \n            bad = (~x['xok']) if self.usee else (~x['xok']) | torch.isnan(x['xval']).any(-1)\n            C,b = self.distmatr (x, bad, True, usey)\n        if x['xmap'] is not None:\n            xmap = gather (x, 'xmap')\n            if not reselect and b is not None:\n                b = distsq.apply(x['ymap'].unsqueeze(-2), xmap).add_(b)\n            C.add_(distsq.apply(xmap.unsqueeze(-3), xmap.unsqueeze(-2)))\n\n        x['xn'] = torch.nan_to_num(x['xval'], 0.)\n        if self.training:\n            if self.biased:\n                x['xn'] = x['xn']-x['xbias']\n            if self.sigmed:\n                x['xn'] = x['xn']/x['xsigma']\n        else:\n            if self.biased:\n                x['xn'].sub_(x['xbias'])\n            if self.sigmed:\n                x['xn'].div_(x['xsigma'])\n        x['xn'][bad] = np.nan\n        xg = gather (x, 'xn')\n        Cb = torch.cat((C,b.unsqueeze(-1)),-1)\n        yn = self.dist (Cb/(b[...,:w.shape[0]]*w).sum(-1,keepdim=True)[...,None] if self.relativer else Cb, xg, None)\n        if self.dist.returnsd:\n            yn,x['sd'] = yn        \n        if self.training:\n            if self.sigmed:\n                yn = yn*x['ysigma']\n                if self.dist.returnsd:\n                    x['sd'] = x['sd']*x['ysigma']\n        else:\n            if self.sigmed:\n                yn.mul_(x['ysigma'])\n                if self.dist.returnsd:\n                    # x['sd'].mul_(x['ysigma'])\n                    x['sd'] = x['sd']*x['ysigma']\n        if self.biased:\n            yn.add_(x['ybias'])\n        return yn\n    def loss(self, x, result, lab=None, lossfun='mse'):\n        dims = list(range(result.ndim-1))\n        loss = torch.zeros_like(result)\n        if lab is None:\n            lab = 'train' if self.training else 'test'\n        if lab+'_'+lossfun not in self.history:\n            self.history[lab+'_'+lossfun] = []\n        ok = torch.isfinite(x['target']) & torch.isfinite(result)\n        cnt = ok.sum(dims).float()\n        if lossfun=='hyber':\n            loss[ok] = nn.functional.smooth_l1_loss (x['target'][ok], result[ok], reduction='none')\n        elif lossfun=='mse':\n            loss[ok] = nn.functional.mse_loss (x['target'][ok], result[ok], reduction='none')\n        elif lossfun=='smape':\n            loss[ok] = 2.*nn.functional.mse_loss (x['target'][ok], result[ok], reduction='none')/(x['target'][ok] + result[ok]).clamp_(min=2.)\n        self.history[lab+'_'+lossfun].append ((loss.sum()/cnt.sum()).item())\n        # printshape(x)\n        # print(result.shape)\n        if self.training and self.gradloss > 0.:\n            g = x['ysigma'][ok[...,0]]\n            e = torch.ones_like(g, requires_grad=True)\n            g = torch.autograd.grad((g*e).sum(), x['yval'], create_graph=True, retain_graph=True)[0]\n            g = torch.autograd.grad(-g.clamp(max=0.).sum(), e, create_graph=True, retain_graph=True)[0]\n            g = (g*g).sum(-1)\n            loss[ok] += g*self.gradloss\n            \n            if self.embedding > 0:\n                g = x['ysigma'][ok[...,0]]\n                e = torch.ones_like(g, requires_grad=True)\n                g = torch.autograd.grad((g*e).sum(), x['yemba'], create_graph=True, retain_graph=True)[0]\n                g = torch.autograd.grad(-g.clamp(max=0.).sum(), e, create_graph=True, retain_graph=True)[0]\n                g = (g*g).sum(-1)\n                loss[ok] += g*self.gradloss\n            # print([g.mean(), g.max()])\n        if self.dist.returnsd and 'sd' in x:# and lossfun != 'mse':\n            ok = torch.isfinite(x['target']) & torch.isfinite(x['sd']) & torch.isfinite(result) & (x['sd'] > 0.)\n            pred = x['target'][ok] - result[ok]; sd = x.pop('sd')[ok].clamp(min=0.2,max=50.)\n            # print(sd.min())\n            nrm = (pred/sd).mul_(np.sqrt(0.5)).clamp(min=-6.,max=6.)\n            # loss[ok] = (pred*torch.erf(nrm)).add_((torch.exp(-nrm*nrm-0.5*np.log(2.))-(sd/x['ysigma'][ok] if self.sigmed else sd)).mul_(np.sqrt(1./np.pi)))\n            nrm = torch.exp(-nrm)*sd\n            loss[ok] = ((pred+nrm)*(pred>0)+(sd+nrm)*0.5*(pred<0)).div_(np.sqrt(0.5))\n            if self.usee and self.enet.returnsd and 'esd' in x and 'yval' in x:\n                ok1 = torch.isfinite(x['target']) & torch.isfinite(x['esd']) & torch.isfinite(x['yval']) & (x['esd'] > 0.)\n                pred = x['target'][ok1] - x['yval'][ok1]; sd = x.pop('esd')[ok1].clamp(min=0.2,max=50.)\n                # print(sd.min())\n                nrm = (pred/sd).mul_(np.sqrt(0.5)).clamp(min=-6.,max=6.)\n                # loss[ok1] = loss[ok1] + 0.2*((pred*torch.erf(nrm)).add_((torch.exp(-nrm*nrm-0.5*np.log(2.))-(sd)).mul_(np.sqrt(1./np.pi))))\n                nrm = torch.exp(-nrm)*sd\n                loss[ok1] = loss[ok1] + ((pred+nrm)*(pred>0)+(sd+nrm)*0.5*(pred<0)).div_(5.*np.sqrt(0.5))\n                cnt += 0.2*ok1.sum(dims).float()\n            if lab+'_sd' not in self.history:\n                self.history[lab+'_sd'] = []\n            self.history[lab+'_sd'].append ((loss.sum()/cnt.sum()).item())\n        if lab+'_n' not in self.history:\n            self.history[lab+'_n'] = []\n        self.history[lab+'_n'].append (cnt.sum().item())\n        return loss.sum(dims), cnt\n\nModel0 = DLOA(0,1,0,implicit=False,edging=True,rmax=1.,usee=False,biased=False,relativer=True).eval()",
  "history_output" : "",
  "history_begin_time" : 1668624365895,
  "history_end_time" : 1668624369757,
  "history_notes" : null,
  "history_process" : "fva4b9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2da6d6s19pp",
  "history_input" : "import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\ndef quantized (x,y,levels=6,nrmy=True,nrmx=False):\n    if nrmy:\n        nrm = torch.nan_to_num(y,0.)\n        nrm = nrm.sum(1)/(nrm>0).float().sum(1)\n        y = y/nrm[:,None]\n        if nrmx:\n            x = x/nrm[:,None]*10\n        ok = torch.isfinite(x+y)&(y>0)&(y<5)\n    else:\n        ok = torch.isfinite(x+y)\n    x  = x[ok].cpu().numpy(); y=y[ok].cpu().numpy()\n    s  = np.argsort(x); x=x[s]\n    x0 = x[0]; x1=x[-1]; x=(x-x0)*(levels/(x1-x0))\n    xq = np.arange(x.shape[0])*levels//x.shape[0]\n    for i in range(levels):\n        x[xq==i] = np.mean(x[xq==i])\n    x = x*((x1-x0)/levels)+x0\n    dec = int(np.floor(3-np.log10(x1-x0)))\n    return np.round(x,dec),y[s]\n\ndef boxplot (inputs,arglist):\n    for i,a in enumerate(arglist):\n        print({a: {mode: (torch.isfinite(inputs[mode]['xinput'][...,i]).sum().item(),\n                          torch.isfinite(inputs[mode]['yinput'][...,i]).sum().item()) for mode in inputs}})\n        fig, ax = plt.subplots(1,1)\n        ax.set_title(a)\n        for mode in ['train']:\n            try:\n                xi,xt = quantized (inputs[mode]['xinput'][...,i], inputs[mode]['xval'][...,0],nrmx=a in ['sde','sdwe'])\n                yi,yt = quantized (inputs[mode]['yinput'][...,i], inputs[mode]['yval'][...,0],nrmx=a in ['sde','sdwe'])\n                sns.boxenplot(x=xi,y=xt, ax=ax, linewidth=1.5)\n                sns.boxenplot(x=yi,y=yt, ax=ax, linewidth=1.5)\n                # ax.plot(xi,xt, '.'); ax.plot(yi,yt, '.')\n            except:\n                pass",
  "history_output" : "Traceback (most recent call last):\n  File \"visualization_boxplot.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1668624343179,
  "history_end_time" : 1668624347228,
  "history_notes" : null,
  "history_process" : "gyechz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hw7c14lbcrn",
  "history_input" : "import numpy as np\nfrom os import path\nimport matplotlib.pyplot as plt\nimport torch\nimport models\n\nmeannan = lambda x,dim=1: torch.nan_to_num(x,0.).sum(dim)/torch.isfinite(x).sum(dim)\ndef temporal(inputs, dates, results, result, uregions, figdir):\n    mon = np.array([d.month for d in dates])\n    ticks = [dates[i] for i in (np.nonzero(mon[1:]>mon[:-1])[0]+1)]\n    tickslabel = [d.strftime('%Y-%m-%D') for d in ticks]\n    for snotonly in [True, False]:\n        for ireg,reg in enumerate(uregions+['others', 'all']):\n            if reg in uregions:\n                region = inputs['yinput'][0,:,ireg]>0.5\n            elif reg == 'others':\n                region = inputs['yinput'][0,:,:2].sum(-1)<0.5\n            else:\n                region = torch.full(inputs['yinput'].shape[1:2], True)\n            if snotonly:\n                ok = torch.isfinite(inputs['target']).all(0)[:,0]&region\n            else:\n                ok = region\n            print(reg+(' SNOTEL' if snotonly else ' ALL')+f' count={ok.sum().item()}')\n            trg = inputs['target'][:,ok]\n            e  = results[:,ok]-trg\n            em = result[:,ok]-trg[...,0]\n            e = torch.sqrt(meannan(e*e))\n            fig, ax = plt.subplots(1,1)\n            ax.set_title(reg+(f' {ok.sum().item()} locations' if snotonly else ''))\n            ax.errorbar(dates, e.mean(-1).numpy(), e.std(-1).numpy(), label='one model error')\n            ax.plot(dates, torch.sqrt(meannan(em*em)).numpy(), '--', label='mixed model error')\n            ax.legend(loc='lower left')\n            ax2 = ax.twinx()\n            ax2.plot(dates, meannan(trg).numpy(), '-g', label='mean SWE (right axis)')\n            ax2.legend(loc='upper right')\n            ax.set_xticks(ticks)\n            ax.set_xticklabels(tickslabel)\n            fig.savefig(path.join(figdir,'Errors'+('_SNOTEL' if snotonly else '_ALL')+reg+'.png'), dpi=300)\n\nimport cartopy.crs as ccrs \nimport cartopy.feature as cfeature \nfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\nfrom matplotlib import ticker, cm, colors\nresoln = '10m'\nocean = cfeature.NaturalEarthFeature('physical', 'ocean', scale=resoln, edgecolor='none', facecolor=cfeature.COLORS['water'])\n# land = cfeature.NaturalEarthFeature('physical', 'land', scale=resoln, edgecolor='k', facecolor=cfeature.COLORS['land'])\nlakes = cfeature.NaturalEarthFeature('physical', 'lakes', scale=resoln, edgecolor='b', facecolor=cfeature.COLORS['water'])\nrivers = cfeature.NaturalEarthFeature('physical', 'rivers_lake_centerlines', scale=resoln, edgecolor='b', facecolor='none')\ncountry_borders = cfeature.NaturalEarthFeature(category='cultural', name='admin_0_boundary_lines_land', scale=resoln, facecolor='none', edgecolor='k')\nprovinc_borders = cfeature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces_lines', scale=resoln, facecolor='none', edgecolor='k')\ncrs = ccrs.PlateCarree()\ndef back (lon1=-124, lon2=-104, lat1=32, lat2=50, grid=True):\n    fig = plt.figure (figsize=(10, 10), edgecolor='w') \n    proj = ccrs.Stereographic (central_longitude=(lon1+lon2)*0.5, central_latitude=(lat1+lat2)*0.5)\n    ax = fig.add_subplot (1, 1, 1, projection=proj)\n    ax.set_extent ([lon1, lon2, lat1, lat2], crs=crs)\n    ax.add_feature (ocean, linewidth=2., edgecolor='b', facecolor='none', zorder=14)\n    ax.add_feature (lakes, linewidth=1., edgecolor='b', facecolor='b', zorder=14)\n    ax.add_feature (rivers, linewidth=1., edgecolor='b', facecolor='none', zorder=14)\n    ax.add_feature (country_borders, linewidth=2., edgecolor='k', facecolor='none', zorder=14)\n    ax.add_feature (provinc_borders, linewidth=1., edgecolor='k', facecolor='none', zorder=14)\n    if grid:\n        gl = ax.gridlines (draw_labels=True, dms=True, x_inline=False, y_inline=False)\n        gl.xlabels_top = False\n        gl.ylabels_left = True\n        gl.ylabels_right = False\n        gl.xlines = True\n        gl.xformatter = LONGITUDE_FORMATTER\n        gl.yformatter = LATITUDE_FORMATTER\n        gl.xlabel_style = gl.ylabel_style = {'size': 15, 'color': 'gray'}\n        gl.xlabel_style = gl.ylabel_style = {'color': 'k', 'weight': 'bold'}\n    return fig, ax\n\ndef spatial(inputs, dates, result, figdir):\n    snot = torch.isfinite(inputs['target']).all(0)[:,0]\n    em = result[:,snot]-inputs['target'][:,snot,0]\n    lon = inputs['ylo'][0,snot].numpy()\n    lat = inputs['yla'][0,snot].numpy()\n        \n    for mon in range(6):            \n        fig,ax = back ()\n        bounds = np.arange(0.2,5.,0.2)\n        cmap = cm.get_cmap ('viridis')\n        norm = colors.BoundaryNorm (bounds, cmap.N)\n        emm = em[torch.tensor([d.month-1==mon for d in dates])]\n        emm = torch.sqrt(meannan(emm*emm,0)).numpy()\n        img = ax.scatter(lon, lat, c=emm, cmap=cmap, norm=norm, transform=crs)\n        plt.colorbar (img, fraction=0.017, pad=0.04, ticks=bounds[4::5])\n        tit = ['January','February','March','April','May','June','July'][mon]\n        ax.set_title (tit)#+f\" mean RMSE={(emm*emm).mean():.3}\")\n        fig.savefig(path.join(figdir,tit+'.png'), dpi=300)\n\ndef importance(inputs, dates, modelslist, result, arglist, nousest, figdir):\n     groups = {'region': lambda a: a in ['central rockies', 'sierras', 'CDEC'],\n               'LandCover': lambda a: a[:9]=='GLOBCOVER',\n               'soil': lambda a: a[:4]=='SOIL',\n               'aspect': lambda a: a[:4] in ['aspe', 'east', 'sout'],\n               'elevation': lambda a: a[:9]=='elevation',\n               'MODIS': lambda a: a[:16]=='MNDSI_Snow_Cover',\n               'regular': lambda a: a in ['isemb']}\n     ret = {}\n     def permutei (x, iperm):\n         xperm = torch.randperm(x[...,0].numel())\n         xin = x.clone()\n         xin[..., iperm] = xin[..., iperm].reshape(xperm.shape[0],-1)[xperm].reshape(*x.shape[:2], -1)\n         return xin\n     for f in groups:\n         iperm = torch.tensor ([groups[f](a) for a in arglist])\n         inp = {key: permutei (inputs[key], iperm) if key[1:] == 'input' \n                 else inputs[key] for key in inputs}\n         ret[f] = models.inference (inp, modelslist, dates, lab=f, print=print, nousest=nousest)\n     e0 = result-inputs['target'][:,:,0]\n     e0 = torch.sqrt(meannan((e0*e0).reshape(-1),0))\n     imp = {}\n     for f in groups:\n         e = ret[f]-inputs['target'][:,:,0]\n         e = torch.sqrt(meannan((e*e).reshape(-1),0))\n         imp[f] = (e-e0).item()\n     order = np.argsort([imp[f] for f in imp])[::-1]        \n     keys = list(imp.keys())\n     fig, ax = plt.subplots(1,1)\n     ax.bar(['regular' if keys[o]=='ASO' else keys[o] for o in order], [imp[keys[o]] for o in order])\n     fig.savefig(path.join(figdir,'Importance.png'), dpi=200)\n\ndef latentsd(inputs, days, k, model, figdir):\n    x = {key: inputs[key][k:k+1].detach().to(models.calcdevice) for key in inputs if key[0] != 'i'}\n    res,sd = model(x)\n    \n    val,vec=torch.eig((x['ymap'][0,:,:,None]*x['ymap'][0,:,None]).sum(0), eigenvectors=True)\n    cv = {lab: (x[lab+'map'][0]*vec[:,0]).sum(-1).detach().cpu().numpy() for lab in 'xy'}\n    \n    cmap = cm.get_cmap ('viridis')\n    for loc in ['', '_CA']:\n        for visual in ['Spread', 'Latent']:\n            if loc == '_CA':\n                fig,ax = back (-122, -118, 36, 40.5)\n            else:\n                fig,ax = back ()\n            if visual == 'Spread':\n                bounds = np.arange(0.4,5.,0.2)*10.                \n                norm = colors.BoundaryNorm (bounds, cmap.N)\n                for lab in 'xy':\n                    img = ax.scatter(x[lab+'lo'][0].cpu().numpy(), x[lab+'la'][0].cpu().numpy(),\n                                      c=10.*x[lab+'sigma'][0,:,0].detach().cpu().numpy(), cmap=cmap, norm=norm, transform=crs)\n                plt.colorbar (img, fraction=0.017, pad=0.04, ticks=bounds[3::5])\n            elif visual == 'Latent':\n                bounds = np.arange(0.,56.)\n                norm = colors.BoundaryNorm (bounds, cmap.N)\n                for lab in 'xy':\n                    img = ax.scatter(x[lab+'lo'][0].cpu().numpy(), x[lab+'la'][0].cpu().numpy(),\n                                      c=cv[lab]-cv['y'].min(), cmap=cmap, norm=norm, transform=crs)\n                plt.colorbar (img, fraction=0.017, pad=0.04, ticks=bounds[::5])\n            ax.set_title (f\"{visual} {days[k]}\")\n            fig.savefig(path.join(figdir,f'{visual} {days[k]}{loc}.png'), dpi=300)\n            ",
  "history_output" : "Traceback (most recent call last):\n  File \"visualization_figures.py\", line 5, in <module>\n    import models\nModuleNotFoundError: No module named 'models'\n",
  "history_begin_time" : 1668624353429,
  "history_end_time" : 1668624356647,
  "history_notes" : null,
  "history_process" : "ro3cv8",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "7e7g59o5coe",
  "history_input" : "import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom visualization.boxplot import quantized\nfrom models.inference import getests\n\ndef monthests (inputs, dates, uregions, r):\n    trg = inputs['target'][...,0]; e = trg-r\n    ok = torch.isfinite(e)\n    \n    est = {}    \n    device = inputs['yinput'].device\n    month = torch.tensor([d.month for d in dates], device=device)\n    for ireg,reg in enumerate(uregions+['others', 'all']):\n        est[reg] = {}\n        if reg in uregions:\n            region = inputs['yinput'][0,:,ireg]>0.5\n        elif reg == 'others':\n            region = inputs['yinput'][0,:,:2].sum(-1)>0.5\n        else:\n            region = torch.full(inputs['yinput'].shape[1:2], True, device=device)\n        for m in [1,2,3,4,5,6,12]:\n            est[reg][m] = getests (inputs, r, select = month==m, region=region)[2:]\n        fig, ax = plt.subplots(1,1)\n        ax.set_title(reg+' BIAS')\n        m = month[:,None].expand(-1,region.sum())[ok[:,region]].cpu().numpy()\n        sns.boxenplot(x=m, y=e[:,region][ok[:,region]].cpu().numpy(), linewidth=1.5, ax=ax)\n        fig, ax = plt.subplots(1,1)\n        ax.set_title(reg+' MAE')\n        sns.boxenplot(x=m, y=torch.abs(e[:,region][ok[:,region]]).cpu().numpy(), linewidth=1.5, ax=ax)\n    \n    fig, ax = plt.subplots(1,1)\n    ax.set_title('BIAS')\n    x,y = quantized (r[ok],e[ok],nrmy=False)\n    sns.boxenplot(x=x,y=y, linewidth=1.5, ax=ax)\n    ax.set_xlabel('forecast')\n    fig, ax = plt.subplots(1,1)\n    ax.set_title('MAE')\n    x,y = quantized (r[ok],torch.abs(e[ok]),nrmy=False)\n    sns.boxenplot(x=x,y=y, linewidth=1.5, ax=ax)\n    ax.set_xlabel('forecast')\n    # ok = torch.isfinite(e) & (torch.abs(e) < 15.)\n    # x,y = quantized (r[ok],torch.abs(e[ok]),nrmy=False)\n    # sns.boxenplot(x=x,y=y, linewidth=1.5)\n    return est",
  "history_output" : "Traceback (most recent call last):\n  File \"visualization_month.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1668624344215,
  "history_end_time" : 1668624347280,
  "history_notes" : null,
  "history_process" : "fs1bt1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8h5kircm1h0",
  "history_input" : "from datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom scipy.io import loadmat, savemat\n\ntry:\n    from data_modis import getinterpolated,getmeanfields,interpfields\nexcept:\n    print(\"Cannot use data.modis. Install requirements\")\n\ndef initmodisnsdi (stswe,gridswe):\n    modisnsdi = {}\n    for mode in stswe:\n        modisnsdi[mode] = pd.DataFrame( {'index':np.hstack((stswe[mode].index,gridswe[mode].index)),\n                                         'latitude': np.hstack((stswe[mode]['latitude'].values,gridswe[mode]['latitude'].values)), \n                                         'longitude': np.hstack((stswe[mode]['longitude'].values,gridswe[mode]['longitude'].values))}  )\n    return pd.concat([modisnsdi[m] for m in stswe]).drop_duplicates(\"index\")\n\ndef getmodisfeatures (workdir, stswe, gridswe, dates, rmode, print=print):\n    # # lat,lon = np.meshgrid(np.arange(lat1,lat2,0.1),np.arange(lon1,lon2,0.1))\n    # # r = getinterpolated('MYD10A1', lat.reshape(-1), lon.reshape(-1), dates['train'][-1], [1])\n    # # plt.contourf(lon,lat,r['NDSI1'].reshape(lon.shape))\n    fmt = '%Y-%m-%d'\n    print ('Loading modis')\n    today = datetime.now()\n    reqdates = np.unique(np.hstack([dates[m] for m in dates]))\n    reqdays = [date.strftime(fmt) for date in reqdates]\n    if rmode == 'oper':\n        loaddates = ['2013-01-01', '2013-01-08', '2013-01-15', '2013-01-22', '2013-01-29', '2013-02-05', '2013-02-12', '2013-02-19', '2013-02-26', '2013-03-05', \n                     '2013-03-12', '2013-03-19', '2013-03-26', '2013-04-02', '2013-04-09', '2013-04-16', '2013-04-23', '2013-04-30', '2013-05-07', '2013-05-14', \n                     '2013-05-21', '2013-05-28', '2013-06-04', '2013-06-11', '2013-06-18', '2013-06-25', '2013-12-03', '2013-12-10', '2013-12-17', '2013-12-24', \n                     '2013-12-31', '2014-01-07', '2014-01-14', '2014-01-21', '2014-01-28', '2014-02-04', '2014-02-11', '2014-02-18', '2014-02-25', '2014-03-04', \n                     '2014-03-11', '2014-03-18', '2014-03-25', '2014-04-01', '2014-04-08', '2014-04-15', '2014-04-22', '2014-04-29', '2014-05-06', '2014-05-13', \n                     '2014-05-20', '2014-05-27', '2014-06-03', '2014-06-10', '2014-06-17', '2014-06-24', '2014-12-02', '2014-12-09', '2014-12-16', '2014-12-23', \n                     '2014-12-30', '2015-01-06', '2015-01-13', '2015-01-20', '2015-01-27', '2015-02-03', '2015-02-10', '2015-02-17', '2015-02-24', '2015-03-03', \n                     '2015-03-10', '2015-03-17', '2015-03-24', '2015-03-31', '2015-04-07', '2015-04-14', '2015-04-21', '2015-04-28', '2015-05-05', '2015-05-12', \n                     '2015-05-19', '2015-05-26', '2015-06-02', '2015-06-09', '2015-06-16', '2015-06-23', '2015-06-30', '2015-12-01', '2015-12-08', '2015-12-15', \n                     '2015-12-22', '2015-12-29', '2016-01-05', '2016-01-12', '2016-01-19', '2016-01-26', '2016-02-02', '2016-02-09', '2016-02-16', '2016-02-23', \n                     '2016-03-01', '2016-03-08', '2016-03-15', '2016-03-22', '2016-03-29', '2016-04-05', '2016-04-12', '2016-04-19', '2016-04-26', '2016-05-03', \n                     '2016-05-10', '2016-05-17', '2016-05-24', '2016-05-31', '2016-06-07', '2016-06-14', '2016-06-21', '2016-06-28', '2016-12-06', '2016-12-13', \n                     '2016-12-20', '2016-12-27', '2017-01-03', '2017-01-10', '2017-01-17', '2017-01-24', '2017-01-31', '2017-02-07', '2017-02-14', '2017-02-21', \n                     '2017-02-28', '2017-03-07', '2017-03-14', '2017-03-21', '2017-03-28', '2017-04-04', '2017-04-11', '2017-04-18', '2017-04-25', '2017-05-02', \n                     '2017-05-09', '2017-05-16', '2017-05-23', '2017-05-30', '2017-06-06', '2017-06-13', '2017-06-20', '2017-06-27', '2017-12-05', '2017-12-12', \n                     '2017-12-19', '2017-12-26', '2018-01-02', '2018-01-09', '2018-01-16', '2018-01-23', '2018-01-30', '2018-02-06', '2018-02-13', '2018-02-20', \n                     '2018-02-27', '2018-03-06', '2018-03-13', '2018-03-20', '2018-03-27', '2018-04-03', '2018-04-10', '2018-04-17', '2018-04-24', '2018-05-01', \n                     '2018-05-08', '2018-05-15', '2018-05-22', '2018-05-29', '2018-06-05', '2018-06-12', '2018-06-19', '2018-06-26', '2018-12-04', '2018-12-11', \n                     '2018-12-18', '2018-12-25', '2019-01-01', '2019-01-08', '2019-01-15', '2019-01-22', '2019-01-29', '2019-02-05', '2019-02-12', '2019-02-19', \n                     '2019-02-26', '2019-03-05', '2019-03-12', '2019-03-19', '2019-03-26', '2019-04-02', '2019-04-09', '2019-04-16', '2019-04-23', '2019-04-30', \n                     '2019-05-07', '2019-05-14', '2019-05-21', '2019-05-28', '2019-06-04', '2019-06-11', '2019-06-18', '2019-06-25', '2019-12-03', '2019-12-10', \n                     '2019-12-17', '2019-12-24', '2019-12-31', '2020-01-07', '2020-01-14', '2020-01-21', '2020-01-28', '2020-02-04', '2020-02-11', '2020-02-18', \n                     '2020-02-25', '2020-03-03', '2020-03-10', '2020-03-17', '2020-03-24', '2020-03-31', '2020-04-07', '2020-04-14', '2020-04-21', '2020-04-28', \n                     '2020-05-05', '2020-05-12', '2020-05-19', '2020-05-26', '2020-06-02', '2020-06-09', '2020-06-16', '2020-06-23', '2020-06-30', '2020-12-01', \n                     '2020-12-08', '2020-12-15', '2020-12-22', '2020-12-29', '2021-01-05', '2021-01-12', '2021-01-19', '2021-01-26', '2021-02-02', '2021-02-09', \n                     '2021-02-16', '2021-02-23', '2021-03-02', '2021-03-09', '2021-03-16', '2021-03-23', '2021-03-30', '2021-04-06', '2021-04-13', '2021-04-20',\n                     '2021-04-27', '2021-05-04', '2021-05-11', '2021-05-18', '2021-05-25', '2021-06-01', '2021-06-08', '2021-06-15', '2021-06-22', '2021-06-29']\n        loaddates = [datetime.strptime(d,fmt) for d in loaddates]\n    else:\n        loaddates = []\n    loaddates = np.unique(np.hstack([datetime(2013,1,1)-timedelta(days=7*dt) for dt in range(1,1200)]+[reqdates,loaddates]))\n    loaddates = [d for d in loaddates if (d<=today or d in reqdates) and d>=datetime(2000,2,24) and (d.month<=7 or d.month>=11)]    \n    loaddays = [date.strftime(fmt) for date in loaddates]\n    rads = modisrads = [1, 3, 10, 30]\n    modisnsdi0 = initmodisnsdi (stswe,gridswe)\n    index = set(modisnsdi0['index'].values)\n    if False:\n    # if rmode in ['oper','test']: \n    # if rmode in ['oper']: \n        modistestfile = path.join(workdir, 'modistest.csv')\n        if path.isfile(modistestfile):\n            modisnsdi = pd.read_csv(modistestfile)\n        else:\n            modisfieldsfile = path.join(workdir, 'modisfields.mat')\n            if path.isfile(modisfieldsfile):\n                modisfields = loadmat(modisfieldsfile)\n            else:\n                modisfields = getmeanfields(reqdates, reqdays, loaddates)\n                savemat(modisfieldsfile, modisfields, do_compression=True)\n            modisnsdi = interpfields(modisfields, modisnsdi0, reqdates, reqdays, modisrads)\n            mds = 'MNDSI_Snow_Cover'\n            for r,r2 in zip(modisrads[1:],modisrads[:-1]):\n                mdsr = mds+str(r); mdsr2 = mds+str(r2)\n                for tday in reqdays:\n                    if tday+mdsr in modisnsdi:\n                        modisnsdi[tday+mdsr2] = np.nan_to_num(modisnsdi[tday+mdsr2]-modisnsdi[tday+mdsr],0.)\n            modisnsdi.set_index('index').to_csv(modistestfile)\n    else:\n        modisclmfile = path.join(workdir, 'modisclm.csv')\n        modisnsdi = {}\n        if path.isfile(modisclmfile):            \n            modisnsdi = pd.read_csv(modisclmfile)\n            noex = index.difference(set(modisnsdi['index'].values))\n            if len(noex) > 0:\n                print(f\"Not found modis features for {noex}\")\n                modisnsdi = {}\n            else:\n                print(f\"Loaded {modisclmfile}\")\n        mds = 'NDSI_Snow_Cover'\n        lastday = loaddays[-1]\n        # if not ((lastday+mds+'1' in modisnsdi) or (lastday+'M'+mds+'10' in modisnsdi)):\n        if not (lastday+'M'+mds+'10' in modisnsdi):\n            modisfile = path.join(workdir, 'modis.csv')\n            if path.isfile(modisfile):\n                modisnsdi = pd.read_csv(modisfile)\n                noex = index.difference(set(modisnsdi['index'].values))\n                if len(noex) > 0:\n                    print(f\"Not found modis features for {noex}\")\n                    modisnsdi = modisnsdi0\n                else:\n                    print(f\"Loaded {modisfile}\")\n            else:\n                modisnsdi = modisnsdi0\n            lastday = [d for d in loaddates if d<=today][-1].strftime(fmt)\n            if not ((lastday+'O'+mds+'1' in modisnsdi) or (lastday+'Y'+mds+'1' in modisnsdi)):                \n                def getmodis(modisnsdi, tday, rads=modisrads, loady=True):\n                    # print ('Loading '+tday)\n                    msg = {}\n                    for product in ['MOD10A1', 'MYD10A1'][:1+loady]:\n                        if tday+product[1]+mds+str(rads[0]) not in modisnsdi:\n                            rs = getinterpolated(product, modisnsdi['latitude'].values, modisnsdi['longitude'].values, datetime.strptime(tday,fmt), rads)\n                            for key in rs:\n                                modisnsdi[tday+product[1]+key] = rs[key]\n                                msg[product[1]+key] = np.isfinite(rs[key]).sum()\n                    if len(msg) > 0:\n                        print (f\"{tday}: {msg}\")\n                for date in loaddates:\n                    try:\n                        getmodis (modisnsdi, date.strftime(fmt), loady=date>=datetime(2002,7,4))\n                    except:\n                        print(f\"Cannot loading modis for {date}\")\n                modisnsdi.set_index('index').to_csv(modisfile)        \n            print ('Calculating modis features')\n            # {tday: np.isnan(modisnsdi[tday+'YNDSI1']).sum() for tday in days['test']}\n            # for mds in ['ONDSI_Snow_Cover', 'ONDSI', 'YNDSI_Snow_Cover', 'YNDSI']:\n            # for mds in ['NDSI_Snow_Cover', 'NDSI']:\n            for mds in ['NDSI_Snow_Cover']:\n                if mds[:4] == 'NDSI':            \n                    for tday in loaddays:\n                        for r in modisrads:\n                            mdsr = mds+str(r)\n                            if tday+'Y'+mdsr in modisnsdi:\n                                modisnsdi[tday+mdsr] = np.nanmean([modisnsdi[tday+'Y'+mdsr].values,modisnsdi[tday+'O'+mdsr].values],0) \\\n                                    if tday+'O'+mdsr in modisnsdi else modisnsdi[tday+'Y'+mdsr]\n                            else:\n                                if tday+'O'+mdsr in modisnsdi:\n                                    modisnsdi[tday+mdsr] = modisnsdi[tday+'O'+mdsr]\n                        for r2 in modisrads:\n                            for product in 'OY':\n                                if tday+product+mds+str(r2) in modisnsdi:\n                                    modisnsdi.pop(tday+product+mds+str(r2))        \n                for r,r2 in zip(modisrads[1:],modisrads[:-1]):\n                    mdsr = mds+str(r); mdsr2 = mds+str(r2)\n                    for tday in loaddays:\n                        if tday+mdsr in modisnsdi:\n                            modisnsdi[tday+mdsr2] = np.nan_to_num(modisnsdi[tday+mdsr2]-modisnsdi[tday+mdsr],0.)\n                for r in modisrads:\n                    mdsr = mds+str(r)\n                    for date,tday in zip (reqdates, reqdays):\n                        if tday+'M'+mdsr not in modisnsdi:\n                            averd = [tday1 for date1,tday1 in zip (loaddates, loaddays) if np.abs(((date-date1).days+180)%365-180)<=14 and\n                                     np.abs((date-date1).days)>180 and tday1+mdsr in modisnsdi]\n                            #     averd = [tday1 for date1,tday1 in zip (loaddates, loaddays) if np.abs(((date-date1).days+180)%365-180)<=14 and\n                            #          date1<date-timedelta(days=180) and tday1+mdsr in modisnsdi]\n                            if len(averd) > 10:\n                                modisnsdi[tday+'M'+mdsr] = np.nanmean([modisnsdi[tday1+mdsr].values for tday1 in averd], 0)\n                                if tday+mdsr in modisnsdi:\n                                    bad = np.isnan(modisnsdi[tday+mdsr])\n                                    modisnsdi[tday+mdsr][bad] = modisnsdi[tday+'M'+mdsr][bad]\n                                elif date <= today:\n                                    modisnsdi[tday+mdsr] = modisnsdi[tday+'M'+mdsr]\n                # for r,r2 in zip(modisrads[1:],modisrads[:-1]):\n                #     mdsr = mds+str(r); mdsr2 = mds+str(r2)\n                #     for tday in reqdays:\n                #         if tday+mdsr in modisnsdi:\n                #             modisnsdi[tday+mdsr2] = np.nan_to_num(modisnsdi[tday+mdsr2]-modisnsdi[tday+mdsr],0.)\n                #         if tday+'M'+mdsr in modisnsdi:\n                #             modisnsdi[tday+'M'+mdsr2] = np.nan_to_num(modisnsdi[tday+'M'+mdsr2]-modisnsdi[tday+'M'+mdsr],0.)\n            for key in list(modisnsdi.keys()):\n                if key[:4].isnumeric():\n                    if key[:10] not in reqdays:\n                        # print(key)\n                        modisnsdi.pop(key)\n                    elif key[10] != 'M':\n                        if key[:10]+'M'+key[10:] in modisnsdi:\n                            modisnsdi[key] -= modisnsdi[key[:10]+'M'+key[10:]]\n                        else:\n                            modisnsdi.pop(key)        \n            print ('Saving modis features')\n            modisnsdi.set_index('index').to_csv(modisclmfile)\n    modisnsdi.pop('latitude')\n    modisnsdi.pop('longitude')\n    # rads = [1, 10, 30]\n    rads = modisrads\n    rsfeatures = []\n    # for mds in ['NDSI_Snow_Cover', 'NDSI']:\n    for mds in ['NDSI_Snow_Cover']:\n        # rsfeatures += [mds+str(r) for r in [10,30]]\n        rsfeatures += ['M'+mds+str(r) for r in rads]\n        # rsfeatures += [mds+str(r)+'_m' for r in rads]\n        # for r in rads:\n        #     mdsr = mds+str(r)\n        #     for date,tday in zip (reqdates, reqdays):\n        #         averd = []\n        #         for dt in range(4):\n        #             tday1 = (date-timedelta(days=7*dt)).strftime(fmt)+mdsr\n        #             if tday1 in modisnsdi:\n        #                 averd.append(tday1)\n        #         modisnsdi[tday+mdsr+'_m'] = np.nanmean([modisnsdi[tday1].values for tday1 in averd], 0)\n    for mode in stswe:\n        stswe[mode] = stswe[mode].join(modisnsdi.rename({'index': 'station_id'}, axis=1).set_index('station_id'))\n        gridswe[mode] = gridswe[mode].join(modisnsdi.rename({'index': 'cell_id'}, axis=1).set_index('cell_id'))\n    del modisnsdi\n    print(f\"Modis loaded. rsfeatures : {rsfeatures}\")\n    return stswe, gridswe, rsfeatures",
  "history_output" : "",
  "history_begin_time" : 1668624362329,
  "history_end_time" : 1668624363894,
  "history_notes" : null,
  "history_process" : "5gd2el",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "4paq38z7l6u",
  "history_input" : "getdays = lambda keys: list(sorted([tday for tday in keys if tday[:4].isnumeric() and len(tday) == 10]))\n\nfrom features_constFeatures import getconstfeatures\nfrom features_modisFeatures import getmodisfeatures\nfrom features_datadict import getdatadict\nfrom features_embeddings import getembindex\nfrom features_inputs import getinputs",
  "history_output" : "data\\modis.py: Modis dir is /var/folders/gl/8p9421ps1pj_ggqtwxl41s900000gn/T/modis contain 0 files\nCannot use data.modis. Install requirements\n",
  "history_begin_time" : 1668624353241,
  "history_end_time" : 1668624360304,
  "history_notes" : null,
  "history_process" : "ub0pfz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0b8n2pjvk4w",
  "history_input" : "import torch\ncalcdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport models_addons as addons\n# import models.catboost as catboost\nimport models_functions as functions\nfrom models_models import Model,Model3\nfrom models_oanet4s import DLOA,Model0\nfrom models_apply import apply,valid_bs,applymodels\nfrom models_inference import inference, test, getests\nfrom models_loadmodels import loadmodels",
  "history_output" : "",
  "history_begin_time" : 1668624354996,
  "history_end_time" : 1668624356673,
  "history_notes" : null,
  "history_process" : "e3qxn4",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o2cs4opq3pu",
  "history_input" : "from data_dem import getdem",
  "history_output" : "",
  "history_begin_time" : 1668624370839,
  "history_end_time" : 1668624372561,
  "history_notes" : null,
  "history_process" : "wzqoxa",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "68fl7oey7bz",
  "history_input" : "import cv2\nfrom datetime import datetime, timedelta\nimport geojson\nfrom geotiff import GeoTiff  \nfrom models_init import Model0\nimport netCDF4\nimport numpy as np\nimport pandas as pd\nimport os\nfrom os import path\nfrom scipy import interpolate\nfrom scipy.io import loadmat, savemat\nimport torch\nimport wget\nimport tarfile\nimport data_init as data\n\ncosd = lambda x: np.cos(np.radians(x))\n\njdays = list(range(0,183+21,7))+list(range(329-21,364,7)) #load Julian days\ndef getconstfeatures(workdir, uregions, awsurl, print=print):\n    datadir = path.join(workdir,'..')\n    print(f\"getconstfeatures: datadir={datadir} list={os.listdir(datadir)}\")    \n    file = path.join(workdir,'grid_cells.geojson')\n    print(f\"Loading {file}\")\n    with open(file) as f:\n        grid0 = geojson.load(f)\n    grid0 = pd.DataFrame([{'cell_id':g['properties']['cell_id'], 'region':g['properties']['region'], \n                          'corners': np.array(g['geometry']['coordinates'])} for g in grid0['features']]).set_index('cell_id')\n    file = path.join(workdir,'ground_measures_metadata.csv')\n    print(f\"Loading {file}\")\n    stmeta0 = pd.read_csv(file).set_index('station_id')\n    \n    stmetafile = path.join(workdir,'stmeta.csv')\n    gridfile = path.join(workdir,'grid.csv')\n    read = path.isfile(stmetafile) and path.isfile(gridfile)\n    if read:\n        print(f'Loading stmeta from {stmetafile} and grid from {gridfile}')\n        stmeta = pd.read_csv(stmetafile).set_index('station_id')\n        grid = pd.read_csv(gridfile).set_index('cell_id') \n        noex = set(stmeta0.index).difference(set(stmeta.index)).union(set(grid0.index).difference(set(grid.index)))\n        if len(noex) > 0:\n            print('unvalid stmeta / grid for {noex}')\n            read = False\n        else:\n            lonr = 1.5\n            lon1 = np.floor(min(grid['longitude'].values.min(),stmeta['longitude'].values.min())/lonr-1.)*lonr\n            lon2 = np.ceil(max(grid['longitude'].values.max(),stmeta['longitude'].values.max())/lonr+1.)*lonr\n            latr = 1.\n            lat1 = np.floor(min(grid['latitude'].values.min(),stmeta['latitude'].values.min())/latr-1.)*latr\n            lat2 = np.ceil(max(grid['latitude'].values.max(),stmeta['latitude'].values.max())/latr+1.)*latr   \n    if not read:        \n        print('Creating stmeta and grid')\n        grid = grid0\n        stmeta = stmeta0\n        gll = np.vstack(grid['corners'].values)\n        grid['latitude'] = gll[:,:,1].mean(1)\n        grid['longitude'] = gll[:,:,0].mean(1)\n        \n        lonr = 1.5; latr = 1.\n        lon1 = np.floor(min(gll[:,:,0].min(),stmeta['longitude'].values.min())/lonr-1.)*lonr\n        lon2 = np.ceil(max(gll[:,:,0].max(),stmeta['longitude'].values.max())/lonr+1.)*lonr\n        lat1 = np.floor(min(gll[:,:,1].min(),stmeta['latitude'].values.min())/latr-1.)*latr\n        lat2 = np.ceil(max(gll[:,:,1].max(),stmeta['latitude'].values.max())/latr+1.)*latr\n        for lab in uregions:\n            grid[lab] = np.array([grid['region'][k]==lab for k in range(grid.shape[0])]).astype(np.float32)\n            stmeta[lab] = np.zeros(stmeta.shape[0])\n            \n        for lab in ['CDEC', 'SNOTEL']:\n            stmeta[lab] = np.array([stmeta.index[k][:len(lab)]==lab for k in range(stmeta.shape[0])]).astype(np.float32)\n            grid[lab] = np.zeros(grid.shape[0])\n                \n        rgauss = 2.0\n        def getaver (lon,lat,elev,r):\n            ry = r/(111.*(lat[1]-lat[0]))\n            rx = r/(111.*(lon[1]-lon[0])*cosd((lat1+lat2)*0.5))\n            av = elev.copy()\n            cv2.GaussianBlur(elev, (2*int(rgauss*rx)+1, 2*int(rgauss*ry)+1), rx, av, ry)\n            f = interpolate.interp2d(lon, lat, av, kind='linear')\n            return lambda lons, lats: np.array([f(lons[k], lats[k])[0] for k in range(lons.shape[0])])\n        \n        demfile = f\"dem_N{lat1}_{lat2}_W{-lon1}_{-lon2}.mat\"\n        fname = path.join(datadir, demfile)\n        if not path.isfile(fname):\n            print('Creating DEM features')\n            dem = data.getdem(lat1,lat2,lon1,lon2,dir=path.join(datadir,'dem'), matfile=fname)\n        else:\n            print(f'Loading {demfile}')\n            dem = loadmat(fname)\n        demlon = dem.pop('lon').squeeze()\n        demlat = dem.pop('lat').squeeze()\n        print('Calculation DEM features')\n        for key in dem:\n            if key[:2] != '__':\n                elev = dem[key]\n                if key == 'elev':\n                    rads = [3, 10, 30, 100]\n                    f = getaver(demlon,demlat,elev,1.)\n                    grid['elevation_m'] = f(grid['longitude'], grid['latitude'])\n                    for r in rads:\n                        f_av = getaver(demlon,demlat,elev,r)\n                        name = 'elevation_'+str(r)\n                        for d in [stmeta, grid]:\n                            d[name] = f_av(d['longitude'], d['latitude']) - d['elevation_m']\n                else:\n                    rads1 = [1, 3, 10, 30]\n                    for r in rads1:\n                        f_av = getaver(demlon,demlat,elev,r)\n                        name = key+str(r)\n                        for d in [stmeta, grid]:\n                            d[name] = f_av(d['longitude'], d['latitude'])\n        ev = getaver(demlon,demlat,dem['elev'],1.)(stmeta['longitude'], stmeta['latitude'])\n        print(f\"dem elevation/stmeta elevation = {ev/stmeta['elevation_m']}\")\n        del demlon,demlat,dem\n        \n        print('Loading GLOBCOVER')  \n        for d in [stmeta, grid]:\n            for key in [key for key in d.keys() if key[:9]=='GLOBCOVER']:\n                d.pop(key)\n        \n        ncname = path.join(datadir,'C3S-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1.nc')\n        if not path.isfile(ncname):\n            arch = 'land_cover_map.tar.gz'\n            fname = path.join(datadir,arch)\n            if not path.isfile(fname):\n                print('Downloading '+arch)\n                wget.download(awsurl+arch, out=fname)\n            tar = tarfile.open(fname, \"r:gz\").extractall(datadir)\n            # ncname = path.join(datadir, tar.getmembers()[0].get_info()['name'])\n            os.remove(fname)\n        print(f'Loading GLOBCOVER from {ncname}')\n        nc = netCDF4.Dataset(ncname)\n        lon = np.array(nc.variables['lon'][:])\n        lat = np.array(nc.variables['lat'][:])\n        ok = ((lat>=lat1)&(lat<=lat2)).nonzero()[0]\n        ilat0 = ok[0]; ilat1 = ok[-1]+1\n        ok = ((lon>=lon1)&(lon<=lon2)).nonzero()[0]\n        ilon0 = ok[0]; ilon1 = ok[-1]+1\n        arr = np.array(nc.variables['lccs_class'][0,ilat0:ilat1,ilon0:ilon1])\n        lon = lon[ilon0:ilon1]\n        lat = lat[ilat0:ilat1]\n        nc.close()\n        \n        printvalstat = lambda arr: print ({t: (arr==t).sum()/arr.size*100. for t in np.unique(arr.reshape(-1))})\n        printvalstat (arr)\n        arr[(arr>=10) & (arr<30)] = 30\n        arr[arr==110] = 100; arr[arr==120] = 100\n        arr[(arr>130)&(arr<160)] = 130\n        arr[arr==72] = 70; arr[arr==71] = 70\n        arr[arr==201] = 200\n        types = [30,70,90,100,130,200,210,220]\n        printvalstat (arr)\n        gstep=1./360.\n        # rads = [1, 3, 10, 30]\n        rads = [3]\n        print('Calculation GLOBCOVER features')\n        def calcfeatures(arr,types,gstep,prefix):\n            for t in types:\n                eq = (arr==t).astype(np.float32)\n                for r in rads:\n                    ry = r/(111.*gstep)\n                    rx = r/(111.*gstep*cosd((lat1+lat2)*0.5))\n                    av = eq.copy()\n                    cv2.GaussianBlur(eq, (2*int(rgauss*rx)+1, 2*int(rgauss*ry)+1), rx, av, ry)\n                    for d in [stmeta, grid]:\n                        ilon = ((d['longitude'].values-lon1)/(lon2-lon1)*arr.shape[1]).astype(np.int64)\n                        ilat = ((lat2-d['latitude'].values)/(lat2-lat1)*arr.shape[0]).astype(np.int64)\n                        d[prefix+str(t)+'_'+str(r)] = np.array([av[ilat[i]:ilat[i]+2,ilon[i]:ilon[i]+2].mean() for i in range(ilon.shape[0])])\n            del eq,av\n        calcfeatures(arr,types,gstep,'GLOBCOVER')\n        del arr\n        \n        print('Loading SOIL')\n        for d in [stmeta, grid]:\n            for key in [key for key in d.keys() if key[:4]=='SOIL']:\n                d.pop(key)\n        tiffile = 'global_soil_regions_geoTIFF/so2015v2.tif'\n        tifname = path.join(datadir,tiffile)\n        if not path.isfile(tifname):\n            arch = 'soil_regions_map.tar.gz'\n            fname = path.join(datadir,arch)\n            if not path.isfile(fname):\n                print('Downloading '+arch)\n                wget.download(awsurl+arch, out=fname)\n            tar = tarfile.open(fname, \"r:gz\").extract('./'+tiffile, datadir)\n            os.remove(fname)\n        \n        print(f'Loading SOIL from {tifname}')\n        arr = np.array(GeoTiff(tifname).read_box([(lon1,lat1),(lon2,lat2)]))\n        printvalstat (arr)\n        # types = [7,21,50,54,64,74,75,81,83,92]\n        arr[arr>10] = np.floor(arr[arr>10]/10)*10\n        arr[arr==5] = 7; arr[arr==6] = 7\n        printvalstat (arr)\n        types = [7,20,50,60,70,80,90]\n        # types = np.unique(arr.reshape(-1))\n        gstep = 1./30.\n        # rads = [3, 10, 30]\n        rads = [10]\n        print('Calculation SOIL features')\n        calcfeatures(arr,types,gstep,'SOIL')\n        del arr\n        \n        # clm = 'ba'\n        # print('Loading '+clm)\n        # badir = path.join(datadir, clm+'-nc')\n        # if not path.isdir(badir):\n        #     arch = 'burned_areas_occurrence_map.tar.gz'\n        #     fname = path.join(datadir,arch)\n        #     if not path.isfile(fname):\n        #         print('Downloading '+arch)\n        #         wget.download(awsurl+arch, out=fname)\n        #     tar = tarfile.open(fname, \"r:gz\").extractall(datadir)\n        #     os.remove(fname)            \n        # rads = [10, 30]\n        # for jd in jdays:\n        #     if all([clm+str(r)+'_'+str(jd) in grid for r in rads]):\n        #         continue\n        #     tday = (datetime(2001,1,1)+timedelta(days=jd)).strftime('%m%d')\n        #     file = path.join(badir,'ESACCI-LC-L4-'+clm+'-Cond-500m-P13Y7D-2000'+tday+'-v2.0.nc')\n        #     print(f'Loading {clm} {tday} from {file}')\n        #     nc = netCDF4.Dataset(file)\n        #     lon = np.array(nc.variables['lon'][:])\n        #     lat = np.array(nc.variables['lat'][:])\n        #     ok = ((lat>=lat1)&(lat<=lat2)).nonzero()[0]\n        #     ilat0 = ok[0]; ilat1 = ok[-1]+1\n        #     ok = ((lon>=lon1)&(lon<=lon2)).nonzero()[0]\n        #     ilon0 = ok[0]; ilon1 = ok[-1]+1\n        #     v = np.array(nc.variables[clm.lower()+'_occ'][ilat0:ilat1,ilon0:ilon1]).astype(np.float32)\n        #     lon = lon[ilon0:ilon1]\n        #     lat = lat[ilat0:ilat1]\n        #     for r in rads:\n        #         f = getaver(lon, lat, v, r)\n        #         for d in [stmeta, grid]:\n        #             d[clm+str(r)+'_'+str(jd)] = f (d['longitude'], d['latitude'])\n        #     nc.close()\n            \n        stmeta = stmeta.copy()\n        grid = grid.copy()\n        \n        print('Saving stmeta to {stmetafile} and grid to {gridfile}')\n        stmeta.to_csv(stmetafile)\n        grid.to_csv(gridfile)\n        \n        print({key: grid[key].mean() for key in grid.keys() if key not in ['region', 'corners']})\n        print({key: stmeta[key].mean() for key in stmeta.keys() if key not in ['name','state']})\n        \n    print('Interpolate regions tags')\n    dtype = torch.float32\n    x = {'xlo': stmeta['longitude'].values, 'xla': stmeta['latitude'].values,\n         'ylo':   grid['longitude'].values, 'yla':   grid['latitude'].values}\n    x = {key: torch.tensor(x[key], dtype=dtype)[None] for key in x}\n    for lab in ['CDEC', 'SNOTEL']:\n        x['xval'] = torch.tensor(stmeta[lab].values, dtype=dtype)[None,:,None]\n        grid[lab] = Model0(x)[0,:,0].detach().numpy()\n    x = {key: x[('y' if key[0]=='x' else 'x')+key[1:]] for key in x if key[1:] in ['lo','la']}\n    for lab in uregions:\n        x['xval'] = torch.tensor(grid[lab].values, dtype=dtype)[None,:,None]\n        stmeta[lab] = Model0(x)[0,:,0].detach().numpy()\n    constfeatures = ['CDEC', 'elevation_m']\n    rads = [100, 30, 10, 3]\n    # rads = [100, 10]\n    # rads = [30, 10, 3]\n    constfeatures += ['elevation_'+str(r) for r in rads]\n    for d in [stmeta, grid]:\n        for r,r2 in zip(rads[1:],rads[:-1]):\n            d['elevation_'+str(r2)] -= d['elevation_'+str(r)]\n    # rads = [1, 3, 10, 30]\n    rads = [1, 3, 30]\n    for key in ['south', 'east']:\n        constfeatures += [key+str(r) for r in rads]\n        for r,r2 in zip(rads[1:],rads[:-1]):\n            for d in [stmeta, grid]:\n                # print([key,r2,np.abs(d[key+str(r2)]).mean(), r,np.abs(d[key+str(r)]).mean(),np.abs(d[key+str(r2)] - d[key+str(r)]).mean()])\n                d[key+str(r2)] -= d[key+str(r)]\n    rads = [1, 3, 10, 30]\n    for key in ['aspect']:\n        constfeatures += [key+str(r) for r in rads]    \n        for r,r2 in zip(rads[1:],rads[:-1]):\n            for d in [stmeta, grid]:\n                d[key+str(r2)] -= d[key+str(r)]\n    # constfeatures += [key for key in grid if key[:9]=='GLOBCOVER' and key[-2:] in ['_1','10']] # and key[9:12] != '220'\n    # constfeatures += [key for key in grid if key[:4]=='SOIL' and key[-2:] in ['_3','30']]\n    constfeatures += [key for key in grid if key[:9]=='GLOBCOVER' and key[-2:] in ['_3']]\n    constfeatures += [key for key in grid if key[:4]=='SOIL' and key[-2:] in ['10']]\n    # constfeatures += [key for key in grid if (key[:9]=='GLOBCOVER') or (key[:4]=='SOIL')]\n    print(f\"constfeatures : {constfeatures}\")\n    return stmeta,grid,constfeatures",
  "history_output" : "",
  "history_begin_time" : 1668624365843,
  "history_end_time" : 1668624370538,
  "history_notes" : null,
  "history_process" : "s28p1t",
  "host_id" : "100001",
  "indicator" : "Done"
}]
