[{
  "history_id" : "qghm0hxigm0",
  "history_input" : "# import math\nimport torch\nimport torch.nn as nn\nimport numpy as np\n        \nclass softdot(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, d, b, dim=0, count=None):        \n        ctx.dim = dim\n        ctx.nocount = count is None\n        if ctx.nocount:\n            dist = nn.functional.softmax(d.data,dim=dim)\n            ctx.save_for_backward(b,dist)\n        else:\n            dist = torch.exp(d.data)\n            sdist = dist.sum(dim=dim,keepdim=True)\n            # print([[(sdist/count<q).float().mean().item()] for q in [0.75,1.,1.25,1.5]])\n            limited = sdist<count\n            sdist[limited] = count[limited].to(dist.dtype) if type(count) is torch.tensor else count\n            dist.div_(sdist)\n            ctx.save_for_backward(b,dist,~limited)\n        if dist.ndim < b.ndim:\n            dist = torch.unsqueeze(dist,b.ndim-1)\n        return (b.data*dist).sum(dim=dim).detach()\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.needs_input_grad[0]:\n            b = ctx.saved_tensors[0]; dist = ctx.saved_tensors[1]\n            grad = b.data*grad_output.unsqueeze(ctx.dim)\n            if dist.ndim < b.ndim:\n                grad = grad.sum(-1)\n            grad.mul_(dist.data)\n            if ctx.nocount:                \n                grad.sub_(dist.data*(grad.sum(dim=ctx.dim,keepdims=True)))\n            else:\n                grad.sub_(dist.data*grad.sum(dim=ctx.dim,keepdims=True)*ctx.saved_tensors[2].float())\n            return grad,None,None,None\n        else:\n            return None,None,None,None\n\nclass softdot1(softdot):\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad=softdot.backward(ctx, grad_output)[0] if ctx.needs_input_grad[0] else None\n        gradb = None\n        if ctx.needs_input_grad[1]:\n            dist = ctx.saved_tensors[1]\n            if dist.ndim < ctx.saved_tensors[0].ndim:\n                gradb = dist.data[:,:,None]*torch.unsqueeze(grad_output,ctx.dim)\n            else:\n                gradb = dist.data*torch.unsqueeze(grad_output,ctx.dim)\n        return grad,gradb,None,None     \n\nclass xp1expm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, exp):        \n        ctx.exp = exp     \n        y = torch.sqrt(x.data)\n        if ctx.exp:\n            z = torch.exp(-y)\n            ctx.save_for_backward(z)\n            return ((y+1)*z).detach()\n        else:\n            ctx.save_for_backward(y)\n            return (torch.log1p(y).sub_(y)).detach()\n    @staticmethod\n    def backward(ctx, grad_output):        \n        if ctx.needs_input_grad[0]:\n            y = ctx.saved_tensors[0]\n            return (grad_output*y if ctx.exp else grad_output/(y+1)).mul_(-0.5), None\n        return None, None\n        \nclass sigma_act(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return (x.clamp(min=0.) + 1./(1. - x.clamp(max=0.))).detach()\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.needs_input_grad[0]:\n            x1 = ctx.saved_tensors[0].clamp(max=0.)-1.\n            return grad_output/x1/x1\n        return None# Write first python in Geoweaver",
  "history_output" : "",
  "history_begin_time" : 1668624357279,
  "history_end_time" : 1668624359852,
  "history_notes" : null,
  "history_process" : "ismi7d",
  "host_id" : "100001",
  "indicator" : "Done"
},]
