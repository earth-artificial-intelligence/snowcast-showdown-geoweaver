[{
  "history_id" : "68fl7oey7bz",
  "history_input" : "import cv2\nfrom datetime import datetime, timedelta\nimport geojson\nfrom geotiff import GeoTiff  \nfrom models_init import Model0\nimport netCDF4\nimport numpy as np\nimport pandas as pd\nimport os\nfrom os import path\nfrom scipy import interpolate\nfrom scipy.io import loadmat, savemat\nimport torch\nimport wget\nimport tarfile\nimport data_init as data\n\ncosd = lambda x: np.cos(np.radians(x))\n\njdays = list(range(0,183+21,7))+list(range(329-21,364,7)) #load Julian days\ndef getconstfeatures(workdir, uregions, awsurl, print=print):\n    datadir = path.join(workdir,'..')\n    print(f\"getconstfeatures: datadir={datadir} list={os.listdir(datadir)}\")    \n    file = path.join(workdir,'grid_cells.geojson')\n    print(f\"Loading {file}\")\n    with open(file) as f:\n        grid0 = geojson.load(f)\n    grid0 = pd.DataFrame([{'cell_id':g['properties']['cell_id'], 'region':g['properties']['region'], \n                          'corners': np.array(g['geometry']['coordinates'])} for g in grid0['features']]).set_index('cell_id')\n    file = path.join(workdir,'ground_measures_metadata.csv')\n    print(f\"Loading {file}\")\n    stmeta0 = pd.read_csv(file).set_index('station_id')\n    \n    stmetafile = path.join(workdir,'stmeta.csv')\n    gridfile = path.join(workdir,'grid.csv')\n    read = path.isfile(stmetafile) and path.isfile(gridfile)\n    if read:\n        print(f'Loading stmeta from {stmetafile} and grid from {gridfile}')\n        stmeta = pd.read_csv(stmetafile).set_index('station_id')\n        grid = pd.read_csv(gridfile).set_index('cell_id') \n        noex = set(stmeta0.index).difference(set(stmeta.index)).union(set(grid0.index).difference(set(grid.index)))\n        if len(noex) > 0:\n            print('unvalid stmeta / grid for {noex}')\n            read = False\n        else:\n            lonr = 1.5\n            lon1 = np.floor(min(grid['longitude'].values.min(),stmeta['longitude'].values.min())/lonr-1.)*lonr\n            lon2 = np.ceil(max(grid['longitude'].values.max(),stmeta['longitude'].values.max())/lonr+1.)*lonr\n            latr = 1.\n            lat1 = np.floor(min(grid['latitude'].values.min(),stmeta['latitude'].values.min())/latr-1.)*latr\n            lat2 = np.ceil(max(grid['latitude'].values.max(),stmeta['latitude'].values.max())/latr+1.)*latr   \n    if not read:        \n        print('Creating stmeta and grid')\n        grid = grid0\n        stmeta = stmeta0\n        gll = np.vstack(grid['corners'].values)\n        grid['latitude'] = gll[:,:,1].mean(1)\n        grid['longitude'] = gll[:,:,0].mean(1)\n        \n        lonr = 1.5; latr = 1.\n        lon1 = np.floor(min(gll[:,:,0].min(),stmeta['longitude'].values.min())/lonr-1.)*lonr\n        lon2 = np.ceil(max(gll[:,:,0].max(),stmeta['longitude'].values.max())/lonr+1.)*lonr\n        lat1 = np.floor(min(gll[:,:,1].min(),stmeta['latitude'].values.min())/latr-1.)*latr\n        lat2 = np.ceil(max(gll[:,:,1].max(),stmeta['latitude'].values.max())/latr+1.)*latr\n        for lab in uregions:\n            grid[lab] = np.array([grid['region'][k]==lab for k in range(grid.shape[0])]).astype(np.float32)\n            stmeta[lab] = np.zeros(stmeta.shape[0])\n            \n        for lab in ['CDEC', 'SNOTEL']:\n            stmeta[lab] = np.array([stmeta.index[k][:len(lab)]==lab for k in range(stmeta.shape[0])]).astype(np.float32)\n            grid[lab] = np.zeros(grid.shape[0])\n                \n        rgauss = 2.0\n        def getaver (lon,lat,elev,r):\n            ry = r/(111.*(lat[1]-lat[0]))\n            rx = r/(111.*(lon[1]-lon[0])*cosd((lat1+lat2)*0.5))\n            av = elev.copy()\n            cv2.GaussianBlur(elev, (2*int(rgauss*rx)+1, 2*int(rgauss*ry)+1), rx, av, ry)\n            f = interpolate.interp2d(lon, lat, av, kind='linear')\n            return lambda lons, lats: np.array([f(lons[k], lats[k])[0] for k in range(lons.shape[0])])\n        \n        demfile = f\"dem_N{lat1}_{lat2}_W{-lon1}_{-lon2}.mat\"\n        fname = path.join(datadir, demfile)\n        if not path.isfile(fname):\n            print('Creating DEM features')\n            dem = data.getdem(lat1,lat2,lon1,lon2,dir=path.join(datadir,'dem'), matfile=fname)\n        else:\n            print(f'Loading {demfile}')\n            dem = loadmat(fname)\n        demlon = dem.pop('lon').squeeze()\n        demlat = dem.pop('lat').squeeze()\n        print('Calculation DEM features')\n        for key in dem:\n            if key[:2] != '__':\n                elev = dem[key]\n                if key == 'elev':\n                    rads = [3, 10, 30, 100]\n                    f = getaver(demlon,demlat,elev,1.)\n                    grid['elevation_m'] = f(grid['longitude'], grid['latitude'])\n                    for r in rads:\n                        f_av = getaver(demlon,demlat,elev,r)\n                        name = 'elevation_'+str(r)\n                        for d in [stmeta, grid]:\n                            d[name] = f_av(d['longitude'], d['latitude']) - d['elevation_m']\n                else:\n                    rads1 = [1, 3, 10, 30]\n                    for r in rads1:\n                        f_av = getaver(demlon,demlat,elev,r)\n                        name = key+str(r)\n                        for d in [stmeta, grid]:\n                            d[name] = f_av(d['longitude'], d['latitude'])\n        ev = getaver(demlon,demlat,dem['elev'],1.)(stmeta['longitude'], stmeta['latitude'])\n        print(f\"dem elevation/stmeta elevation = {ev/stmeta['elevation_m']}\")\n        del demlon,demlat,dem\n        \n        print('Loading GLOBCOVER')  \n        for d in [stmeta, grid]:\n            for key in [key for key in d.keys() if key[:9]=='GLOBCOVER']:\n                d.pop(key)\n        \n        ncname = path.join(datadir,'C3S-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1.nc')\n        if not path.isfile(ncname):\n            arch = 'land_cover_map.tar.gz'\n            fname = path.join(datadir,arch)\n            if not path.isfile(fname):\n                print('Downloading '+arch)\n                wget.download(awsurl+arch, out=fname)\n            tar = tarfile.open(fname, \"r:gz\").extractall(datadir)\n            # ncname = path.join(datadir, tar.getmembers()[0].get_info()['name'])\n            os.remove(fname)\n        print(f'Loading GLOBCOVER from {ncname}')\n        nc = netCDF4.Dataset(ncname)\n        lon = np.array(nc.variables['lon'][:])\n        lat = np.array(nc.variables['lat'][:])\n        ok = ((lat>=lat1)&(lat<=lat2)).nonzero()[0]\n        ilat0 = ok[0]; ilat1 = ok[-1]+1\n        ok = ((lon>=lon1)&(lon<=lon2)).nonzero()[0]\n        ilon0 = ok[0]; ilon1 = ok[-1]+1\n        arr = np.array(nc.variables['lccs_class'][0,ilat0:ilat1,ilon0:ilon1])\n        lon = lon[ilon0:ilon1]\n        lat = lat[ilat0:ilat1]\n        nc.close()\n        \n        printvalstat = lambda arr: print ({t: (arr==t).sum()/arr.size*100. for t in np.unique(arr.reshape(-1))})\n        printvalstat (arr)\n        arr[(arr>=10) & (arr<30)] = 30\n        arr[arr==110] = 100; arr[arr==120] = 100\n        arr[(arr>130)&(arr<160)] = 130\n        arr[arr==72] = 70; arr[arr==71] = 70\n        arr[arr==201] = 200\n        types = [30,70,90,100,130,200,210,220]\n        printvalstat (arr)\n        gstep=1./360.\n        # rads = [1, 3, 10, 30]\n        rads = [3]\n        print('Calculation GLOBCOVER features')\n        def calcfeatures(arr,types,gstep,prefix):\n            for t in types:\n                eq = (arr==t).astype(np.float32)\n                for r in rads:\n                    ry = r/(111.*gstep)\n                    rx = r/(111.*gstep*cosd((lat1+lat2)*0.5))\n                    av = eq.copy()\n                    cv2.GaussianBlur(eq, (2*int(rgauss*rx)+1, 2*int(rgauss*ry)+1), rx, av, ry)\n                    for d in [stmeta, grid]:\n                        ilon = ((d['longitude'].values-lon1)/(lon2-lon1)*arr.shape[1]).astype(np.int64)\n                        ilat = ((lat2-d['latitude'].values)/(lat2-lat1)*arr.shape[0]).astype(np.int64)\n                        d[prefix+str(t)+'_'+str(r)] = np.array([av[ilat[i]:ilat[i]+2,ilon[i]:ilon[i]+2].mean() for i in range(ilon.shape[0])])\n            del eq,av\n        calcfeatures(arr,types,gstep,'GLOBCOVER')\n        del arr\n        \n        print('Loading SOIL')\n        for d in [stmeta, grid]:\n            for key in [key for key in d.keys() if key[:4]=='SOIL']:\n                d.pop(key)\n        tiffile = 'global_soil_regions_geoTIFF/so2015v2.tif'\n        tifname = path.join(datadir,tiffile)\n        if not path.isfile(tifname):\n            arch = 'soil_regions_map.tar.gz'\n            fname = path.join(datadir,arch)\n            if not path.isfile(fname):\n                print('Downloading '+arch)\n                wget.download(awsurl+arch, out=fname)\n            tar = tarfile.open(fname, \"r:gz\").extract('./'+tiffile, datadir)\n            os.remove(fname)\n        \n        print(f'Loading SOIL from {tifname}')\n        arr = np.array(GeoTiff(tifname).read_box([(lon1,lat1),(lon2,lat2)]))\n        printvalstat (arr)\n        # types = [7,21,50,54,64,74,75,81,83,92]\n        arr[arr>10] = np.floor(arr[arr>10]/10)*10\n        arr[arr==5] = 7; arr[arr==6] = 7\n        printvalstat (arr)\n        types = [7,20,50,60,70,80,90]\n        # types = np.unique(arr.reshape(-1))\n        gstep = 1./30.\n        # rads = [3, 10, 30]\n        rads = [10]\n        print('Calculation SOIL features')\n        calcfeatures(arr,types,gstep,'SOIL')\n        del arr\n        \n        # clm = 'ba'\n        # print('Loading '+clm)\n        # badir = path.join(datadir, clm+'-nc')\n        # if not path.isdir(badir):\n        #     arch = 'burned_areas_occurrence_map.tar.gz'\n        #     fname = path.join(datadir,arch)\n        #     if not path.isfile(fname):\n        #         print('Downloading '+arch)\n        #         wget.download(awsurl+arch, out=fname)\n        #     tar = tarfile.open(fname, \"r:gz\").extractall(datadir)\n        #     os.remove(fname)            \n        # rads = [10, 30]\n        # for jd in jdays:\n        #     if all([clm+str(r)+'_'+str(jd) in grid for r in rads]):\n        #         continue\n        #     tday = (datetime(2001,1,1)+timedelta(days=jd)).strftime('%m%d')\n        #     file = path.join(badir,'ESACCI-LC-L4-'+clm+'-Cond-500m-P13Y7D-2000'+tday+'-v2.0.nc')\n        #     print(f'Loading {clm} {tday} from {file}')\n        #     nc = netCDF4.Dataset(file)\n        #     lon = np.array(nc.variables['lon'][:])\n        #     lat = np.array(nc.variables['lat'][:])\n        #     ok = ((lat>=lat1)&(lat<=lat2)).nonzero()[0]\n        #     ilat0 = ok[0]; ilat1 = ok[-1]+1\n        #     ok = ((lon>=lon1)&(lon<=lon2)).nonzero()[0]\n        #     ilon0 = ok[0]; ilon1 = ok[-1]+1\n        #     v = np.array(nc.variables[clm.lower()+'_occ'][ilat0:ilat1,ilon0:ilon1]).astype(np.float32)\n        #     lon = lon[ilon0:ilon1]\n        #     lat = lat[ilat0:ilat1]\n        #     for r in rads:\n        #         f = getaver(lon, lat, v, r)\n        #         for d in [stmeta, grid]:\n        #             d[clm+str(r)+'_'+str(jd)] = f (d['longitude'], d['latitude'])\n        #     nc.close()\n            \n        stmeta = stmeta.copy()\n        grid = grid.copy()\n        \n        print('Saving stmeta to {stmetafile} and grid to {gridfile}')\n        stmeta.to_csv(stmetafile)\n        grid.to_csv(gridfile)\n        \n        print({key: grid[key].mean() for key in grid.keys() if key not in ['region', 'corners']})\n        print({key: stmeta[key].mean() for key in stmeta.keys() if key not in ['name','state']})\n        \n    print('Interpolate regions tags')\n    dtype = torch.float32\n    x = {'xlo': stmeta['longitude'].values, 'xla': stmeta['latitude'].values,\n         'ylo':   grid['longitude'].values, 'yla':   grid['latitude'].values}\n    x = {key: torch.tensor(x[key], dtype=dtype)[None] for key in x}\n    for lab in ['CDEC', 'SNOTEL']:\n        x['xval'] = torch.tensor(stmeta[lab].values, dtype=dtype)[None,:,None]\n        grid[lab] = Model0(x)[0,:,0].detach().numpy()\n    x = {key: x[('y' if key[0]=='x' else 'x')+key[1:]] for key in x if key[1:] in ['lo','la']}\n    for lab in uregions:\n        x['xval'] = torch.tensor(grid[lab].values, dtype=dtype)[None,:,None]\n        stmeta[lab] = Model0(x)[0,:,0].detach().numpy()\n    constfeatures = ['CDEC', 'elevation_m']\n    rads = [100, 30, 10, 3]\n    # rads = [100, 10]\n    # rads = [30, 10, 3]\n    constfeatures += ['elevation_'+str(r) for r in rads]\n    for d in [stmeta, grid]:\n        for r,r2 in zip(rads[1:],rads[:-1]):\n            d['elevation_'+str(r2)] -= d['elevation_'+str(r)]\n    # rads = [1, 3, 10, 30]\n    rads = [1, 3, 30]\n    for key in ['south', 'east']:\n        constfeatures += [key+str(r) for r in rads]\n        for r,r2 in zip(rads[1:],rads[:-1]):\n            for d in [stmeta, grid]:\n                # print([key,r2,np.abs(d[key+str(r2)]).mean(), r,np.abs(d[key+str(r)]).mean(),np.abs(d[key+str(r2)] - d[key+str(r)]).mean()])\n                d[key+str(r2)] -= d[key+str(r)]\n    rads = [1, 3, 10, 30]\n    for key in ['aspect']:\n        constfeatures += [key+str(r) for r in rads]    \n        for r,r2 in zip(rads[1:],rads[:-1]):\n            for d in [stmeta, grid]:\n                d[key+str(r2)] -= d[key+str(r)]\n    # constfeatures += [key for key in grid if key[:9]=='GLOBCOVER' and key[-2:] in ['_1','10']] # and key[9:12] != '220'\n    # constfeatures += [key for key in grid if key[:4]=='SOIL' and key[-2:] in ['_3','30']]\n    constfeatures += [key for key in grid if key[:9]=='GLOBCOVER' and key[-2:] in ['_3']]\n    constfeatures += [key for key in grid if key[:4]=='SOIL' and key[-2:] in ['10']]\n    # constfeatures += [key for key in grid if (key[:9]=='GLOBCOVER') or (key[:4]=='SOIL')]\n    print(f\"constfeatures : {constfeatures}\")\n    return stmeta,grid,constfeatures",
  "history_output" : "",
  "history_begin_time" : 1668624365843,
  "history_end_time" : 1668624370538,
  "history_notes" : null,
  "history_process" : "s28p1t",
  "host_id" : "100001",
  "indicator" : "Done"
},]
