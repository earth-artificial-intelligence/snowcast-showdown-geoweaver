[{
  "history_id" : "yyildd9xhxw",
  "history_input" : "import math\nimport torch\nimport torch.nn as nn\nfrom torch.optim.optimizer import Optimizer\nimport torch.nn.init as init\nfrom collections import OrderedDict\nimport numpy as np\nimport random\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\nfrom threading import Thread\nimport os\n \ndef set_seed(seed, deterministic=False):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n        \ndef _applyfn(module, fn):\n    for key in module.__dict__:\n        if isinstance(module.__dict__[key], torch.Tensor):\n            module.__dict__[key] = fn(module.__dict__[key])\n\nclass Linear2Function(torch.autograd.Function):\n    # Note that both forward and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, input, weight, bias, eps=None, mom=None, covx=None, step=1):\n        # u = None\n        if covx is not None and input.shape[0] > 1:\n            nrm = input - input.mean(0, keepdim=True)\n            covx.data.addmm_(nrm.t(), nrm, beta=mom, alpha=(1.-mom)/input.shape[0])\n            # covx.data.addmm_(input.t(), input, beta=mom, alpha=(1.-mom)/input.shape[0])\n            # u = covx/(1.-mom**step)\n            ctx.eps=eps; ctx.mom=mom; ctx.step=step\n        ctx.save_for_backward(input, weight, covx)\n        ctx.withbias = bias is not None\n        return input.mm(weight.t()) + bias if ctx.withbias else input.mm(weight.t())\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, covx = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)            \n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n            if covx is not None and input.shape[0] > 1:\n                # grad_weight.add_(grad_weight.mean(1,keepdim=True), alpha=-0.9)\n                # grad_weight.sub_(grad_weight.mean(1,keepdim=True))\n                # grad_weight.div_(torch.sqrt((grad_weight*grad_weight).mean(1,keepdim=True)).add_(eps))\n                # grad_weight.div_(torch.abs(grad_weight).mean(1,keepdim=True).add_(eps))\n                u = covx/(1.-ctx.mom**ctx.step)\n                d = torch.sqrt(u.diagonal())\n                eye = torch.eye(*u.shape, out=torch.empty_like(u))\n                (u.div_(d[None,:]).div_(d[:,None])).add_(eye, alpha=1.+ctx.eps)\n                fp16 = u.dtype == torch.float16\n                # grad_weight = torch.mm(grad_weight, u.inverse())\n                gw = (grad_weight.float() if fp16 else grad_weight).unsqueeze(-1)\n                if hasattr(torch, 'linalg'):\n                    grad_weight = torch.linalg.solve(u.float() if fp16 else u,gw).squeeze(-1)\n                else:\n                    grad_weight = torch.solve(gw,u.float() if fp16 else u)[0].squeeze(-1)\n                if fp16:\n                    grad_weight = grad_weight.half()\n                # grad_weight.div_(torch.sqrt((grad_weight*grad_weight).mean(1,keepdim=True)).add_(eps))\n                # grad_weight = torch.mm(grad_weight, u.float().inverse().to(grad_weight.dtype))    \n        if ctx.withbias and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n        return grad_input, grad_weight, grad_bias, None, None, None, None\nclass Linear2(nn.Linear):\n    def __init__(self, in_features, out_features, bias=True, eps=1e-6, mom=0.9):\n        nn.Linear.__init__(self, in_features, out_features, bias=bias)        \n        self.eps,self.mom = eps, torch.autograd.Variable(torch.tensor(mom), requires_grad=False)\n        self.reset_parameters ()\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        self.covx = torch.autograd.Variable(torch.zeros(self.weight.shape[1],self.weight.shape[1],dtype=self.weight.dtype,device=self.weight.device), requires_grad=False)\n        self.step = 0\n    def forward(self, input):\n        sh = list(input.shape)\n        if self.training:\n            if self.covx.device == input.device:\n                self.step += 1\n                step = self.step\n                covx = self.covx\n            else:\n                step = self.step + 1\n                covx = self.covx.to(input.device)\n            if len(sh) > 2:\n                return Linear2Function.apply(input.reshape(-1,sh[-1]), self.weight, self.bias, self.eps, self.mom, covx, step).reshape(sh[:-1]+[-1])\n            else:\n                return Linear2Function.apply(input, self.weight, self.bias, self.eps, self.mom, covx, step)\n        else:\n            if len(sh) > 2:\n                return Linear2Function.apply(input.reshape(-1,sh[-1]), self.weight, self.bias).reshape(sh[:-1]+[-1])\n            else:\n                return Linear2Function.apply(input, self.weight, self.bias)\n    def _apply(self, fn):\n        nn.Linear._apply(self, fn)\n        _applyfn(self, fn)\n        return self\nclass LinearConv2(Linear2):\n    def __init__(self, in_features, out_features, bias=True, eps=1e-6, mom=0.9):\n        Linear2.__init__(self, in_features, out_features, bias=bias, eps=eps, mom=mom)\n    def forward(self, x):\n        return Linear2.forward(self, x.permute(0,2,1)).permute(0,2,1)\n        \nclass normalizeFunc(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, dims=[0], momentum=0.1, eps=1e-5, running_mean=None, running_var=None, factor=None):\n        if factor is not None:\n            mean = x.mean(dims, keepdim=True).float()\n            if torch.isfinite(mean).all():\n                running_mean.data.mul_(1. - momentum).add_(mean,alpha=momentum)\n                running_var.data.mul_(1. - momentum).add_((x*x).mean(dims, keepdim=True).float(),alpha=momentum)\n                # xmin = x.min(dims[-1], keepdim=True)[0]\n                # xmax = x.max(dims[-1], keepdim=True)[0]\n                # for d in dims[:-1]:\n                #     xmin = xmin.min(d, keepdim=True)[0]\n                #     xmax = xmax.max(d, keepdim=True)[0]\n                # running_var.data.mul_(1. - momentum).add_((xmax-xmin).float(),alpha=momentum)\n                factor.data.mul_(1. - momentum).add_(momentum)\n                bias1 = -running_mean/factor\n                weight.data.copy_((1./(torch.sqrt((running_var/factor - bias1*bias1 + eps).clamp_(min=eps)))).to(x.dtype))\n                # weight.data.copy_((1./(running_var/factor+eps)).to(x.dtype))\n                bias.data.copy_((weight*bias1).to(x.dtype))\n        ctx.save_for_backward(weight)\n        if x.is_mkldnn:\n            return (x.to_dense() * weight + bias).to_mkldnn()\n        else:\n            return x * weight + bias\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad = None\n        if ctx.needs_input_grad[0]:\n            grad = ctx.saved_tensors[0]*grad_output\n        return grad, None, None, None, None, None, None, None, None\n    \nclass Normalize(nn.Module):\n    def __init__(self, num_features, dims=[0], eps=1e-5, momentum=0.01, frozen=False, mkl=False, *args, **kwargs):\n        super(Normalize, self).__init__()        \n        self.num_features = num_features\n        self.dims = dims\n        self.eps = eps\n        self.mkl = mkl\n        self.momentum = momentum\n        self.frozen = frozen\n        shape = [1]*(len(dims)+len(num_features))\n        for j,i in enumerate ([i for i in range(len(shape)) if i not in dims]):\n            shape[i] = num_features[j]\n        self.register_buffer('weight', torch.ones(*shape))\n        self.register_buffer('bias', torch.zeros(*shape))\n        self.register_buffer('running_mean', torch.zeros(*shape))\n        self.register_buffer('running_var', torch.ones(*shape))\n        self.register_buffer('factor', torch.zeros(1))\n    def _apply(self, fn):\n        nn.Module._apply(self, fn)\n        _applyfn(self, fn)\n        return self\n    def float(self):\n        fp16 = self.weight.dtype is torch.float16\n        nn.Module.float(self)\n        if fp16:\n            self.weight = self.weight.half()\n            self.bias = self.bias.half()\n\n    def forward(self, x: torch.Tensor):\n        if not self.frozen and self.training and self.momentum > 0:\n            y = normalizeFunc.apply(x, self.weight, self.bias, self.dims, self.momentum, self.eps, self.running_mean, self.running_var, self.factor)\n        else:\n            y = normalizeFunc.apply(x, self.weight, self.bias)\n        return y.to_mkldnn() if self.mkl else y\n            \n    # def frombn(self,bn):\n    #     self.factor = torch.ones(1)\n    #     with torch.no_grad():\n    #         self.running_var = bn.running_var/bn.weight.data/bn.weight.data - self.eps\n    #         self.weight = bn.weight.data/torch.sqrt(bn.running_var + self.eps)\n    #         self.bias   = bn.bias.data - bn.running_mean * self.weight\n    #         self.running_mean = bn.running_mean - bn.bias.data / self.weight\n    #         self.running_var += self.running_mean*self.running_mean\n        \ndef weight_init(m,gain=1.):\n    '''\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    '''\n    if hasattr(m,'bias'):    \n        if m.bias is not None and isinstance(m.bias, torch.Tensor):\n            # init.constant_(m.bias.data, 0)\n            init.normal_(m.bias.data, mean=0, std=0.1*gain)\n    if isinstance(m, nn.Conv1d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.Conv2d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.Conv3d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose1d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose2d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.ConvTranspose3d):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.BatchNorm1d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.BatchNorm2d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.BatchNorm3d):\n        init.normal_(m.weight.data, mean=1, std=0.02*gain)\n    elif isinstance(m, nn.Linear):\n        init.xavier_uniform_(m.weight.data,gain=gain)\n    elif isinstance(m, nn.LSTM):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain*0.1, mean=param.data.mean())\n    elif isinstance(m, nn.LSTMCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.GRU):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.GRUCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data, std=gain)\n    elif isinstance(m, nn.Embedding):\n        m.weight.data.uniform_(-0.01,0.01)\n\nclass AdamW(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0., L1=0.,\n                       warmup = 100, amsgrad=False, belief=False, gc=0.9, parallel=True, gradinit=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= gc <= 1.0:\n            raise ValueError(\"Invalid gc value: {}\".format(gc))\n        \n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, warmup = warmup, \n                        amsgrad=amsgrad, belief=belief, gc=gc, gradinit=gradinit, L1=L1)\n        params = list(params)\n        super(AdamW, self).__init__([params[i] for i in np.argsort([p.numel() for p in params])[-1::-1]], defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n            \n    def stepone(self, group, p):\n        p_data_fp32 = p.data.float()\n        grad = p.grad.data.float()\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['exp_avg'] = torch.zeros_like(p_data_fp32)\n            state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n            if group['amsgrad']:\n                # Maintains max of all exp. moving avg. of sq. grad. values\n                state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n        # else:\n        #     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n        #     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n        \n        warmup = group['warmup'] > state['step']\n        gradinit = group['gradinit'] and warmup and p_data_fp32.ndim>1 and p_data_fp32.numel() > 8\n        if gradinit:\n            # grad = torch.full_like(p_data_fp32, (grad*p_data_fp32).mean())\n            grad = p_data_fp32*(grad*p_data_fp32).mean(0,keepdim=True)\n            # grad = (grad*p_data_fp32).mean(0,keepdim=True).expand_as(p_data_fp32)\n        exp_avg = state['exp_avg']\n        exp_avg_sq = state['exp_avg_sq']\n        beta1, beta2 = group['betas']\n\n        state['step'] += 1\n        bias_correction1 = 1. - beta1 ** state['step']\n        bias_correction2 = math.sqrt(1. - beta2 ** state['step'])\n                \n        # Gradient centralization\n        if group['gc']>0. and grad.ndim > 1:\n            grad.add_(grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True), alpha=-group['gc'])\n        if group['belief'] and grad.ndim > 1:\n            grad_residual = grad.add(exp_avg, alpha=-1./bias_correction1)\n            exp_avg_sq.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1. - beta2)\n        else:\n            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1. - beta2)\n        exp_avg.mul_(beta1).add_(grad, alpha=1. - beta1)\n                \n        scheduled_lr = 1e-8+state['step']*group['lr']/group['warmup'] if warmup and not gradinit else group['lr']\n        if group['weight_decay'] != 0:\n            p_data_fp32.mul_(1. - group['weight_decay']*scheduled_lr)\n        if group['L1'] != 0:\n            p_data_fp32.add_(-p_data_fp32.sign() * group['L1']*scheduled_lr)\n        if group['amsgrad']:\n            # Maintains the maximum of all 2nd moment running avg. till now\n            torch.max(state['max_exp_avg_sq']*beta2, exp_avg_sq, out=state['max_exp_avg_sq'])\n            denom = state['max_exp_avg_sq'].sqrt()\n        else:\n            denom = exp_avg_sq.sqrt()\n        p_data_fp32.addcdiv_(exp_avg, denom.add_(group['eps']*bias_correction2), \n                             value = -scheduled_lr*bias_correction2/bias_correction1)\n        p.data.copy_(p_data_fp32)\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        threads = []\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                if torch.isnan(grad).any() or (grad == 0.).all():\n                    continue\n                if sum([d>1 for d in p.data.shape])>1:\n                    threads.append(Thread (target=self.stepone, args=(group, p)))\n                    threads[-1].start()\n                else:\n                    self.stepone(group, p)\n        for thread in threads:\n            thread.join()\n        return loss\n\ndef reduce_mem_usage(df, use_float16=False,output=print):\n#    \"\"\"\n#    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n#    \"\"\"\n#    start_mem = df.memory_usage().sum() / 1024**2\n#    output(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n#    end_mem = df.memory_usage().sum() / 1024**2\n#    output(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n#    output(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))    \n    return df                ",
  "history_output" : "",
  "history_begin_time" : 1668624357181,
  "history_end_time" : 1668624360299,
  "history_notes" : null,
  "history_process" : "b320xo",
  "host_id" : "100001",
  "indicator" : "Done"
},]
